{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura: Reconocimiento de patrones binarios usando neuronas de McCulloch-Pitts --- 30:00 min\n",
    "===\n",
    "\n",
    "* Última modificación: Marzo 10, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema real abordado por McCulloch y Pitts consistía en desarrollar un sistema de visión que permite identificar patrones binarios simples. Es decir, asociar el patrón de entrada (0, 0, 1) a 1, o el (1, 1, 1) a 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En términos de los datos, se tiene un conjunto de cuatro patrones binarios de entrada que deben ser reconocidos (que aparecen en la figura de abajo), donde cada patrón está conformado por tres dígitos binarios $\\{0, 1\\}$. Cada patrón binario es representado como una columna con cajones, y los valores binarios están codificados con los colores blanco y gris. En este caso, el sistema de visión debe determinar el valor de los tres bits (entrada al modelo) y el cerebro debe determinar si el patrón arbitrario observado corresponde a uno de los cuatro patrones indicados (decisión). \n",
    "\n",
    "En términos matemáticos, este problema puede ser definido como un problema de clasificación de patrones donde las entradas son todas las cadenas de tres dígitos binarios posibles, y la salida es 1 si la cadena es reconocida y 0 en caso contrario. Dicho de otra forma, los todos los patrones binarios podrían ser asignados de forma única a dos conjuntos posibles, y el modelo determina a cual conjunto {0, 1} pertenece el patrón."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![assets/McCullochPitts-01.png](assets/McCullochPitts-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otras palabras, se desea tener un modelo matemático $f($ `Entrada` $)$ = `Salida` cuyas entradas y salidas están determinadas por la siguiente tabla, donde cero es blanco y uno es gris. Los patrones con salida 1 son los que deben ser reconocidos.  \n",
    "    \n",
    "     Entrada   Salida\n",
    "    ------------------\n",
    "       000       0\n",
    "       001       1\n",
    "       010       0\n",
    "       011       0\n",
    "       100       1\n",
    "       101       0\n",
    "       110       1\n",
    "       111       1\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo matemático de la neurona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de neurona de [McCulloch-Pitts](https://en.wikipedia.org/wiki/Artificial_neuron) fue propuesto originalmente como un postulado sobre la forma en que el cerebro puede reconocer patrones complejos (parte derecha de la figura anterior). Este modelo plantea que, en general, una célula (neurona) puede representarse matemáticamente como una función no lineal que es descrita a continuación.\n",
    "\n",
    "El modelo de la neurona se basa en una unidad genérica de cómputo que aparece en la figura de abajo. La neurona (unidad de cómputo) recibe varias entradas binarias excitatorias notadas como $x_i$; la neurona agrega estas entradas mediante la función $g()$, definida usualmente como (parte izquierda de la figura de abajo):\n",
    "\n",
    "$$g(x_1, ...,x_n) = v = \\sum_{i=1}^n x_i$$\n",
    "\n",
    "para obtener una entrada neta $v$. Posteriormente la entrada neta $v$ es transformada con una función no lineal $f()$ definida como:\n",
    "\n",
    "$$f(v) = \n",
    "\\begin{cases}      \n",
    "      1, & \\text{Si $v \\ge \\theta$}\\\\\n",
    "      0, & \\text{Si $v \\lt \\theta$}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "El valor $\\theta$ es un umbral. Así, la salida de la neurona es un dígito binario $\\{0, 1\\}$ (parte central de la figura de abajo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![assets/McCullochPitts-02.png](assets/McCullochPitts-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, para el primer patrón de la tabla de la primera figura (000), la entrada neta será 0 (= 0 + 0 + 0), mientras que para el último patrón (111( será 3 (= 1 + 1 + 1). Si se tiene que parar dicha neurona el $\\theta$ es igual a 2, la salida para el patron 000, será 0 (0 < 2) y para el último 1 (3 > 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, la neurona artificial contiene conexiones inhibitorias notadas como $y_m$, tal que la salida siempre es cero si alguna de las entradas inhibitorias vale 1, independientemente del valor que puedan tomar las conexiones excitatorias. La representación gráfica de la neurona de McCulloch-Pitts aparece en la parte derecha de la figura anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las entradas $x_i$ son señales exitadores, y las señales $y_j$ son inhibidoras. La salida es cero (0) si alguna de las señales inhibidoras es uno (1).  La salida es uno (1) si la suma de señales de entrada es mayor o igual que el umbral ($\\theta$), y todas las señales inhibidoras son cero (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una neurona de McCulloch-Pitts puede interpretarse como una compuerta lógica de umbral (circuito lógico) amplicamente conocida en electrónica:\n",
    "\n",
    "![assets/McCullochPitts-06.png](assets/McCullochPitts-06.png)\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "- Para $x_1$ AND $x_2$:\n",
    "\n",
    "```\n",
    "       x1   x2     x1 AND x2    v   Umbral    Salida\n",
    "                   (deseada)                v >= umbral\n",
    "      ---------------------------------------------------\n",
    "       0    0         0         0     2          0\n",
    "       0    1         0         1     2          0\n",
    "       1    0         0         1     2          0\n",
    "       1    1         1         2     2          1\n",
    "```       \n",
    "       \n",
    "- Para $x_1$ NOR $x_2$:\n",
    "\n",
    "```\n",
    "       x1   x2     x1 NOR x2    v   Umbral    Salida\n",
    "                   (deseada)                v >= umbral\n",
    "      ---------------------------------------------------\n",
    "       0    0         1         0     0          1\n",
    "       0    1         0     x1 es inhibitoria    0\n",
    "       1    0         0     x2 es inhibitoria    0\n",
    "       1    1         0  x1, x2 son inhibitorias 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio.---** Calcule la salida para la siguiente red de neuronas de McCulloch-Pitts.\n",
    "\n",
    "![assets/McCullochPitts-04.png](assets/McCullochPitts-04.png)\n",
    "\n",
    "Rta/ \n",
    "     \n",
    "      Entrada  Salida\n",
    "     -----------------\n",
    "       0 0 0     0\n",
    "       0 0 1     0\n",
    "       0 1 0     0\n",
    "       0 1 1     1\n",
    "       1 0 0     0\n",
    "       1 0 1     0\n",
    "       1 1 0     1\n",
    "       1 1 1     1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación del problema de clasificación como una función lógica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema de clasificación planteado inicialmente, puede ser resuelto mediante la construcción de una función lógica $y=f(x_1, x_2, ..., x_n)$ definida como $f:\\{0,1\\}^n \\to \\{0,1\\}$ con:  $y  \\in \\{0,1\\}$, y $x_i \\in \\{0,1\\}$. En otras palabras, una función lógica es una función $f$ que recibe como entrada una cadena de bits de tamaño $n$ o $\\{0,1\\}^n$ y devuelve un dígito binario $\\{0,1\\}$. En términos del problema planteado, las entradas a la función lógica son las cadenas de bits observadas y la salida en un número binario que indica que el patrón se reconoce o no. Para el problema abordado, la función lógica requerida es definida por la siguiente tabla:\n",
    "\n",
    "     x1 x2 x3   f\n",
    "    ---------------\n",
    "      0  0  0   0\n",
    "      0  0  1   1\n",
    "      0  1  0   0\n",
    "      0  1  1   0\n",
    "      1  0  0   1\n",
    "      1  0  1   0\n",
    "      1  1  0   1\n",
    "      1  1  1   1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación de funciones lógicas mediante redes de neuronas de McCulloch-Pitts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red de neuronas de McCulloch-Pitts de dos capas puede representar cualquier función lógica $F:\\{0,1\\}^n \\to \\{0,1\\}$. A continuación se presenta el proceso de construcción para la función lógica que se presenta en la siguiente figura:\n",
    "\n",
    "![assets/McCullochPitts-03.png](assets/McCullochPitts-03.png)\n",
    "\n",
    "- Se crea una neurona en la primera capa por cada salida igual a 1; para el ejemplo planteado se requieren dos neuronas.  \n",
    "\n",
    "- La segunda capa contiene una neurona que representa la función OR; esto es, si todas las entradas a la neurona son cero, la salida es cero; si una o más entradas son uno, la salida de la neurona es uno. Esta neurona recibe como entrada todas las salida de las neuronas de la primera capa; todas las entradas son excitatorias y el umbral es 1.\n",
    "\n",
    "- Cada neurona de entrada se especializa en un patrón binario de entrada así: si una entrada es cero, la correspondiente conexión se hace inhibitoria y excitatoria en caso contrario; por ejemplo, para el patrón de entrada 001 (primera neurona de la primera capa) las conexiones para $x_1$ y $x2$ son inhibitorias y la conexión para $x_3$ es excitatoria; y para el patrón 010, las conexiones correspondientes a $x_1$ y $x_3$ son inhibitorias y para $x_2$ excitatoria. El valor del umbral de la neurona es la cantidad de unos de la entrada. Así para los patrones 001 y 010 el umbral es 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación matricial de la operación de una red de neuronas \n",
    "\n",
    "Para la implementación, las conexiones entre las neuronas son representadas mediante matrices. Si las conexiones inhibitorias son representas por un número negativo grande $N$, las conexiones a la primera capa de procesamiento puede representarse como:\n",
    "\n",
    "$$\\mathbf{w} = \n",
    "\\begin{bmatrix}\n",
    " N & N & 1 \\\\\n",
    " N & 1 & N\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "donde las filas representan las neuronas y las columnas los dígitos binarios de la entrada.\n",
    "\n",
    "De esta forma, la entrada neta para el patron 001 es:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    " N & N & 1 \\\\\n",
    " N & 1 & N\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ N \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "En la práctica resulta más conveniente usar un vector para representar los umbrales de las neuronas (que en este caso sería un vector de unos), tal que:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1 \\\\ N \\end{bmatrix} - \n",
    "\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \n",
    "\\begin{bmatrix} 0 \\\\ N-1 \\end{bmatrix}  \n",
    "$$\n",
    "\n",
    "Seguidamente, se aplica la función de transformación no lineal $f()$. El resultado obtenido corresponde a la salida de las dos neuronas de la primera capa de procesamiento.\n",
    "\n",
    "$$\n",
    "f \\left(\\begin{bmatrix} 0 \\\\ N-1 \\end{bmatrix} \\right) = \n",
    "\\begin{bmatrix} f(0) \\\\ f(N-1) \\end{bmatrix} = \n",
    "\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "que fue definida anteriormente como:\n",
    "\n",
    "$$f(v) = \n",
    "\\begin{cases}      \n",
    "      1, & \\text{Si $v \\ge \\theta$}\\\\\n",
    "      0, & \\text{Si $v \\lt \\theta$}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Recuerde que $N$ es un número negativo muy grande, tal que $f()$ siempre se evalua a cero.\n",
    "\n",
    "\n",
    "Finalmente, la función OR que representa neurona de salida puede ser computada con la función vectorial $\\max()$, la cual devuelve el valor máximo de su argumento.\n",
    "\n",
    "$$\n",
    "\\max \\left(\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right) = \\max(1, 0) = 1\n",
    "$$\n",
    "\n",
    "Este es el proceso de cálculo que se implementa computacionalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solución al problema propuesto "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el problema propuesto, cada patrón puede ser codificado como un vector de tres posiciones. Cuando el cuadro es negro, el valor de la posición correspondiente del vector es +1 y cuando es blanco es 0. Cada patrón es asociado a una variable de salida que toma el valor de +1 cuando el patrón debe ser reconocido y 0 cuando debe ser ignorado. De esta forma, el problema puede plantearse como:\n",
    "\n",
    "         Entrada    Salida\n",
    "      (x1, x2, x3)                 +----+----+----+\n",
    "     -----------------------       | x1 | x2 | x3 |\n",
    "           000        0            +----+----+----+\n",
    "           001        1                  \n",
    "           010        0                 \n",
    "           011        0                 \n",
    "           100        1                 \n",
    "           101        0\n",
    "           110        1\n",
    "           111        1       \n",
    "           \n",
    "De esta forma, el patrón 100 se representaría matricialmente como:\n",
    "\n",
    "$$\\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    " 1 \\\\\n",
    " 0 \\\\\n",
    " 0\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    #\n",
    "    # Se implementa una clase genérica de capa. Al agregar varias capas se\n",
    "    # obtiene una red neuronal.\n",
    "    #\n",
    "    def __init__(\n",
    "        self,\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Número de neuronas de salida de la capa\n",
    "        units,\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Número de neuronas de entrada a la capa\n",
    "        input_dim,\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Función de activación de la capa (esta corresponde a la función\n",
    "        # umbral)\n",
    "        activation=None,\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Semilla para la generación de pesos aleatorios de las conexiones.\n",
    "        seed=None,\n",
    "    ):\n",
    "        #\n",
    "        # Se inicializacn los parámetros de la capa\n",
    "        #\n",
    "        self.units = units\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = activation\n",
    "        self.kernel = None\n",
    "        self.bias = None\n",
    "\n",
    "        #\n",
    "        # Si el usuario no especifica una semilla para el generador aleatorio,\n",
    "        # se usa el valor por defecto del sistemas\n",
    "        #\n",
    "        if seed is None:\n",
    "            self.rng = np.random.default_rng()\n",
    "        else:\n",
    "            self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def check_activation(self):\n",
    "        #\n",
    "        # Se verifica si la activación es None y se asigna la función identidad\n",
    "        #\n",
    "        if self.activation is None:\n",
    "            self.activation = lambda x: x\n",
    "\n",
    "        if isinstance(self.activation, str):\n",
    "            #\n",
    "            # La función de activación puede ser especificada como un string\n",
    "            #\n",
    "            self.activation = {\n",
    "                \"linear\": lambda x: x,\n",
    "                \"sigmoid\": lambda x: 1 / (1 + np.exp(-x)),\n",
    "                \"relu\": lambda x: np.where(x <= 0, 0, x),\n",
    "                \"step\": lambda x: np.where(x < 0, 0, 1),\n",
    "            }[self.activation]\n",
    "\n",
    "    def check_weights(self):\n",
    "        #\n",
    "        # Si los pesos no han sido inicializados se asignan valores aleatorios.\n",
    "        # Este es el procedimiento normal en modelos de redes neuronales.\n",
    "        #\n",
    "        if self.bias is None:\n",
    "            self.bias = rng.uniform(\n",
    "                low=-1,\n",
    "                high=1,\n",
    "                shape=self.units,\n",
    "            )\n",
    "\n",
    "        if self.kernel is None:\n",
    "            self.kernel = rng.uniform(\n",
    "                low=-1,\n",
    "                high=1,\n",
    "                shape=(self.input_dim, self.weights),\n",
    "            )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        #\n",
    "        # La capa puede llamarse como si fuese una función. Primero, se\n",
    "        # verifican los pesos y luego se hace el cálculo de la propagación de\n",
    "        # la señal\n",
    "        #\n",
    "        self.check_weights()\n",
    "        self.check_activation()\n",
    "        return self.activation(np.matmul(x, self.kernel) + self.bias)\n",
    "\n",
    "\n",
    "class OrLayer(Layer):\n",
    "    #\n",
    "    # Se implementa una capa especializada para representar la función OR.\n",
    "    # Note que se usa la clase general Layer como base y solo se redefinen\n",
    "    # las funciones que cambian.\n",
    "    #\n",
    "    def __init__(self, units, input_dim):\n",
    "        super().__init__(\n",
    "            units=units,\n",
    "            input_dim=input_dim,\n",
    "            activation=\"step\",\n",
    "        )\n",
    "\n",
    "    def check_weights(self):\n",
    "        if self.kernel is None:\n",
    "            self.kernel = np.ones(shape=(self.input_dim, self.units))\n",
    "        if self.bias is None:\n",
    "            self.bias = -np.ones(shape=self.units)\n",
    "\n",
    "\n",
    "class McCullochPittsNetwork:\n",
    "    #\n",
    "    # Esta clase representa una red neuronal de McCullochPitts con una capa\n",
    "    # oculta y una capa de ssalida con una única neurona.\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.output_dim = 1\n",
    "        self.hidden_layer = None\n",
    "        self.output_layer = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #\n",
    "        # X es una lista de listas.\n",
    "        # y es una lista\n",
    "        #\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        #\n",
    "        # Se copia X para evitar efectos colaterles y se extraen únicamente los\n",
    "        # patrones para los cuales y == 1.\n",
    "        #\n",
    "        X = X.copy()\n",
    "        X = X[y == 1, :]\n",
    "\n",
    "        #\n",
    "        # La cantidad de entradas a la red es igual a la cantidad de columas de\n",
    "        # la matriz de ejemplos.\n",
    "        #\n",
    "        input_dim = X.shape[1]\n",
    "        \n",
    "        #\n",
    "        # La capa oculta tiene tantas neuronas como 1s exitan en y.\n",
    "        #\n",
    "        hidden_dim = sum(y)\n",
    "        self.hidden_layer = Layer(\n",
    "            units=hidden_dim, input_dim=input_dim, activation=\"step\"\n",
    "        )\n",
    "        \n",
    "        #\n",
    "        # Aquí se computa el theta para cada neurona de la capa oculta. Note\n",
    "        # que axis=1, indica que se hace la suma por filas para la matriz X.\n",
    "        # Es decir, el theta es igual a la cantidad de 1s que hay en el patrón\n",
    "        # de entrada.\n",
    "        #\n",
    "        self.hidden_layer.bias = -np.sum(X, axis=1)\n",
    "\n",
    "        #\n",
    "        # En este segmento de código se computa el N que aparecen en las \n",
    "        # ecuaciones. Para el patron 001, la salida debe ser 1, y para los\n",
    "        # patrones 101, 011 y 111 la salida de esta neurona debe ser 0. \n",
    "        #\n",
    "        # Recuerde que hay otra neurona especializada en el patron 111 del\n",
    "        # ejemplo.\n",
    "        #\n",
    "        # Para lograr esto, los pesos deben ser [-4, -4, 1], teniendo en \n",
    "        # cuenta que -4 = -3 (cantidad de columnas) - 1\n",
    "        #\n",
    "        w = np.transpose(X.copy())\n",
    "        w = np.where(w == 0, -(input_dim + 1), w)\n",
    "        self.hidden_layer.kernel = w\n",
    "\n",
    "        #\n",
    "        # La capa de salida de la red es una capa con una sola neurona OR\n",
    "        #\n",
    "        self.output_layer = OrLayer(units=1, input_dim=hidden_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.hidden_layer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def __repr__(self):\n",
    "        #\n",
    "        # Aca simplemente se imprime la representación en texto de la red \n",
    "        # neuronal\n",
    "        #\n",
    "        def coef2string(coef, isvar):\n",
    "            if coef > 0:\n",
    "                if coef == 1 and isvar is True:\n",
    "                    return \" + \"\n",
    "                return \" + {}\".format(coef)\n",
    "            if coef < 0:\n",
    "                if coef == -1 and isvar is True:\n",
    "                    return \" - \"\n",
    "                return \" - {}\".format(-coef)\n",
    "            return \"\"\n",
    "\n",
    "        def var2string(coef, index):\n",
    "            if coef != 0:\n",
    "                return coef2string(coef, True) + \"x{}\".format(index)\n",
    "            return \"\"\n",
    "\n",
    "        text_hidden = []\n",
    "        for neuron, bias in zip(\n",
    "            np.transpose(self.hidden_layer.kernel), self.hidden_layer.bias\n",
    "        ):\n",
    "            eq = [var2string(weight, i) for i, weight in enumerate(neuron)]\n",
    "            if bias != 0:\n",
    "                eq = \"\".join(eq) + coef2string(bias, False)\n",
    "            else:\n",
    "                eq = \"\".join(eq)\n",
    "\n",
    "            eq = eq.strip()\n",
    "            if eq[0] == \"+\":\n",
    "                eq = eq[1:]\n",
    "            text_hidden.append(\"step(\" + eq.strip() + \")\")\n",
    "\n",
    "        text_output = [\n",
    "            coef2string(int(w[0]), True) + t\n",
    "            for w, t in zip(self.output_layer.kernel, text_hidden)\n",
    "        ]\n",
    "\n",
    "        if self.output_layer.bias != 0:\n",
    "            text_output.append(coef2string(int(self.output_layer.bias[0]), False))\n",
    "\n",
    "        text_output = [\"    \" + t.strip() + \"\\n\" for t in text_output]\n",
    "        text_output = \"step(\\n\" + \"\".join(text_output) + \")\"\n",
    "\n",
    "        return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Ejemplo propuesto\n",
    "#\n",
    "X = [\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 1],\n",
    "]\n",
    "\n",
    "y = [\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "]\n",
    "\n",
    "# \n",
    "# Se crea la red neuronal\n",
    "#\n",
    "nn = McCullochPittsNetwork()\n",
    "\n",
    "#\n",
    "# Se realiza el entrenamiento\n",
    "#\n",
    "nn.fit(X, y)\n",
    "\n",
    "#\n",
    "# Se infiere para un conjunto de datos de entrada\n",
    "#\n",
    "nn(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn(X).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4,  1,  1,  1],\n",
       "       [-4, -4,  1,  1],\n",
       "       [ 1, -4, -4,  1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Valores de las conexiones de la capa de entrada a la capa oculta\n",
    "#\n",
    "nn.hidden_layer.kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -2, -3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Valores de theta para las neuronas de la capa oculta\n",
    "#\n",
    "nn.hidden_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step(\n",
       "    + step(- 4x0 - 4x1 + x2 - 1)\n",
       "    + step(x0 - 4x1 - 4x2 - 1)\n",
       "    + step(x0 + x1 - 4x2 - 2)\n",
       "    + step(x0 + x1 + x2 - 3)\n",
       "    - 1\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# La red neuronal es equivalente a la siguiente función\n",
    "#\n",
    "nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de construcción del modelo no resulta adecuado para:\n",
    "\n",
    "* Patrones de muchos bits (muchas entradas).\n",
    "\n",
    "* Más de una salida. En este caso se construye una red para salida."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
