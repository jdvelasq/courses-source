{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84a4ac1-51e0-4bbe-951a-86fda5004c95",
   "metadata": {
    "tags": []
   },
   "source": [
    "Conteo de palabras en Hadoop y carga de resultados a MariaDB\n",
    "===\n",
    "\n",
    "* Última modificación: Mayo 26, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f6728-e6cf-4904-9977-91d3a38f4328",
   "metadata": {},
   "source": [
    "Archivos de datos\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d8c1aa-6f0b-40cb-b8e7-a5976bc88308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/wordcount\n",
      "input\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /tmp/wordcount\n",
    "!mkdir -p /tmp/wordcount/input\n",
    "%cd /tmp/wordcount\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c060c78-dc15-4078-b531-1744b7419809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7f07b4-3ec8-428c-bf16-ed31ec3a161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e71a5e-9829-4ea5-afd9-836ddd4da8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3a220-30d9-49f6-a2c9-0ef18bb855fa",
   "metadata": {},
   "source": [
    "Movimiento de datos al HDFS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d19af8c-3a76-47a3-ba59-7b2167eb4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted input\n",
      "-rw-r--r--   1 root supergroup       1093 2022-05-26 19:52 input/text0.txt\n",
      "-rw-r--r--   1 root supergroup        352 2022-05-26 19:52 input/text1.txt\n",
      "-rw-r--r--   1 root supergroup        440 2022-05-26 19:52 input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r input\n",
    "!hadoop fs -mkdir input\n",
    "!hadoop fs -copyFromLocal input/* input\n",
    "!hadoop fs -ls input/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743cead-6346-4b11-b588-ae5dd2270972",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">   \n",
    "    \n",
    "**Importante**:\n",
    "\n",
    "Se supone que los datos para ejecutar la aplicación siempre estarán ubicados en el HDFS. Es decir, la aplicación no es responsable de hacer la ingesta al HDFS.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec3aed-8d95-4866-9900-0ad7eff3ce1c",
   "metadata": {},
   "source": [
    "Parte 1: Creación del mapper y reducer\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee4d72d7-422d-4aeb-b71b-8550b8034c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#! /usr/bin/python3\n",
    "\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        for word in line.split():\n",
    "            sys.stdout.write(\"{}\\t1\\n\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a18d5a-adc6-4c38-bd08-fb19742f3353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#! /usr/bin/python3\n",
    "\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    curkey = None\n",
    "    total = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "\n",
    "        key, val = line.split(\"\\t\")\n",
    "        val = int(val)\n",
    "\n",
    "        if key == curkey:\n",
    "            total += val\n",
    "        else:\n",
    "            if curkey is not None:\n",
    "                sys.stdout.write(\"{}\\t{}\\n\".format(curkey, total))\n",
    "\n",
    "            curkey = key\n",
    "            total = val\n",
    "\n",
    "    sys.stdout.write(\"{}\\t{}\\n\".format(curkey, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52283e17-e902-47f1-8b83-961fefefcaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x mapper.py reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c898ae30-fce1-441c-973c-466a0236774a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '-': No such file or directory\n",
      "ls: cannot access '1': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88e8a8-df06-48e4-ac84-bdd94de90dfe",
   "metadata": {},
   "source": [
    "Paso 2: Aplicación en Hadoop\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "011ae417-3ba6-4a42-853d-5395c11ed65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hadoop_app.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile hadoop_app.sh\n",
    "#\n",
    "# Se ejecuta en Hadoop.\n",
    "#   -files: archivos a copiar al hdfs\n",
    "#   -input: archivo de entrada\n",
    "#   -output: directorio de salida\n",
    "#   -file: archivos a copiar de la maquina local al hdfs\n",
    "#   -maper: programa que ejecuta el map\n",
    "#   -reducer: programa que ejecuta la reducción\n",
    "#\n",
    "\n",
    "if hdfs dfs -test -d output; then\n",
    "    hdfs dfs -rm -r output\n",
    "fi\n",
    "\n",
    "hadoop jar \\\n",
    "    /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "    -files mapper.py,reducer.py  \\\n",
    "    -input input  \\\n",
    "    -output output \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py\n",
    "\n",
    "rm -rf output\n",
    "\n",
    "hdfs dfs -copyToLocal output output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a6982-9bab-4580-9cff-832da976cf8e",
   "metadata": {},
   "source": [
    "Paso 3: Aplicación para llenar la base de datos\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f7623b8-c317-4cbd-ada8-d9d3b2e74d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing populate_db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile populate_db.py\n",
    "\n",
    "import fileinput\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import mariadb\n",
    "\n",
    "\n",
    "def run():\n",
    "    conn = create_connection()\n",
    "    create_table(conn)\n",
    "    populate_table(conn)\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def populate_table(conn):\n",
    "    cur = conn.cursor()\n",
    "    files = glob.glob(\"output/*\")\n",
    "    with fileinput.input(files=files) as f:\n",
    "        for line in f:\n",
    "            row = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            sql = \"INSERT INTO words VALUES (%s,%d)\"\n",
    "            cur.execute(sql, tuple(row))\n",
    "            conn.commit()\n",
    "\n",
    "\n",
    "def create_table(conn):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"DROP DATABASE IF EXISTS wordcount;\")\n",
    "    cur.execute(\"CREATE DATABASE wordcount;\")\n",
    "    cur.execute(\"USE wordcount;\")\n",
    "    cur.execute(\"DROP TABLE IF EXISTS words;\")\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE words (\n",
    "            word       VARCHAR(20),\n",
    "            frequency  INT\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def create_connection():\n",
    "    return mariadb.connect(\n",
    "        user=\"root\",\n",
    "        password=\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b15e4-d636-4f04-8c15-ec45ba49558d",
   "metadata": {},
   "source": [
    "Coordinador\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2aa89a1-02f6-4cab-821d-65f6753b53d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_program.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_program.sh\n",
    "bash hadoop_app.sh \\\n",
    "&& hdfs dfs -ls output/ \\\n",
    "&& python3 populate_db.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98318952-9626-4a6f-9ff8-872e56c5de41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted output\n",
      "packageJobJar: [/tmp/hadoop-unjar1956316352292756910/] [] /tmp/streamjob5088729400220191662.jar tmpDir=null\n",
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2022-05-26 19:52 output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup       1649 2022-05-26 19:52 output/part-00000\n"
     ]
    }
   ],
   "source": [
    "!bash my_program.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb671a-e62e-4d0f-821e-f81385f424ce",
   "metadata": {},
   "source": [
    "Verificación\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5cf3346-2cb3-4c18-b97b-d67d3f4a78aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| word       | frequency |\n",
      "+------------+-----------+\n",
      "| (DA)       |         1 |\n",
      "| (see       |         1 |\n",
      "| Analytics  |         2 |\n",
      "| Analytics, |         1 |\n",
      "| Big        |         1 |\n",
      "+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "!mariadb -u root -e \"USE wordcount; SELECT * FROM words LIMIT 5;\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
