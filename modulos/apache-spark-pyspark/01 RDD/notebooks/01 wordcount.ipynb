{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de palabras en PySpark\n",
    "===\n",
    "\n",
    "* Última modificacion: Junio 22, 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se introducen los principales conceptos del algoritmo MapReduce en que se basa el modelo de Big Data. Como ejemplo se presenta, el conteo de palabras de un grupo de archivos. \n",
    "\n",
    "Al finalizar este tutorial, usted estará en capacidad de:\n",
    "\n",
    "* Explicar los fundamentos del algoritmo MapReduce.\n",
    "\n",
    "* Mover un conjunto de archivos entre el sistema local y el sistema HDFS.\n",
    "\n",
    "* Aplicar MapReduce al conteo de frecuencia de elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición del problema\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea contar la frecuencia de las palabras que aparecen en varios archivos de textos. Para simplificar el problema, pruebe el algoritmo con los archivos generados en las siguientes celdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/wordcount\n",
      "input\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se crea el directorio de entrada\n",
    "#\n",
    "!rm -rf /tmp/wordcount\n",
    "!mkdir -p /tmp/wordcount/input\n",
    "%cd /tmp/wordcount\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo MapReduce\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MapReduce](https://en.wikipedia.org/wiki/MapReduce) es el término utilizado para describir un modelo de programación en paralelo que permite el procesamiento de grandes volúmenes de datos o [Big Data](https://en.wikipedia.org/wiki/Big_data) que resultan difíciles de  procesar en las aplicaciones tradicionales de procesamiento de datos. En el concepto de Big Data convergen las técnicas de almacenamiento distribuido de datos con la computación de alto desempeño mediante clusters.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejemplificar el proceso, a continuación se presenta el ejemplo del conteo de la frecuencia de las letras que aparecen en el texto:\n",
    "\n",
    "    A A C\n",
    "    C B D\n",
    "    A C D\n",
    "    \n",
    "\n",
    "En la figura de abajo aparece, el esquema de operación de MapReduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![assets/map-reduce.jpg](assets/map-reduce.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso está conformado por los siguientes pasos:\n",
    "\n",
    "**Paso 1.--** Cada línea del archivo es enviada a un proceso diferente (que podría ser un nodo diferente de un clúster); esto permite la operación en paralelo sobre conjuntos muy grandes de datos.\n",
    "\n",
    "**Paso 2.-- MAP**: El mapeo consiste en convertir la información a parejas <clave, valor>. La definición de que se toma como clave y que se toma como valor depende de cada problema específico. En el ejemplo presentado, para realizar el conteo de letras, la clave corresponde a la letra y el valor al número 1 (conteo). La función que realiza este proceso se conoce como mapper.\n",
    "\n",
    "**Paso 3.-- SORT**: El sistema ordena las parejas <clave, valor>, tal que todas las parejas que tengan la misma clave queden juntas.\n",
    "\n",
    "**Paso 4.--REDUCE**: Consiste en reducir todas las parejas que tienen la misma clave a una sola; para ello, se debe definir como se computará (reducirá) el valor final. Para este ejemplo, la reducción consiste en sumar todos los valores que tengan la misma clave. La función que realiza este proceso se conoce como reducer.\n",
    "\n",
    "**Paso 5.--** El sistema entrega el resultado consolidado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso de mapear y reducir se conoce como un trabajo (Job). Un cómputo complejo puede requerir muchos jobs, los cuales pueden ser ordenados en procesos, de acuerdo con los requerimientos. Varios ejemplos de encadenamientos de jobs pueden verse en la siguiente figura, donde la M representa el mapper y R el reducer. SS es *shuffle & sort* que es donde se ordenan las parejas <clave, valor> por claves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![assets/map-reduce-jobs.jpg](assets/map-reduce-jobs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el modelo computacional implementado en Hadoop, el mapper lee del sistema de archivos de Hadoop (HDFS) y el reducer escribe al sistema de archivos, ya que la cantidad de datos es tal que no podrían ser cargados a memoria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spark](https://en.wikipedia.org/wiki/Apache_Spark) es un modelo computacional en el cual se elimina elimina a escritura a disco entre jobs (sólo se escribe cuando es necesario), lo que permite que el proceso se ejecute mucho más rápido. Sus principales componentes son las siguientes:\n",
    "\n",
    "* SparkRDD: opera sobre conjuntos de datos distribuidos mediante operaciones MapReduce.\n",
    "\n",
    "* SparkQL: Implementación del lenguaje SQL que puede ejecutarse sobre datos estructurados como tablas.\n",
    "\n",
    "* SparkML: Implementación de algoritmos de aprendizaje estadístico y aprendizaje automática que operan sobre datos estructurados como tablas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de palabras usando SparkRDD\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se realiza el conteo de frecuencia de palabras usando SparkRDD. Se asume que Spark está ejecutándose en un cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movimiento de los archivos al HDFS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark lee y escribe archivos en el sistema HDFS. Por lo tanto, es necesario transferir los datos del sistema local al HDFS. La gestión de archivos entre el sistema local y el HDFS se realiza mediante comandos similares a los del sistema operativo Unix en Terminal. A continuación se resumen los principales comandos.\n",
    "\n",
    "* `hdfs dfs -help`:  Imprime la ayuda en pantalla para todos los comandos.\n",
    "\n",
    "\n",
    "**Gestion de directorios y archivos.**\n",
    "\n",
    "\n",
    "* `hdfs dfs -ls <path>`\n",
    "\n",
    "\n",
    "* `hdfs dfs -mkdir <path>`\n",
    "\n",
    "\n",
    "* `hdfs dfs -rmdir <path>`\n",
    "\n",
    "\n",
    "* `hdfs dfs -cp <src> <dest>`\n",
    "\n",
    "\n",
    "* `hdfs dfs -mv <src> <dest>`\n",
    "\n",
    "\n",
    "* `hdfs dfs -rm <path>`\n",
    "\n",
    "\n",
    "* `hdfs dfs -cat <path>`\n",
    "\n",
    "\n",
    "* `hdfs dfs -head <path>`\n",
    "\n",
    "\n",
    "* `hdfs dfs -tail <path>`\n",
    "\n",
    "\n",
    "* `hdfs dfs -text <path>`. Imprime el arachivo en `<path>` y lo imprime en formato texto. Soporta archivos zip, TextRecordInputStream y Avro.\n",
    "\n",
    "\n",
    "* `hdfs dfs -stat <path>`: Imprime estadísticos de `<path>`.\n",
    "\n",
    "\n",
    "**Transferencia de información entre el sistema local y el HDFS**.\n",
    "\n",
    "\n",
    "* `hdfs dfs -get <src> <localdest>` / `hdfs dfs -copyToLocal <src> <localdest>`. Copia el contenido de `<src>` en el HDFS en `<localdest>` en el sistema local.\n",
    "\n",
    "\n",
    "* `hdfs dfs -put <localsrc> <dest>` / `hdfs dfs -copyFromLocal <src> <localdest>`. Copia el contenido de `<localsrc>` en el sistema local a `<dest>` en el HDFS.\n",
    "\n",
    "\n",
    "* `hdfs dfs -count <path>`. Cuenta el número de directorios, archivos y bytes en `<path>`.\n",
    "\n",
    "\n",
    "* `hdfs dfs -appendToFile <localsrc> <dest>`: pega al final de `<dest>` el contenido de los archivos en `<localsrc>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Se usan un directorio temporal en el HDFS. La siguiente\n",
    "# instrucción muestra el contenido del dicho directorio\n",
    "#\n",
    "!hdfs dfs -ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Crea la carpeta wordcount en el hdfs\n",
    "#\n",
    "!hdfs dfs -mkdir /tmp/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - root supergroup          0 2022-05-27 14:38 /tmp/wordcount\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Verifica la creación de la carpeta\n",
    "#\n",
    "!hdfs dfs -ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copia los archvios del directorio local wordcount/\n",
    "# al directorio /tmp/wordcount/ en el hdfs\n",
    "#\n",
    "!hdfs dfs -copyFromLocal input  /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - root supergroup          0 2022-05-27 14:38 /tmp/wordcount/input\n",
      "Found 3 items\n",
      "-rw-r--r--   1 root supergroup       1093 2022-05-27 14:38 /tmp/wordcount/input/text0.txt\n",
      "-rw-r--r--   1 root supergroup        352 2022-05-27 14:38 /tmp/wordcount/input/text1.txt\n",
      "-rw-r--r--   1 root supergroup        440 2022-05-27 14:38 /tmp/wordcount/input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Verifica que los archivos esten copiados en el hdfs\n",
    "#\n",
    "!hdfs dfs -ls /tmp/wordcount\n",
    "!hdfs dfs -ls /tmp/wordcount/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación en PySpark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación en PySpark es la siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# findspark: Permite usar PySpark como una libreria de Python\n",
    "#\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#\n",
    "# Importa las librerias requeridas para conectar\n",
    "# a Python con PySpark\n",
    "#\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "#\n",
    "# operador de agregación (MapReduce)\n",
    "#\n",
    "from operator import add\n",
    "\n",
    "#\n",
    "# Nombre de la aplicación en el cluster\n",
    "#\n",
    "APP_NAME = \"My Spark Application\"\n",
    "\n",
    "#\n",
    "# Configure Spark\n",
    "#\n",
    "conf = SparkConf().setAppName(APP_NAME) \n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Analytics is the discovery, interpretation, and communication of meaningful patterns ',\n",
       " 'in data. Especially valuable in areas rich with recorded information, analytics relies ',\n",
       " 'on the simultaneous application of statistics, computer programming and operations research ',\n",
       " 'to quantify performance.',\n",
       " '',\n",
       " 'Organizations may apply analytics to business data to describe, predict, and improve business ',\n",
       " 'performance. Specifically, areas within analytics include predictive analytics, prescriptive ',\n",
       " 'analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big ',\n",
       " 'Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, ',\n",
       " 'marketing optimization and marketing mix modeling, web analytics, call analytics, speech ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Lee los archivos del hdfs y los carga a la variable text\n",
    "#\n",
    "text = sc.textFile(\"/tmp/wordcount/input/*.txt\")\n",
    "\n",
    "# Se imprimen las primeras 10 líneas\n",
    "text.collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Analytics',\n",
       " 'is',\n",
       " 'the',\n",
       " 'discovery,',\n",
       " 'interpretation,',\n",
       " 'and',\n",
       " 'communication',\n",
       " 'of',\n",
       " 'meaningful',\n",
       " 'patterns']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# separa por palabras (split) con una palabra por registro\n",
    "#\n",
    "words = text.flatMap(lambda x: x.split())\n",
    "\n",
    "# Se imprimen las primeras 10 palabras\n",
    "words.collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Analytics', 1),\n",
       " ('is', 1),\n",
       " ('the', 1),\n",
       " ('discovery,', 1),\n",
       " ('interpretation,', 1),\n",
       " ('and', 1),\n",
       " ('communication', 1),\n",
       " ('of', 1),\n",
       " ('meaningful', 1),\n",
       " ('patterns', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Genera las parejas <clave, valor> representandolas\n",
    "# com la tupla (word, 1)\n",
    "#\n",
    "wc = words.map(lambda x: (x,1))\n",
    "wc.collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('interpretation,', 1),\n",
       " ('of', 8),\n",
       " ('in', 5),\n",
       " ('data.', 1),\n",
       " ('Especially', 1),\n",
       " ('analytics', 8),\n",
       " ('simultaneous', 1),\n",
       " ('operations', 1),\n",
       " ('research', 2),\n",
       " ('quantify', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Suma los valores para la misma clave. Spark internamente ordena por claves\n",
    "#\n",
    "counts = wc.reduceByKey(add)\n",
    "counts.collect()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Escribe los resultados al directorio `/tmp/output`\n",
    "#\n",
    "counts.saveAsTextFile(\"/tmp/wordcount/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archivos de resultados\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados son escritos al HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - root supergroup          0 2022-05-27 14:38 /tmp/wordcount/input\n",
      "drwxr-xr-x   - root supergroup          0 2022-05-27 14:39 /tmp/wordcount/output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   1 root supergroup          0 2022-05-27 14:39 /tmp/wordcount/output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup        778 2022-05-27 14:39 /tmp/wordcount/output/part-00000\n",
      "-rw-r--r--   1 root supergroup        562 2022-05-27 14:39 /tmp/wordcount/output/part-00001\n",
      "-rw-r--r--   1 root supergroup        510 2022-05-27 14:39 /tmp/wordcount/output/part-00002\n",
      "-rw-r--r--   1 root supergroup        594 2022-05-27 14:39 /tmp/wordcount/output/part-00003\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Archivos con los resultados. Note que se \n",
    "# generan varios archivos de resultados.\n",
    "#\n",
    "!hdfs dfs -ls /tmp/wordcount/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo `/tmp/output/_SUCCESS` es un archivo vacio que indica que el programa fue ejecutado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('interpretation,', 1)\n",
      "('of', 8)\n",
      "('in', 5)\n",
      "('data.', 1)\n",
      "('Especially', 1)\n",
      "('analytics', 8)\n",
      "('simultaneous', 1)\n",
      "('operations', 1)\n",
      "('research', 2)\n",
      "('quantify', 1)\n",
      "('Organizations', 1)\n",
      "('may', 1)\n",
      "('business', 4)\n",
      "('predict,', 1)\n",
      "('include', 1)\n",
      "('decision', 1)\n",
      "('descriptive', 1)\n",
      "('store', 1)\n",
      "('optimization,', 2)\n",
      "('modeling,', 2)\n",
      "('speech', 1)\n",
      "('promotion', 1)\n",
      "('risk', 1)\n",
      "('fraud', 1)\n",
      "('Since', 1)\n",
      "('algorithms', 1)\n",
      "('used', 3)\n",
      "('harness', 1)\n",
      "('current', 1)\n",
      "('field', 1)\n",
      "('involves', 1)\n",
      "('studying', 1)\n",
      "('potential', 1)\n",
      "('trends,', 1)\n",
      "('performance', 1)\n",
      "('goal', 1)\n",
      "('changes.', 1)\n",
      "('process', 1)\n",
      "('draw', 1)\n",
      "('specialized', 1)\n",
      "('systems', 1)\n",
      "('software.', 1)\n",
      "('techniques', 1)\n",
      "('are', 1)\n",
      "('commercial', 1)\n",
      "('organizations', 1)\n",
      "('disprove', 1)\n",
      "('scientific', 1)\n",
      "('hypotheses.', 1)\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /tmp/wordcount/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movimiento de los archivos de resultados a la máquina local\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copia los archivos de resultados a la maquina local\n",
    "#\n",
    "!mkdir -p output\n",
    "!hdfs dfs -getmerge /tmp/wordcount/output/* output/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('interpretation,', 1)\n",
      "('of', 8)\n",
      "('in', 5)\n",
      "('data.', 1)\n",
      "('Especially', 1)\n",
      "('analytics', 8)\n",
      "('simultaneous', 1)\n",
      "('operations', 1)\n",
      "('research', 2)\n",
      "('quantify', 1)\n"
     ]
    }
   ],
   "source": [
    "!head output/results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpieza de las carpetas de trabajo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/wordcount\n"
     ]
    }
   ],
   "source": [
    "!rm -rf input\n",
    "!rm -rf output\n",
    "!hdfs dfs -rm -r -f /tmp/wordcount/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
