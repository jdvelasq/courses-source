{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operaciones sobre RDD (resilient distributed datasets)\n",
    "===\n",
    "\n",
    "* Última modificación: Junio 22, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark (mediante sus interfaces en Scala, Java, Python) permite ejecutar operaciones de transformación en paralelo sobre conjuntos de datos que se encuentran particionados sobre los nodos del cluster. Estas operaciones son usadas principalmente en tareas de extracción, transformación y preparación de datos. Los datos procesados son  usados en aplicaciones de aprendizaje automático e inteligencia de negocios.\n",
    "\n",
    "Al finalizar este tutorial, el lector estará en capacidad de:\n",
    "\n",
    "* Crear colecciones paralelizadas en Spark a partir de datos existentes en Python.\n",
    "\n",
    "* Cargar archivos de datos en diferentes formatos.\n",
    "\n",
    "* Aplicar transformaciones y acciones sobre Datasets.\n",
    "\n",
    "* Usar algoritmos simples basados en transformaciones y acciones sobre conjuntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archivos de datos de ejemplo para las corridas\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se usan archivos de texto para demostrar algunas de las características de Spark. A coninuación se crean dichos archivos de texto y se mueven al HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se borra la carpeta y su contenido en la máquina local si existe\n",
    "!rm -rf wordcount/\n",
    "\n",
    "# Se crea la carpeta vacía\n",
    "!mkdir -p wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea la carpeta /tmp/wordcount en el hdfs\n",
    "!hdfs dfs -mkdir /tmp/wordcount\n",
    "\n",
    "# copia los archvios del directorio local wordcount/\n",
    "# al directorio /tmp/wordcount/ en el hdfs\n",
    "!hdfs dfs -copyFromLocal wordcount/* /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización de la aplicación\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se realiza la inicialización del sistema. `findspark` permite la conexión de Python con Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# findspark permite usar pyspark (interfaz de Python a Spark),\n",
    "# desde cualquier programa escrito en Python.\n",
    "#\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#\n",
    "# A continuación se inicializan las variables obligatorias\n",
    "# requeridas para trabajar con Spark desde Python:\n",
    "#\n",
    "#  SparkContext representa la conexión al cluster de Spark.\n",
    "#  SparkConf representa la configuración particular de una aplicación\n",
    "#     escrita en Spark.\n",
    "#  SparkSession representa la conexión para trabajar con SQL.\n",
    "#\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkConf = SparkConf().setAppName(\"My SparkQL Application\") \n",
    "sc = SparkContext(conf=sparkConf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de colecciones paralelizadas a partir de datos nativos en Python\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean mediante la función `parallelize` del `SparkContext`. Sobre estas colecciones se pueden aplicar operaciones en paralelo. Se crean a partir de datos nativos de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# crea una colección a partir de una lista\n",
    "#\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "#\n",
    "# rdd contiene un objeto en memoria donde\n",
    "# se almacena la lista, no la lista como tal.\n",
    "#\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Retorna una lista que contiene todos los \n",
    "# elementos en un RDD\n",
    "#\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# En el siguiente ejemplo se paraleliza un diccionario.\n",
    "# collect() retorna las claves.\n",
    "#\n",
    "rdd = sc.parallelize({\"a\":1, \"b\":2, \"c\":3, \"d\":4})\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de archivos con formato <clave, valor>\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark permite la carga de datos desde el sistema local, servicios web y formato `(key, value)` de Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# crea una secuencia de parejas clave-valor\n",
    "#\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "#\n",
    "# Una vez se tiene un objeto RDD, es posible aplicar operaciones\n",
    "# en paralelo sobre él. En la siguiente línea de código, se aplica \n",
    "# la función lambda x:(x, x**2) a cada elemento del RDD, mediante\n",
    "# la función map. Como resultado se genera una secuencia de pares\n",
    "# [(1, 1), (2, 4), ..., (5, 25)]\n",
    "#\n",
    "rdd = rdd.map(lambda x:(x, x**2))\n",
    "\n",
    "#\n",
    "# Salva la secuencia al hdfs\n",
    "#\n",
    "rdd.saveAsSequenceFile('/tmp/out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   1 root supergroup          0 2019-11-15 00:44 /tmp/out/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup        108 2019-11-15 00:44 /tmp/out/part-00000\n",
      "-rw-r--r--   1 root supergroup        108 2019-11-15 00:44 /tmp/out/part-00001\n",
      "-rw-r--r--   1 root supergroup        108 2019-11-15 00:44 /tmp/out/part-00002\n",
      "-rw-r--r--   1 root supergroup        124 2019-11-15 00:44 /tmp/out/part-00003\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# El archivo se almacena en /tmp/out/. Aquí se usa\n",
    "# ls para listar los archivos de la carpeta /tmp/\n",
    "# en el sistema hdfs. Note se creó la carpeta \n",
    "# /tmp/out como resultado del paso anterior\n",
    "#\n",
    "!hdfs dfs -ls /tmp/out/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Una vez se tiene una secuencia de pares almacenada en el hdfs,\n",
    "# es posible volverla a cargar mediante sequenceFile().\n",
    "# \n",
    "sc.sequenceFile(\"/tmp/out\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/out\n"
     ]
    }
   ],
   "source": [
    "# Se limpia el hdfs\n",
    "!hdfs dfs -rm -r -f /tmp/out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de archivos de texto con textFile y wholeTextFiles\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/tmp/wordcount/ MapPartitionsRDD[11] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# textFile() puede leer un archivo de texto o los archivos\n",
    "# especificados. A continuación, se leen todos los archivos \n",
    "# de texto de la carpeta /tmp/wordcount/ en el hdfs\n",
    "#\n",
    "rdd = sc.textFile(\"/tmp/wordcount/\")\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Se obtiene el timpo de dato almacenado en la variable rdd\n",
    "#\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Analytics is the discovery, interpretation, and communication of meaningful patterns ',\n",
       " 'in data. Especially valuable in areas rich with recorded information, analytics relies ',\n",
       " 'on the simultaneous application of statistics, computer programming and operations research ',\n",
       " 'to quantify performance.',\n",
       " '',\n",
       " 'Organizations may apply analytics to business data to describe, predict, and improve business ',\n",
       " 'performance. Specifically, areas within analytics include predictive analytics, prescriptive ',\n",
       " 'analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big ',\n",
       " 'Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, ',\n",
       " 'marketing optimization and marketing mix modeling, web analytics, call analytics, speech ',\n",
       " 'analytics, sales force sizing and optimization, price and promotion modeling, predictive ',\n",
       " 'science, credit risk analysis, and fraud analytics. Since analytics can require extensive ',\n",
       " 'computation (see big data), the algorithms and software used for analytics harness the most ',\n",
       " 'current methods in computer science, statistics, and mathematics.',\n",
       " 'The field of data analysis. Analytics often involves studying past historical data to ',\n",
       " 'research potential trends, to analyze the effects of certain decisions or events, or to ',\n",
       " 'evaluate the performance of a given tool or scenario. The goal of analytics is to improve ',\n",
       " 'the business by gaining knowledge which can be used to make improvements or changes.',\n",
       " 'Data analytics (DA) is the process of examining data sets in order to draw conclusions ',\n",
       " 'about the information they contain, increasingly with the aid of specialized systems ',\n",
       " 'and software. Data analytics technologies and techniques are widely used in commercial ',\n",
       " 'industries to enable organizations to make more-informed business decisions and by ',\n",
       " 'scientists and researchers to verify or disprove scientific models, theories and ',\n",
       " 'hypotheses.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# rdd almacena como strings, el contenido de los archivos\n",
    "# text0.txt, text1.txt y text2.txt\n",
    "#\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hdfs://0.0.0.0:9000/tmp/wordcount/text0.txt', 'Analytics is the discovery, interpretation, and communication of meaningful patterns \\nin data. Especially valuable in areas rich with recorded information, analytics relies \\non the simultaneous application of statistics, computer programming and operations research \\nto quantify performance.\\n\\nOrganizations may apply analytics to business data to describe, predict, and improve business \\nperformance. Specifically, areas within analytics include predictive analytics, prescriptive \\nanalytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \\nData Analytics, retail analytics, store assortment and stock-keeping unit optimization, \\nmarketing optimization and marketing mix modeling, web analytics, call analytics, speech \\nanalytics, sales force sizing and optimization, price and promotion modeling, predictive \\nscience, credit risk analysis, and fraud analytics. Since analytics can require extensive \\ncomputation (see big data), the algorithms and software used for analytics harness the most \\ncurrent methods in computer science, statistics, and mathematics.\\n')\n",
      "----\n",
      "('hdfs://0.0.0.0:9000/tmp/wordcount/text1.txt', 'The field of data analysis. Analytics often involves studying past historical data to \\nresearch potential trends, to analyze the effects of certain decisions or events, or to \\nevaluate the performance of a given tool or scenario. The goal of analytics is to improve \\nthe business by gaining knowledge which can be used to make improvements or changes.\\n')\n",
      "----\n",
      "('hdfs://0.0.0.0:9000/tmp/wordcount/text2.txt', 'Data analytics (DA) is the process of examining data sets in order to draw conclusions \\nabout the information they contain, increasingly with the aid of specialized systems \\nand software. Data analytics technologies and techniques are widely used in commercial \\nindustries to enable organizations to make more-informed business decisions and by \\nscientists and researchers to verify or disprove scientific models, theories and \\nhypotheses.\\n')\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# wholeTextFiles() retorna una lista de pares que contienen el nombre \n",
    "# del archivo y su correspondiente texto\n",
    "#\n",
    "rdd = sc.wholeTextFiles(\"/tmp/wordcount/\")\n",
    "for row in rdd.collect():\n",
    "    print(row)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones sobre RDD\n",
    "\n",
    "Spark soporta dos tipos de transformaciones sobre RDD:\n",
    "\n",
    "* Transformaciones: Son funciones que crean un nuevo RDD a partir de uno existente. Estas pueden entenderse como equivalentes a la función Map en el algorimto MapReduce.\n",
    "\n",
    "* Acciones: Retornan un valor después de realizar una computación sobre el RDD. Pueden entenderse como un reducer.\n",
    "\n",
    "Ver http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo se computa la cantidad de caracteres leídos de un grupo de archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 87, 92, 24, 0, 94, 93, 91, 88, 89, 89, 90, 92, 65, 86, 88, 90, 84, 87, 85, 87, 83, 81, 11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1861"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Carga de los archivos.\n",
    "#\n",
    "rdd = sc.textFile(\"/tmp/wordcount/\")\n",
    "\n",
    "#\n",
    "# Calculo de la longitud de cada línea usando la función len().\n",
    "# Se aplica la función len() a cada elemento del rdd\n",
    "#\n",
    "rdd = rdd.map(len)\n",
    "print(rdd.collect())\n",
    "\n",
    "#\n",
    "# Calculo del total de caracteres\n",
    "#\n",
    "rdd = rdd.reduce(lambda a, b: a+b)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 87, 92, 24, 0, 94, 93, 91, 88, 89, 89, 90, 92, 65, 86, 88, 90, 84, 87, 85, 87, 83, 81, 11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1861"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# En este ejemplo se pasa una función arbitraria a `map`.\n",
    "# La función add es equivalente a `lambda a, b: a+b`\n",
    "#\n",
    "from operator import add\n",
    "rdd = sc.textFile(\"/tmp/wordcount/\")\n",
    "rdd = rdd.map(len)\n",
    "print(rdd.collect())\n",
    "rdd = rdd.reduce(add)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformaciones\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las transformaciones son las siguientes:\n",
    "\n",
    "* **map**( f ): Aplica la función f a cada elemento del dataset.\n",
    "\n",
    "\n",
    "* **filter**( f ): retorna un dataset con los elementos para los cuales f retorna `True`.\n",
    "\n",
    "\n",
    "* **flatMap**( f ): Es similar a `map` pero cada item de entrada es mapeado a los items de salida.\n",
    "\n",
    "\n",
    "* **mapPattitions**( f ): Similar a `map` pero se ejecuta separadamente en cada partición.\n",
    "\n",
    "\n",
    "* **sample**( withReplacement, fraction, seed ): muestrea una fracción de los datos.\n",
    "\n",
    "\n",
    "* **union**( otherRDD ): retorna la unión de los dos RDD.\n",
    "\n",
    "\n",
    "* **intersection**( otherRDD ): retorna la intersección de los dos RDD.\n",
    "\n",
    "\n",
    "* **distinct**(): retorna los elementos diferentes en el RDD.\n",
    "\n",
    "\n",
    "* **groupByKey**(): retorna los elementos agrupados por clave.\n",
    "\n",
    "\n",
    "* **reduceByKey**( f ): reduce por clave usando la función f.\n",
    "\n",
    "\n",
    "* **aggregateByKey**(): agrega los elementos por clave.\n",
    "\n",
    "\n",
    "* **sortByKey**(): retorna RDD ordenado por clave.\n",
    "\n",
    "\n",
    "* **join**( otherRDD ): para los RDD (K, V) y (K, W) retorna (K, (V, W)).\n",
    "\n",
    "\n",
    "* **cogroup**( otherRDD ): para los RDD (K, V) y (K, W) retorna (K, (iterable(V), iterable(K))).\n",
    "    \n",
    "    \n",
    "* **cartesian**( otherRDD ): retorna el producto cartesiano de los dos RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acciones\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **reduce**( f ): agrega los elementos con la misma clave usando f.\n",
    "\n",
    "\n",
    "* **collect**():  retorna todos los elementos del RDD.\n",
    "\n",
    "\n",
    "* **count**(): Retorna el número de elementos del RDD.\n",
    "\n",
    "\n",
    "* **first**(): Retorna el primer elemento del RDD.\n",
    "\n",
    "\n",
    "* **take**( n ): Retorna los primeros n elementos del RDD.\n",
    "\n",
    "\n",
    "* **takeSample**(withReplacement, n): Retorna una muestra aleatoria del dataset.\n",
    "\n",
    "\n",
    "* **takeOrdered**(n): Retorna los primeros n elementos del RDD ordenado.\n",
    "\n",
    "\n",
    "* **saveAsText**(path): Salva el RDD como un arhivo en disco.\n",
    "\n",
    "\n",
    "* **saveAsSequenceFile**(path): Salva como clave-valor a Hadoop.\n",
    "\n",
    "\n",
    "* **countByKey**(): cuenta los elementos por clave.\n",
    "\n",
    "\n",
    "* **foreach**( f ): le aplica la función f a cada elemento del RDD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se presentan varios ejemplos de cálculos comúnes, implementados como operaciones sobre RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de frecuencia de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 1), ('a', 1), ('c', 1), ('d', 1), ('a', 1), ('b', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# creación de parejas (key, value). Se desea\n",
    "# contar el número de ocurrencias de cada letra \n",
    "# en la siguiente lista.\n",
    "#\n",
    "rdd = sc.parallelize([\"a\", \"b\", \"a\", \"c\", \"d\", \"a\", \"b\"])\n",
    "\n",
    "##\n",
    "# Se crean las parejas <clave, valor>\n",
    "##\n",
    "rdd = rdd.map(lambda s: (s, 1))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 2), ('c', 1), ('a', 3), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se reducen las parejas con la misma clave, sumando \n",
    "# los valores.\n",
    "#\n",
    "rdd = rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo, se genera el siguiente archivo de datos separados por tabuladores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data.txt\n",
    "E\t1\tb,g,f\tjjj:3,bbb:0,ddd:9,ggg:8,hhh:2\n",
    "A\t2\ta,f,c\tccc:2,ddd:0,aaa:3,hhh:9\n",
    "B\t5\tf,e,a,c\tddd:2,ggg:5,ccc:6,jjj:1\n",
    "A\t3\ta,b\thhh:9,iii:5,eee:7,bbb:1\n",
    "C\t6\tf,g,d,a\tiii:6,ddd:5,eee:4,jjj:3\n",
    "A\t7\tc,d\tbbb:2,hhh:0,ccc:4,fff:1,aaa:7\n",
    "A\t9\tg,d,a\taaa:5,fff:8,ddd:2,iii:0,jjj:7,ccc:1\n",
    "B\t1\tb,a\tfff:3,hhh:1,ddd:2\n",
    "E\t2\td,e,a,f\teee:4,ccc:5,iii:9,fff:7,ggg:6,bbb:0\n",
    "B\t3\td,b,g,f\tbbb:7,jjj:9,fff:5,iii:4,ggg:2,eee:3\n",
    "C\t7\td,c,f,b\thhh:6,eee:4,iii:0,fff:2,jjj:1\n",
    "C\t5\td,e,a,c\tbbb:7,iii:6,ggg:9\n",
    "D\t3\tg,e,f,b\tbbb:9,aaa:3,ccc:6,fff:4,eee:2\n",
    "E\t8\tc,f\taaa:8,ddd:5,jjj:1\n",
    "B\t9\td,b\tccc:0,jjj:6,fff:7,ddd:3,aaa:2\n",
    "D\t1\tf,e\tccc:0,eee:6,bbb:9,ddd:3\n",
    "E\t3\te,b,f\tbbb:6,iii:3,hhh:5,fff:4,ggg:9,ddd:2\n",
    "D\t5\tg,a\thhh:4,jjj:5,ccc:9\n",
    "E\t8\te,c,f,a\tccc:1,iii:6,fff:9\n",
    "E\t9\te,a\tbbb:9,aaa:3,fff:1\n",
    "E\t7\te,f\tddd:9,iii:2,aaa:4\n",
    "E\t3\tc,b,g\tccc:5,fff:8,iii:7\n",
    "D\t5\tc,f,a\teee:3,jjj:2,ddd:7\n",
    "A\t1\tf,a,d\tjjj:1,ggg:0,ccc:7,ddd:9,bbb:3\n",
    "E\t4\tc,d\tjjj:6,ccc:0,aaa:1,hhh:9,iii:7,ggg:8\n",
    "E\t6\te,d,c\tfff:3,eee:6,iii:4,bbb:7,ddd:0,ccc:1\n",
    "A\t8\ta,e,f\tfff:0,ddd:5,ccc:4\n",
    "E\t5\tc,a,g\tggg:6,hhh:3,ddd:9,ccc:0,jjj:7\n",
    "A\t6\tf,e\thhh:6,jjj:0,eee:5,iii:7,ccc:3\n",
    "C\t0\tf,c,a,g\teee:1,fff:4,aaa:2,ccc:7,ggg:0,ddd:6\n",
    "A\t1\tb,f\tccc:6,aaa:9,eee:5,ddd:0,bbb:3\n",
    "D\t2\tb,f\tbbb:7,hhh:1,aaa:6,iii:4,fff:9,ddd:5\n",
    "E\t5\ta,c\tfff:3,ccc:1,ggg:2,eee:5\n",
    "B\t4\tb,f,c\tiii:7,ggg:3,ddd:0,jjj:8,hhh:5,ccc:1\n",
    "B\t6\tf,a,e\thhh:6,ccc:3,jjj:0,bbb:8,ddd:7\n",
    "D\t7\ta,f\taaa:0,fff:5,ddd:3\n",
    "B\t8\tc,a\tddd:5,jjj:2,iii:7,ccc:0,bbb:4\n",
    "C\t9\tc,a,e,f\teee:0,fff:2,hhh:6\n",
    "E\t1\te,d\tfff:9,iii:2,eee:0\n",
    "E\t5\tf,a,d\thhh:8,ggg:3,jjj:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia el archivo del sistema local al hdfs\n",
    "!hdfs dfs -copyFromLocal data.txt /tmp/data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte a**\n",
    "\n",
    "Compute la suma de la columna 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['E', '1', 'b,g,f', 'jjj:3,bbb:0,ddd:9,ggg:8,hhh:2'],\n",
       " ['A', '2', 'a,f,c', 'ccc:2,ddd:0,aaa:3,hhh:9'],\n",
       " ['B', '5', 'f,e,a,c', 'ddd:2,ggg:5,ccc:6,jjj:1'],\n",
       " ['A', '3', 'a,b', 'hhh:9,iii:5,eee:7,bbb:1'],\n",
       " ['C', '6', 'f,g,d,a', 'iii:6,ddd:5,eee:4,jjj:3'],\n",
       " ['A', '7', 'c,d', 'bbb:2,hhh:0,ccc:4,fff:1,aaa:7'],\n",
       " ['A', '9', 'g,d,a', 'aaa:5,fff:8,ddd:2,iii:0,jjj:7,ccc:1'],\n",
       " ['B', '1', 'b,a', 'fff:3,hhh:1,ddd:2'],\n",
       " ['E', '2', 'd,e,a,f', 'eee:4,ccc:5,iii:9,fff:7,ggg:6,bbb:0'],\n",
       " ['B', '3', 'd,b,g,f', 'bbb:7,jjj:9,fff:5,iii:4,ggg:2,eee:3'],\n",
       " ['C', '7', 'd,c,f,b', 'hhh:6,eee:4,iii:0,fff:2,jjj:1'],\n",
       " ['C', '5', 'd,e,a,c', 'bbb:7,iii:6,ggg:9'],\n",
       " ['D', '3', 'g,e,f,b', 'bbb:9,aaa:3,ccc:6,fff:4,eee:2'],\n",
       " ['E', '8', 'c,f', 'aaa:8,ddd:5,jjj:1'],\n",
       " ['B', '9', 'd,b', 'ccc:0,jjj:6,fff:7,ddd:3,aaa:2'],\n",
       " ['D', '1', 'f,e', 'ccc:0,eee:6,bbb:9,ddd:3'],\n",
       " ['E', '3', 'e,b,f', 'bbb:6,iii:3,hhh:5,fff:4,ggg:9,ddd:2'],\n",
       " ['D', '5', 'g,a', 'hhh:4,jjj:5,ccc:9'],\n",
       " ['E', '8', 'e,c,f,a', 'ccc:1,iii:6,fff:9'],\n",
       " ['E', '9', 'e,a', 'bbb:9,aaa:3,fff:1'],\n",
       " ['E', '7', 'e,f', 'ddd:9,iii:2,aaa:4'],\n",
       " ['E', '3', 'c,b,g', 'ccc:5,fff:8,iii:7'],\n",
       " ['D', '5', 'c,f,a', 'eee:3,jjj:2,ddd:7'],\n",
       " ['A', '1', 'f,a,d', 'jjj:1,ggg:0,ccc:7,ddd:9,bbb:3'],\n",
       " ['E', '4', 'c,d', 'jjj:6,ccc:0,aaa:1,hhh:9,iii:7,ggg:8'],\n",
       " ['E', '6', 'e,d,c', 'fff:3,eee:6,iii:4,bbb:7,ddd:0,ccc:1'],\n",
       " ['A', '8', 'a,e,f', 'fff:0,ddd:5,ccc:4'],\n",
       " ['E', '5', 'c,a,g', 'ggg:6,hhh:3,ddd:9,ccc:0,jjj:7'],\n",
       " ['A', '6', 'f,e', 'hhh:6,jjj:0,eee:5,iii:7,ccc:3'],\n",
       " ['C', '0', 'f,c,a,g', 'eee:1,fff:4,aaa:2,ccc:7,ggg:0,ddd:6'],\n",
       " ['A', '1', 'b,f', 'ccc:6,aaa:9,eee:5,ddd:0,bbb:3'],\n",
       " ['D', '2', 'b,f', 'bbb:7,hhh:1,aaa:6,iii:4,fff:9,ddd:5'],\n",
       " ['E', '5', 'a,c', 'fff:3,ccc:1,ggg:2,eee:5'],\n",
       " ['B', '4', 'b,f,c', 'iii:7,ggg:3,ddd:0,jjj:8,hhh:5,ccc:1'],\n",
       " ['B', '6', 'f,a,e', 'hhh:6,ccc:3,jjj:0,bbb:8,ddd:7'],\n",
       " ['D', '7', 'a,f', 'aaa:0,fff:5,ddd:3'],\n",
       " ['B', '8', 'c,a', 'ddd:5,jjj:2,iii:7,ccc:0,bbb:4'],\n",
       " ['C', '9', 'c,a,e,f', 'eee:0,fff:2,hhh:6'],\n",
       " ['E', '1', 'e,d', 'fff:9,iii:2,eee:0'],\n",
       " ['E', '5', 'f,a,d', 'hhh:8,ggg:3,jjj:5']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carga los datos\n",
    "rdd = sc.textFile(\"/tmp/data.txt\")\n",
    "\n",
    "# separa los datos por los tabuladores\n",
    "rdd = rdd.map(lambda x: x.split('\\t'))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 5, 3, 6]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extrae la segunda columna y la convierte en número\n",
    "rdd = rdd.map(lambda x: int(x[1]))\n",
    "rdd.collect()[0:5] # se imprimen los primeros cinco valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd.reduce(add) # Se reduce sumando los valores. Note que no hay parejas clave-valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# El codigo se puede reescribir como:\n",
    "#\n",
    "sc.textFile(\"/tmp/data.txt\") \\\n",
    "    .map(lambda x: x.split('\\t')) \\\n",
    "    .map(lambda x: int(x[1])) \\\n",
    "    .reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte b**\n",
    "\n",
    "Genere un RDD donde la clave sea el número de la columna 2, y el valor sean las letras correspondientes de la columna 1 del archivo original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', ['E', 'B', 'D', 'A', 'A', 'E']),\n",
       " ('9', ['A', 'B', 'E', 'C']),\n",
       " ('8', ['E', 'E', 'A', 'B']),\n",
       " ('4', ['E', 'B']),\n",
       " ('0', ['C']),\n",
       " ('2', ['A', 'E', 'D']),\n",
       " ('5', ['B', 'C', 'D', 'D', 'E', 'E', 'E']),\n",
       " ('3', ['A', 'B', 'D', 'E', 'E']),\n",
       " ('6', ['C', 'E', 'A', 'B']),\n",
       " ('7', ['A', 'C', 'E', 'D'])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga el archivo y lo separa por tabuladores\n",
    "rdd = sc.textFile(\"/tmp/data.txt\").map(lambda x: x.split('\\t'))\n",
    "\n",
    "#\n",
    "# Se forman parejas <columna 2, [columna 1]> puesto que \n",
    "# se desea agrupar por la columna 2. Esto es, \n",
    "# [ (1, E), (2, A), ..., (1, E), (5, E)]\n",
    "# Note que el segundo elemento del par es una lista.\n",
    "#\n",
    "rdd = rdd.map(lambda x: (x[1], [x[0]]))\n",
    "\n",
    "#\n",
    "rdd.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte c**\n",
    "\n",
    "Calcule la cantidad de registros por clave de la columna 4. En otras palabras, ¿cuántos registros hay que tengan la clave `aaa`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jjj', 18),\n",
       " ('ccc', 23),\n",
       " ('aaa', 13),\n",
       " ('iii', 18),\n",
       " ('eee', 15),\n",
       " ('bbb', 16),\n",
       " ('ddd', 23),\n",
       " ('ggg', 13),\n",
       " ('hhh', 16),\n",
       " ('fff', 20)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga el archivo y lo separa por tabuladores\n",
    "rdd = sc.textFile(\"/tmp/data.txt\").map(lambda x: x.split('\\t'))\n",
    "\n",
    "#\n",
    "# Se obtiene un rdd que contenga únicamente la columna 4.\n",
    "#\n",
    "#  ['jjj:3,bbb:0,ddd:9,ggg:8,hhh:2',\n",
    "#  ...\n",
    "#  'hhh:8,ggg:3,jjj:5']\n",
    "#\n",
    "rdd = rdd.map(lambda x: x[3])\n",
    "\n",
    "#\n",
    "# Se parte por la ','. flatMap() genera un nuevo registro\n",
    "# en el rdd por cada valor en la lista retornada por \n",
    "# la función lambda x: x.split(',')\n",
    "#\n",
    "#   ['jjj:3',\n",
    "#    'bbb:0', \n",
    "#    ......\n",
    "#    'ggg:3',\n",
    "#    'jjj:5']\n",
    "#\n",
    "rdd = rdd.flatMap(lambda x: x.split(','))\n",
    "\n",
    "#\n",
    "# Separa por los ':' y toma el primer elemento.\n",
    "# \n",
    "#   ['jjj',\n",
    "#    'bbb',\n",
    "#    .....\n",
    "#    'ggg',\n",
    "#    'jjj']\n",
    "#\n",
    "rdd = rdd.map(lambda x: x.split(':')[0])\n",
    "\n",
    "#\n",
    "# Se aplica el algoritmo de conteo de palabras\n",
    "#\n",
    "rdd = rdd.map(lambda s: (s, 1)).reduceByKey(add)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte d**\n",
    "\n",
    "Genere una tabla que contenga la primera columna, la cantidad de elementos en la columna 3 y la cantidad de elementos en la columna 4. La columna 4 es una lista de claves y valores separados por comas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['E', 3, 5],\n",
       " ['A', 3, 4],\n",
       " ['B', 4, 4],\n",
       " ['A', 2, 4],\n",
       " ['C', 4, 4],\n",
       " ['A', 2, 5],\n",
       " ['A', 3, 6],\n",
       " ['B', 2, 3],\n",
       " ['E', 4, 6],\n",
       " ['B', 4, 6],\n",
       " ['C', 4, 5],\n",
       " ['C', 4, 3],\n",
       " ['D', 4, 5],\n",
       " ['E', 2, 3],\n",
       " ['B', 2, 5],\n",
       " ['D', 2, 4],\n",
       " ['E', 3, 6],\n",
       " ['D', 2, 3],\n",
       " ['E', 4, 3],\n",
       " ['E', 2, 3],\n",
       " ['E', 2, 3],\n",
       " ['E', 3, 3],\n",
       " ['D', 3, 3],\n",
       " ['A', 3, 5],\n",
       " ['E', 2, 6],\n",
       " ['E', 3, 6],\n",
       " ['A', 3, 3],\n",
       " ['E', 3, 5],\n",
       " ['A', 2, 5],\n",
       " ['C', 4, 6],\n",
       " ['A', 2, 5],\n",
       " ['D', 2, 6],\n",
       " ['E', 2, 4],\n",
       " ['B', 3, 6],\n",
       " ['B', 3, 5],\n",
       " ['D', 2, 3],\n",
       " ['B', 2, 5],\n",
       " ['C', 4, 3],\n",
       " ['E', 2, 3],\n",
       " ['E', 3, 3]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga el archivo y lo separa por tabuladores\n",
    "rdd = sc.textFile(\"/tmp/data.txt\").map(lambda x: x.split('\\t'))\n",
    "\n",
    "#\n",
    "# Elimina la columna 2. Queda así:\n",
    "#\n",
    "#   [['E', 'b,g,f', 'jjj:3,bbb:0,ddd:9,ggg:8,hhh:2'],\n",
    "#    ['A', 'a,f,c', 'ccc:2,ddd:0,aaa:3,hhh:9'],\n",
    "#   ....\n",
    "#   ['E', 'e,d', 'fff:9,iii:2,eee:0'],\n",
    "#   ['E', 'f,a,d', 'hhh:8,ggg:3,jjj:5']]\n",
    "#\n",
    "rdd = rdd.map(lambda x: [x[0], x[2], x[3]])\n",
    "\n",
    "#\n",
    "# Parte las columnas 2 y 3 por la coma y cuenta los elementos\n",
    "#\n",
    "rdd = rdd.map(lambda x: [x[0], \\\n",
    "                         len(x[1].split(',')), \\\n",
    "                         len(x[2].split(','))])\n",
    "\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte e**\n",
    "\n",
    "Compute la cantidad de registros por letra de la columna 3 y clave de al columna 4; esto es, por ejemplo, la cantidad de registros en tienen la letra `a` en la columna 3 y la clave `aaa` en la columna 4 es:\n",
    "\n",
    "     ((a,aaa), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 'aaa'), 5),\n",
       " (('a', 'bbb'), 7),\n",
       " (('a', 'ccc'), 13),\n",
       " (('a', 'ddd'), 13),\n",
       " (('a', 'eee'), 7),\n",
       " (('a', 'fff'), 10),\n",
       " (('a', 'ggg'), 8),\n",
       " (('a', 'hhh'), 8),\n",
       " (('a', 'iii'), 7),\n",
       " (('a', 'jjj'), 10),\n",
       " (('b', 'aaa'), 4),\n",
       " (('b', 'bbb'), 7),\n",
       " (('b', 'ccc'), 5),\n",
       " (('b', 'ddd'), 7),\n",
       " (('b', 'eee'), 5),\n",
       " (('b', 'fff'), 8),\n",
       " (('b', 'ggg'), 4),\n",
       " (('b', 'hhh'), 7),\n",
       " (('b', 'iii'), 7),\n",
       " (('b', 'jjj'), 5),\n",
       " (('c', 'aaa'), 5),\n",
       " (('c', 'bbb'), 4),\n",
       " (('c', 'ccc'), 12),\n",
       " (('c', 'ddd'), 9),\n",
       " (('c', 'eee'), 6),\n",
       " (('c', 'fff'), 8),\n",
       " (('c', 'ggg'), 7),\n",
       " (('c', 'hhh'), 7),\n",
       " (('c', 'iii'), 8),\n",
       " (('c', 'jjj'), 8),\n",
       " (('d', 'aaa'), 4),\n",
       " (('d', 'bbb'), 6),\n",
       " (('d', 'ccc'), 7),\n",
       " (('d', 'ddd'), 5),\n",
       " (('d', 'eee'), 6),\n",
       " (('d', 'fff'), 8),\n",
       " (('d', 'ggg'), 6),\n",
       " (('d', 'hhh'), 4),\n",
       " (('d', 'iii'), 9),\n",
       " (('d', 'jjj'), 8),\n",
       " (('e', 'aaa'), 3),\n",
       " (('e', 'bbb'), 8),\n",
       " (('e', 'ccc'), 9),\n",
       " (('e', 'ddd'), 7),\n",
       " (('e', 'eee'), 7),\n",
       " (('e', 'fff'), 9),\n",
       " (('e', 'ggg'), 4),\n",
       " (('e', 'hhh'), 4),\n",
       " (('e', 'iii'), 8),\n",
       " (('e', 'jjj'), 3),\n",
       " (('f', 'aaa'), 8),\n",
       " (('f', 'bbb'), 10),\n",
       " (('f', 'ccc'), 13),\n",
       " (('f', 'ddd'), 17),\n",
       " (('f', 'eee'), 11),\n",
       " (('f', 'fff'), 11),\n",
       " (('f', 'ggg'), 9),\n",
       " (('f', 'hhh'), 10),\n",
       " (('f', 'iii'), 10),\n",
       " (('f', 'jjj'), 12),\n",
       " (('g', 'aaa'), 3),\n",
       " (('g', 'bbb'), 3),\n",
       " (('g', 'ccc'), 6),\n",
       " (('g', 'ddd'), 5),\n",
       " (('g', 'eee'), 4),\n",
       " (('g', 'fff'), 5),\n",
       " (('g', 'ggg'), 4),\n",
       " (('g', 'hhh'), 3),\n",
       " (('g', 'iii'), 4),\n",
       " (('g', 'jjj'), 6)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga el archivo y lo separa por tabuladores\n",
    "rdd = sc.textFile(\"/tmp/data.txt\").map(lambda x: x.split('\\t'))\n",
    "\n",
    "#\n",
    "# Elimina las dos primeras columnas. \n",
    "# \n",
    "# Resultado:\n",
    "#\n",
    "#  [['b,g,f', 'jjj:3,bbb:0,ddd:9,ggg:8,hhh:2'],\n",
    "#   ['a,f,c', 'ccc:2,ddd:0,aaa:3,hhh:9'],\n",
    "#   ...\n",
    "#   ['e,d', 'fff:9,iii:2,eee:0'],\n",
    "#   ['f,a,d', 'hhh:8,ggg:3,jjj:5']]\n",
    "#\n",
    "rdd = rdd.map(lambda x: [x[2], x[3]])\n",
    "\n",
    "#\n",
    "# Expande el dataset generando un registro por cada \n",
    "# elemento de la columna 1.\n",
    "#\n",
    "# Resultado:\n",
    "#\n",
    "#  [['b', 'jjj:3,bbb:0,ddd:9,ggg:8,hhh:2'],\n",
    "#   ['g', 'jjj:3,bbb:0,ddd:9,ggg:8,hhh:2'],\n",
    "#  ...\n",
    "#   ['a', 'hhh:8,ggg:3,jjj:5'],\n",
    "#   ['d', 'hhh:8,ggg:3,jjj:5']]\n",
    "#\n",
    "rdd = rdd.flatMap(lambda x: [[e, x[1]]  for e in x[0].split(',')])\n",
    "\n",
    "#\n",
    "# Cambia la segunda columna por una lista \n",
    "# partiendo los elementos por las comas\n",
    "##\n",
    "# Resultado:\n",
    "#\n",
    "#  [['b', ['jjj:3', 'bbb:0', 'ddd:9', 'ggg:8', 'hhh:2']],\n",
    "#   ['g', ['jjj:3', 'bbb:0', 'ddd:9', 'ggg:8', 'hhh:2']],\n",
    "#   ....\n",
    "#   ['a', ['hhh:8', 'ggg:3', 'jjj:5']],\n",
    "#   ['d', ['hhh:8', 'ggg:3', 'jjj:5']]]\n",
    "#\n",
    "rdd = rdd.map(lambda x: [x[0], x[1].split(',')])\n",
    "\n",
    "#\n",
    "# Parte cada elemento por los ':' y toma el primer elemento\n",
    "#\n",
    "# Resultado:\n",
    "#\n",
    "#    [['b', ['jjj', 'bbb', 'ddd', 'ggg', 'hhh']],\n",
    "#     ['g', ['jjj', 'bbb', 'ddd', 'ggg', 'hhh']],\n",
    "#     ...\n",
    "#     ['a', ['hhh', 'ggg', 'jjj']],\n",
    "#     ['d', ['hhh', 'ggg', 'jjj']]]\n",
    "#\n",
    "rdd = rdd.map(lambda x: [x[0], [a.split(':')[0] for a in x[1] ]])\n",
    "\n",
    "#\n",
    "# Genera las parejas clave valor.\n",
    "# \n",
    "# Resultado:\n",
    "#\n",
    "#   [(('b', 'jjj'), 1),\n",
    "#    (('b', 'bbb'), 1),\n",
    "#    ...\n",
    "#    (('d', 'ggg'), 1),\n",
    "#    (('d', 'jjj'), 1)]\n",
    "#\n",
    "rdd = rdd.flatMap(lambda x: [((x[0], a), 1)  for a in x[1]])\n",
    "\n",
    "#\n",
    "# Reduce por clave para hacer el conteo.\n",
    "# Se usa sortByKey() para ordenas la salida\n",
    "# por claves\n",
    "#\n",
    "rdd.reduceByKey(add).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpieza de la carpeta de trabajo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/wordcount\n",
      "Deleted /tmp/data.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r -f /tmp/wordcount\n",
    "!hdfs dfs -rm /tmp/data.txt\n",
    "!hdfs dfs -ls /tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
