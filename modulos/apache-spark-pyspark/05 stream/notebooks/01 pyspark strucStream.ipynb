{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordCount usando Structured Streaming\n",
    "===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tutorial es adaptado de https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tutorial introduce el uso de Spark para el manejo de streamings estructurados. Específicamente, se presenta un ejemplo de conteo de letras que es realizado a medida que estas son introducidas por un usuario desde el teclado en una consola. \n",
    "\n",
    "Al finalizar este tutorial, usted estará en capacidad de:\n",
    "\n",
    "* Explicar los fundamentos del procesamiento de datos en streaming usando Spark.\n",
    "\n",
    "* Escribir programas cortos para procesar datos.\n",
    "\n",
    "* Aplicar operaciones básicas en Spark para manejo de tablas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea realizar el conteo de frecuencia de palabras en tiempo real, a partir de frases o letras digitas obtenidas desde un puerto en el sistema operativo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de palabras en PySpark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se revisará como contar palabras de la forma tradicional usando SparkSQL, las cuales están almacenadas y representadas como letras en el siguiente archivo de texto: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting words.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile words.txt\n",
    "value\n",
    "A B C A\n",
    "A B A A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/words.txt\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se copia el archivo al HDFS\n",
    "##\n",
    "!hdfs dfs -rm -f /tmp/words.txt\n",
    "!hdfs dfs -copyFromLocal words.txt /tmp/words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Se inicia la aplicación en PySpark\n",
    "##\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkConf = SparkConf().setAppName(\"My SparkQL Application\") \n",
    "sc = SparkContext(conf=sparkConf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|A B C A|\n",
      "|A B A A|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se lee el archivo del hdfs en formato CSV.\n",
    "## Cada fila del DataFrame es un renglón del archivo\n",
    "##\n",
    "df = spark.read.load(\n",
    "    \"/tmp/words.txt\",\n",
    "    format=\"csv\", \n",
    "    sep=\",\", \n",
    "    inferSchema=\"true\", \n",
    "    header=\"true\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "|   A|\n",
      "|   B|\n",
      "|   C|\n",
      "|   A|\n",
      "|   A|\n",
      "|   B|\n",
      "|   A|\n",
      "|   A|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "##\n",
    "## La función split parte cada línea de texto por los espacios en\n",
    "## blanco, retornando un vector; por ejemplo, para la primera  \n",
    "## línea retorna ['A', 'B', 'C', 'A']. Seguidamente, la función\n",
    "## explode genera un registro por cada elemento del vector, tal\n",
    "## como se muestra a continuación.\n",
    "##\n",
    "words = df.select(\n",
    "   explode(\n",
    "       split(df.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|   B|    2|\n",
      "|   C|    1|\n",
      "|   A|    5|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Para realizar el conteo propiamente, se realizar un \n",
    "## groupBy por letra, y se cuenta la cantidad de registros\n",
    "## por grupo usando la función `count`.\n",
    "##\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "wordCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Se finaliza la aplicación\n",
    "##\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de palabras usando Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se realizar el conteo de palabras leyendo los datos desde un streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las diferencias con el ejemplo anterior están comentadas en el código. Note que se está creando un archivo, con el fin de ejecutar el programa por fuera de Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wc-pyspark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wc-pyspark.py\n",
    "\n",
    "## Identico ------>>>\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## <<<------\n",
    "\n",
    "\n",
    "## \n",
    "## Los datos se leen desde un flujo de entrada en vez de un archivo \n",
    "## en disco. Para ello, se crea un DataFrame que representa las líneas \n",
    "## de texto de entrada, las cuales son leídas desde una conexión a \n",
    "## localhost:9999. El flujo de datos puede considerarse como un DataFrame\n",
    "## infinito, donde los nuevos datos se van adicionando al final.\n",
    "##\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "## Identico ------>>>\n",
    "\n",
    "words = df.select(\n",
    "   explode(\n",
    "       split(df.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "## <<<------\n",
    "\n",
    "\n",
    "##\n",
    "## Crea un stream de salida a la consola, en la que se van\n",
    "## escribiendo los resultados a medida que se van ingresando\n",
    "## datos.\n",
    "##\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución de la aplicación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar el programa se requieren dos consolas; en la primera se ejecuta el comando `nc -lk 9999`, el cual crea un servidor de datos interactivo que usa el puerto 9999 (que es el mismo puerto que está escuchando la aplicación de conteo de palabras). En la segunda consola, se ejecuta la aplicación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrá un Terminal y ejecute el comando `nc -lk 9999`. El Terminal quedará en espera para que ingrese texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abra otro Terminal y ejecute el comando `spark-submit wc-pyspark.py` para lanzar la aplicación de conteo de palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su pantalla debe mostrar una salida similar a la siguiente:\n",
    "\n",
    "![assets/pyspark-wc-spark.py-1.jpg](assets/pyspark-wc-spark.py-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digite `A B C A` y luego la tecla enter; después de un momento, la aplicación reportará el conteo preliminar de letras:\n",
    "\n",
    "![assets/pyspark-wc-spark.py-2.jpg](assets/pyspark-wc-spark.py-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digite la segunda línea de texto `A B A A`; luego aparecerá el conteo actualizado de letras.\n",
    "\n",
    "![assets/pyspark-wc-spark.py-3.jpg](assets/pyspark-wc-spark.py-3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalice las aplicaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programa equivalente en SparkR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se presenta el código equivalente en SparkR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wc-sparkR.R\n"
     ]
    }
   ],
   "source": [
    "%%writefile wc-sparkR.R\n",
    "\n",
    "library(SparkR)\n",
    "sparkR.session()\n",
    "\n",
    "df <- read.stream(\n",
    "    \"socket\", \n",
    "    host = \"localhost\", \n",
    "    port = 9999)\n",
    "\n",
    "words <- selectExpr(\n",
    "    df, \n",
    "    \"explode(split(value, ' ')) as word\")\n",
    "\n",
    "wordCounts <- count(group_by(words, \"word\"))\n",
    "\n",
    "query <- write.stream(\n",
    "    wordCounts, \n",
    "    \"console\", \n",
    "    outputMode = \"complete\")\n",
    "\n",
    "awaitTermination(query)\n",
    "\n",
    "sparkR.session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar el programa en R, use `spark-submit wc-sparkR.R` en el Paso 2 del ejemplo presentado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción del modelo de programación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se describe en detalle el modelo de programación con base en el ejemplo presentado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flujo de datos de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El flujo de datos de entrada (`readStream`), creado en la siguiente porción de código:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "puede interpretarse como un DataFrame infinito, donde los nuevos datos se van agragando al final de la tabla, tal como se indica a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![assets/data-stream.jpg](assets/data-stream.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones sobre el flujo de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las operaciones realizadas en el DataFrame son similares a las usadas en los DataFrames estáticos, tal como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "words = df.select(\n",
    "   explode(\n",
    "       split(df.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con la única diferencia que Spark sigue revisando el flujo de entrada para determinar si existen más datos. En este modelo computacional, Spark combina de forma automática los resultados de un instante anterior, con los obtenidos para los nuevos datos procesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipos de entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark usa cuatro tipos de fuentes de entrada:\n",
    "\n",
    "* **File source**: Spark lee los archivos que se van colocando en un directorio como un stream de datos usando el objeto `DataStreamReader`. Los archivos pueden tener formato csv, json, orc o parquet. \n",
    "\n",
    "* **Kafka source**: Lee datos generados mediante Apache Kafka.\n",
    "\n",
    "* **Socket source**: Lee texto desde un puerto (socket) y es usado únicamente para pruebas. Esta es la fuente usada en el ejemplo presentado.\n",
    "\n",
    "* **Rate source**: Genera datos a un número específico de filas por segundo, donde cada fila tiene un campo de tiempo (`timestamp`) y un campo valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flujo de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente porción de código define como se escribirán los resultados (`outputMode`) en el flujo de salida (`writeStream`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **complete**: La tabla resultante  `wordCounts` es escrita completamente en la consola cuando es actualizada. Esta es el tipo de salida usada en el ejemplo presentado.\n",
    "\n",
    "* **append**: Solo se escriben las nuevas filas que se van generando en la tabla resultante `wordCounts`. Este modo se usa sólo cuando las filas creadas no cambian posteriormente.\n",
    "\n",
    "* **update**: Se escriben solo las filas que se actualiza en la tabla de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formato de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark proporciona los siguientes formatos de salida:\n",
    "\n",
    "* **File sink**: Los resultados son escritos a un archivo en formato csv, orc, json o parquet; por ejemplo:\n",
    "\n",
    "```python\n",
    "writeStream\n",
    "    .format(\"parquet\") \n",
    "    .option(\"path\", \"path/to/destination/dir\")\n",
    "    .start()\n",
    "```\n",
    "\n",
    "* **Kafka sink**: Almacena el resultado usando Apache Kafka.\n",
    "\n",
    "* **Foreach sink**: Realiza cómputos arbitrarios sobre los registros de la salida.\n",
    "\n",
    "* **Console sink**: Esta es una salida usada únicamente para depuración y permite la escritura del DataFrame de salida en el Terminal. Este es el tipo de salida usado en el ejemplo desarrollado.\n",
    "\n",
    "```python\n",
    "writeStream\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    "```\n",
    "\n",
    "* **Memory sink**: El resultado es almacenado como una tabla en memoria, y se usa únicamente en la depuración de programas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definen el intervalo de procesamiento del stream de datos.\n",
    "\n",
    "* **Unspecified**: Este el el intervalo por defecto. El stream es procesado por microlotes, donde cada microlote es procesado tan pronto como el microlote anterior es terminado. Un microlote está conformado por un grupo de registros a la vez. Por ejemplo:\n",
    "\n",
    "```python\n",
    "## En PySpark\n",
    "df.writeStream \\\n",
    "  .format(\"console\") \\\n",
    "  .start()\n",
    "```\n",
    "\n",
    "```R\n",
    "## En SparkR\n",
    "write.stream(df, \"console\")\n",
    "```\n",
    "\n",
    "* **Fixed interval micro-batches**: Los microlotes están conformados por los registros recolectados en un intervalo fijo de tiempo, por ejemplo, todos los registros que llegan en un intervalo de dos segundos:\n",
    "\n",
    "```python\n",
    "## En PySpark\n",
    "df.writeStream \\\n",
    "  .format(\"console\") \\\n",
    "  .trigger(processingTime='2 seconds') \\\n",
    "  .start()\n",
    "```    \n",
    "\n",
    "```R\n",
    "## En SparkR\n",
    "write.stream(df, \"console\", trigger.processingTime = \"2 seconds\")\n",
    "```\n",
    "\n",
    "\n",
    "* **One-time micro-batch**: El proceso es ejecutado una sola vez, con todos los registros disponibles. Se usa cuando el procesamiento debe realizarse periodicamente. El codigo correspondiente en Python sería:\n",
    "\n",
    "```python\n",
    "## En PySpark\n",
    "df.writeStream \\\n",
    "  .format(\"console\") \\\n",
    "  .trigger(once=True) \\\n",
    "  .start()\n",
    "```\n",
    "\n",
    "```R\n",
    "## En SparkR\n",
    "write.stream(df, \"console\", trigger.once = TRUE)\n",
    "```\n",
    "\n",
    "\n",
    "* **Continuous with fixed checkpoint interval (experimental)**: El proceso es ejecutado a intervalos muy bajos de tiempo, simulando un proceso continuo.\n",
    "\n",
    "```python\n",
    "## En PySpark\n",
    "df.writeStream\n",
    "  .format(\"console\")\n",
    "  .trigger(continuous='1 second')\n",
    "  .start()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
