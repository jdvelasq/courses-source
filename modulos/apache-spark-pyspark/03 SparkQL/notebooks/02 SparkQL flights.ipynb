{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis de datos usando  PySpark SQL\n",
    "===\n",
    "\n",
    "* Última modificación: Junio 22, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se presenta el análisis de una base de datos sobre vuelos usando SQL en Spark desde la interfaz de Python. En este documento se ejemplifica el uso de DataFrames para la realización de consultas usando sus funciones nativas, como también el envio de comandos SQL. Adicionalmente, se demuestra como salvar los resultados al HDFS en distintos formatos. \n",
    "\n",
    "Al finalizar este documento, el lector estará en capacidad de:\n",
    "\n",
    "* Mover archivos entre el HDFS y el sistema local.\n",
    "\n",
    "* Importar tablas en formato CSV a PySpark.\n",
    "\n",
    "* Aplicar operadores de selección, filtrado y agregación desde Python.\n",
    "\n",
    "* Usar los resultados obtenidos para construir gráficos.\n",
    "\n",
    "* Exportar los resultados a archivos en el sistema HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo está basado en el tutorial de Spark de HortoWorks, disponible en https://es.hortonworks.com/tutorial/learning-spark-sql-with-zeppelin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripción de los campos del archivo\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo usado contiene la información sobre vuelos entre 1987 y 2008, y cuenta con los siguientes campos:\n",
    "\n",
    "* Year: 1987-2008\n",
    "\n",
    "* Month: 1-12\n",
    "\n",
    "* DayofMonth: 1-31\n",
    "\n",
    "* DayOfWeek: 1 (Monday) - 7 (Sunday)\n",
    "\n",
    "* DepTime: actual departure time (local, hhmm)\n",
    "\n",
    "* CRSDepTime: scheduled departure time (local, hhmm)\n",
    "\n",
    "* ArrTime: actual arrival time (local, hhmm)\n",
    "\n",
    "* CRSArrTime: scheduled arrival time (local, hhmm)\n",
    "\n",
    "* UniqueCarrier: unique carrier code\n",
    "\n",
    "* FlightNum: flight number\n",
    "\n",
    "* TailNum: plane tail number\n",
    "\n",
    "* ActualElapsedTime: in minutes\n",
    "\n",
    "* CRSElapsedTime: in minutes\n",
    "\n",
    "* AirTime: in minutes\n",
    "\n",
    "* ArrDelay: arrival delay, in minutes\n",
    "\n",
    "* DepDelay: departure delay, in minutes\n",
    "\n",
    "* Origin: origin IATA airport code\n",
    "\n",
    "* Dest: destination IATA airport code\n",
    "\n",
    "* Distance: in miles\n",
    "\n",
    "* TaxiIn: taxi in time, in minutes\n",
    "\n",
    "* TaxiOut: taxi out time in minutes\n",
    "\n",
    "* Cancelled: was the flight cancelled?\n",
    "\n",
    "* CancellationCode: reason for cancellation (A = carrier, B = weather, C = NAS, D = security)\n",
    "\n",
    "* Diverted: 1 = yes, 0 = no\n",
    "\n",
    "* CarrierDelay: in minutes\n",
    "\n",
    "* WeatherDelay: in minutes\n",
    "\n",
    "* NASDelay: in minutes\n",
    "\n",
    "* SecurityDelay: in minutes\n",
    "\n",
    "* LateAircraftDelay: in minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "sparkConf = SparkConf().setAppName(\"Flights SparkQL Application\") \n",
    "sc = SparkContext(conf=sparkConf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-15 00:51:18--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/flights.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9719482 (9.3M) [text/plain]\n",
      "Saving to: 'flights.csv.1'\n",
      "\n",
      "flights.csv.1       100%[===================>]   9.27M  1.47MB/s    in 13s     \n",
      "\n",
      "2019-11-15 00:51:34 (721 KB/s) - 'flights.csv.1' saved [9719482/9719482]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/flights.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copia de archivos al HDFS\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup    9719482 2019-11-15 00:51 /tmp/flights.csv\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# El archivo flights.csv se encuentra en la capeta de\n",
    "# trabajo de la máquina local. Se copia el archivo \n",
    "# a la carpeta /tmp del sistema HDFS.\n",
    "#\n",
    "!hdfs dfs -copyFromLocal flights.csv /tmp/\n",
    "\n",
    "#\n",
    "# Se listan los archivos en la carpeta /tmp del HDFS\n",
    "# para verificar que el archivo haya sido copiado\n",
    "#\n",
    "!hdfs dfs -ls /tmp/*csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de datos en Spark\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Crea un DataFrame a partir del archivo fligths.csv\n",
    "#\n",
    "flights = spark.read.load(\"/tmp/flights.csv\",\n",
    "                          format=\"csv\", \n",
    "                          sep=\",\", \n",
    "                          inferSchema=\"true\", \n",
    "                          header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se imprime el esquema para verificar la lectura\n",
    "# del archivo.\n",
    "#\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+--------+--------+--------+\n",
      "|UniqueCarrier|FlightNum|DepDelay|ArrDelay|Distance|\n",
      "+-------------+---------+--------+--------+--------+\n",
      "|           WN|      335|       8|     -14|     810|\n",
      "|           WN|     3231|      19|       2|     810|\n",
      "|           WN|      448|       8|      14|     515|\n",
      "|           WN|     1746|      -4|      -6|     515|\n",
      "|           WN|     3920|      34|      34|     515|\n",
      "+-------------+---------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Se imprime un subconjunto de las columnas para verificar\n",
    "# la lectura\n",
    "#\n",
    "flights.select(['UniqueCarrier', 'FlightNum', 'DepDelay', 'ArrDelay', 'Distance']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Número total de registros leidos\n",
    "#\n",
    "numTotalFlights = flights.count()\n",
    "numTotalFlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculos usando funciones de los DataFrames\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cómputo del porcentaje de vuelos retrasados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|UniqueCarrier|DepDelay|\n",
      "+-------------+--------+\n",
      "|           WN|      19|\n",
      "|           WN|      34|\n",
      "|           WN|      25|\n",
      "|           WN|      67|\n",
      "|           WN|      94|\n",
      "+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# La variable delayedFlights contiene las columnas UniqueCarrier y DepDelay \n",
    "# para los vuelos con DepDelay > 15 minutos.\n",
    "#\n",
    "delayedFlights = flights.select(['UniqueCarrier', 'DepDelay']).filter(flights['DepDelay'] > 15)\n",
    "delayedFlights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de vuelos retrasados: 19.58719587195872%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Porcentaje de vuelos retrasados.\n",
    "#\n",
    "numDelayedFlights = delayedFlights.count()\n",
    "print(\"Porcentaje de vuelos retrasados: \" + str(numDelayedFlights / numTotalFlights * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creación de variables usando funciones de usuario**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se desea crear una nueva columna llamada `IsDelayed` que vale 0 si el vuelo se realizó a tiempo y 1 si se retraso. Ya que la nueva columna, es computada como función de otra, se una una función de usuario (o udf) programada en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Esta función se aplicará sobre el DataFrame.\n",
    "# El parámetro time será la columna DepDelay\n",
    "#\n",
    "def is_delayed_py(time):\n",
    "    if time == \"NA\":\n",
    "        return 0 \n",
    "    elif int(time) > 15:\n",
    "        return 1 \n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se registra la función en Spark y se aplica sobre el Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|DepDelay|isDelayed|\n",
      "+--------+---------+\n",
      "|       8|        0|\n",
      "|      19|        1|\n",
      "|       8|        0|\n",
      "|      -4|        0|\n",
      "|      34|        1|\n",
      "|      25|        1|\n",
      "|      67|        1|\n",
      "|      -1|        0|\n",
      "|       2|        0|\n",
      "|       0|        0|\n",
      "|       6|        0|\n",
      "|      94|        1|\n",
      "|      -4|        0|\n",
      "|       0|        0|\n",
      "|       2|        0|\n",
      "|       9|        0|\n",
      "|      27|        1|\n",
      "|       9|        0|\n",
      "|      28|        1|\n",
      "|      51|        1|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Importa la función udf que permite registra funciones \n",
    "# escritas en Python dentro de Spark\n",
    "# \n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "#\n",
    "# Se registra la función con el tipo de dato que devuelve.\n",
    "# is_delayed_udf corresponde a la función registrada en Spark.\n",
    "# Adicionalmente, se indica el tipo de dato que devuelve.\n",
    "#\n",
    "is_delayed_udf = udf(is_delayed_py, LongType())\n",
    "\n",
    "#\n",
    "# Se crea un nuevo DataFrame que contiene la columna \n",
    "# IsDelayed, la cual es computada con la udf\n",
    "#\n",
    "flightsWithDelays = flights.select('Year', \n",
    "                                   'Month', \n",
    "                                   'DayofMonth', \n",
    "                                   'UniqueCarrier', \n",
    "                                   'FlightNum', \n",
    "                                   'DepDelay',\n",
    "                                   is_delayed_udf(\"DepDelay\").alias(\"IsDelayed\"))\n",
    "\n",
    "#\n",
    "# Se imprimen algunos registros para verificar el resultado.\n",
    "#\n",
    "flightsWithDelays.select('DepDelay', 'isDelayed').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|Porcentaje de vuelos retrasados|\n",
      "+-------------------------------+\n",
      "|              19.58719587195872|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# A continuación, se calcula el porcentaje de vuelos con retrasos\n",
    "#\n",
    "from pyspark.sql import functions as F\n",
    "flightsWithDelays.agg((F.sum('IsDelayed') * 100 / F.count('DepDelay')) \\\n",
    "                      .alias(\"Porcentaje de vuelos retrasados\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cómputo de los tiempos promedio de despegue (Taxi-in)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------------------+\n",
      "|Origin|Dest|         AvgTaxiIn|\n",
      "+------+----+------------------+\n",
      "|   CLT| IAH|              22.0|\n",
      "|   IAH| ABQ|              18.0|\n",
      "|   MCI| IAH|14.666666666666666|\n",
      "|   BHM| EWR|              13.0|\n",
      "|   SMF| GEG|12.462962962962964|\n",
      "|   MHT| CLE|              12.0|\n",
      "|   CRW| IAH|              12.0|\n",
      "|   IAH| JAX|              11.0|\n",
      "|   ONT| COS|10.903225806451612|\n",
      "|   SMF| COS|10.610169491525424|\n",
      "+------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# En este código se ilustra como computar un promedio \n",
    "# y luego realizar el ordenamiento de la tabla por\n",
    "# ese mismo promedio.\n",
    "#\n",
    "(flights.select(\"Origin\", \"Dest\", \"TaxiIn\") \\\n",
    "    .groupBy(\"Origin\", \"Dest\") \\\n",
    "    .agg(F.avg(\"TaxiIn\").alias(\"AvgTaxiIn\"))) \\\n",
    "        .orderBy(\"AvgTaxiIn\", ascending = False) \\\n",
    "        .show(10)                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cómputo de los tiempos promedio de aterrizaje**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+\n",
      "|Origin|Dest|AvgTaxiOut|\n",
      "+------+----+----------+\n",
      "|   LCH| IAH|      84.0|\n",
      "|   EWR| BHM|      63.0|\n",
      "|   EWR| SDF|      45.0|\n",
      "|   EWR| GSO|      36.5|\n",
      "|   MHT| CLE|      33.0|\n",
      "|   EWR| JAX|      28.0|\n",
      "|   EWR| DTW|      27.0|\n",
      "|   CLE| SDF|      27.0|\n",
      "|   ORD| EWR|      26.0|\n",
      "|   EWR| MCI|      26.0|\n",
      "+------+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# En este código se ilustra como computar un promedio \n",
    "# y luego realizar el ordenamiento de la tabla por\n",
    "# ese mismo promedio.\n",
    "#\n",
    "(flights.select(\"Origin\", \"Dest\", \"TaxiOut\") \\\n",
    "    .groupBy(\"Origin\", \"Dest\") \\\n",
    "    .agg(F.avg(\"TaxiOut\").alias(\"AvgTaxiOut\"))) \\\n",
    "        .orderBy(\"AvgTaxiOut\", ascending = False) \\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómputos usando SQL\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se realizan los mismos cálculos anteriores, pero usando SQL desde Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Se crea la tabla\n",
    "#\n",
    "flights.createOrReplaceTempView('flightsView')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|DepDelay|isDelayed|\n",
      "+--------+---------+\n",
      "|       8|        0|\n",
      "|      19|        1|\n",
      "|       8|        0|\n",
      "|      -4|        0|\n",
      "|      34|        1|\n",
      "|      25|        1|\n",
      "|      67|        1|\n",
      "|      -1|        0|\n",
      "|       2|        0|\n",
      "|       0|        0|\n",
      "|       6|        0|\n",
      "|      94|        1|\n",
      "|      -4|        0|\n",
      "|       0|        0|\n",
      "|       2|        0|\n",
      "|       9|        0|\n",
      "|      27|        1|\n",
      "|       9|        0|\n",
      "|      28|        1|\n",
      "|      51|        1|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se registra la función para usar con Spark SQL\n",
    "# \n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Registra la función.\n",
    "spark.udf.register('isDelayed_SQL', is_delayed_py)\n",
    "\n",
    "# Aplica la función\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DepDelay, \n",
    "    isDelayed_SQL(DepDelay) as isDelayed \n",
    "FROM \n",
    "    flightsView\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|UniqueCarrier|NumDelays|\n",
      "+-------------+---------+\n",
      "|           XE|   1014.0|\n",
      "|           WN|  18573.0|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Numero total de retrasos por transportador\n",
    "#\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    UniqueCarrier, \n",
    "    SUM(isDelayed_SQL(DepDelay)) AS NumDelays \n",
    "FROM \n",
    "    flightsView \n",
    "GROUP BY \n",
    "    UniqueCarrier\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    UniqueCarrier, \n",
    "    SUM(isDelayed_SQL(DepDelay)) AS NumDelays \n",
    "FROM \n",
    "    flightsView \n",
    "GROUP BY \n",
    "    UniqueCarrier\n",
    "\"\"\").toPandas().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|UniqueCarrier|TotalTimeDelay|\n",
      "+-------------+--------------+\n",
      "|           XE|       47502.0|\n",
      "|           WN|      978547.0|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Tiempo total de retrasos por transportador\n",
    "#\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    UniqueCarrier, \n",
    "    SUM(DepDelay) AS TotalTimeDelay \n",
    "FROM \n",
    "    flightsView \n",
    "GROUP BY \n",
    "    UniqueCarrier\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|UniqueCarrier|AvgDistanceTraveled|\n",
      "+-------------+-------------------+\n",
      "|           XE|  738.0462651413189|\n",
      "|           WN|  623.7926638668864|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Distancia recorrida por operador\n",
    "#\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    UniqueCarrier, \n",
    "    avg(Distance) AS AvgDistanceTraveled \n",
    "FROM \n",
    "    flightsView \n",
    "GROUP BY \n",
    "    UniqueCarrier \n",
    "ORDER BY \n",
    "    AvgDistanceTraveled DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrasos por día de la semana**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+\n",
      "|DayOfWeek|delayed|Count|\n",
      "+---------+-------+-----+\n",
      "|        1|      0|11863|\n",
      "|        1|      1| 2656|\n",
      "|        2|      1| 1799|\n",
      "|        2|      0|12910|\n",
      "|        3|      1| 1434|\n",
      "|        3|      0|13260|\n",
      "|        4|      1| 4808|\n",
      "|        4|      0|12271|\n",
      "|        5|      1| 3514|\n",
      "|        5|      0|11003|\n",
      "|        6|      1| 1878|\n",
      "|        6|      0| 9407|\n",
      "|        7|      0| 9698|\n",
      "|        7|      1| 3498|\n",
      "+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DayOfWeek, \n",
    "    delayed,\n",
    "    COUNT(1) AS Count\n",
    "FROM\n",
    "    (SELECT\n",
    "        DayOfWeek,\n",
    "        isDelayed_SQL(DepDelay) AS delayed\n",
    "     FROM\n",
    "        flightsView)\n",
    "GROUP BY \n",
    "    DayOfWeek, \n",
    "    delayed\n",
    "ORDER BY \n",
    "    DayOfWeek\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrasos por hora del día**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|Hour|delayed|Count|\n",
      "+----+-------+-----+\n",
      "|   6|      1|  208|\n",
      "|   6|      0| 6126|\n",
      "|   7|      0| 7274|\n",
      "|   7|      1|  372|\n",
      "|   8|      1|  547|\n",
      "|   8|      0| 5956|\n",
      "|   9|      1|  761|\n",
      "|   9|      0| 5861|\n",
      "|  10|      0| 5783|\n",
      "|  10|      1|  903|\n",
      "|  11|      0| 5115|\n",
      "|  11|      1|  986|\n",
      "|  12|      1| 1128|\n",
      "|  12|      0| 5174|\n",
      "|  13|      1| 1346|\n",
      "|  13|      0| 5225|\n",
      "|  14|      0| 4434|\n",
      "|  14|      1| 1335|\n",
      "|  15|      0| 4818|\n",
      "|  15|      1| 1562|\n",
      "+----+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    Hour,\n",
    "    delayed, \n",
    "    COUNT(1) AS Count\n",
    "FROM \n",
    "(\n",
    "    SELECT\n",
    "        CAST(CRSDepTime / 100 AS INT) AS Hour, \n",
    "        isDelayed_SQL(DepDelay) AS delayed\n",
    "    FROM\n",
    "        flightsView\n",
    ")\n",
    "GROUP BY \n",
    "    Hour, \n",
    "    delayed\n",
    "ORDER BY \n",
    "    Hour\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almacenamiento y lectura de tablas calculadas\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Escritura de resultados en el HDFS con formato ORC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Se salva la tabla calculada al directorio tmp del HDFS.\n",
    "# Primero se borra si existe.\n",
    "#\n",
    "!hdfs dfs -rm -r -f /tmp/flightsWithDelays.orc\n",
    "\n",
    "# Se salva en formato ORC\n",
    "flightsWithDelays.write.format(\"orc\").save(\"/tmp/flightsWithDelays.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   1 root supergroup          0 2019-11-15 00:52 /tmp/flightsWithDelays.orc/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup      89140 2019-11-15 00:52 /tmp/flightsWithDelays.orc/part-00000-86363a17-4ec0-4c5d-8fe1-2bb6d54d18f0-c000.snappy.orc\n",
      "-rw-r--r--   1 root supergroup     104943 2019-11-15 00:52 /tmp/flightsWithDelays.orc/part-00001-86363a17-4ec0-4c5d-8fe1-2bb6d54d18f0-c000.snappy.orc\n",
      "-rw-r--r--   1 root supergroup      34305 2019-11-15 00:52 /tmp/flightsWithDelays.orc/part-00002-86363a17-4ec0-4c5d-8fe1-2bb6d54d18f0-c000.snappy.orc\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Contenido del directorio donde se salvó la tabla\n",
    "#\n",
    "!hdfs dfs -ls /tmp/flightsWithDelays.orc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carga de los resultados desde el HDFS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Se lee la tabla calculada desde el HDFS\n",
    "#\n",
    "test = spark.read.format(\"orc\").load(\"/tmp/flightsWithDelays.orc\")\n",
    "\n",
    "# verifica la cantidad de registros.\n",
    "assert test.count() == flightsWithDelays.count(), print(\"Archivos con diferentes tamaños.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salva el DataFrame como una tabla permamente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# La tabla queda guardada en la carpeta\n",
    "# spark-warehouse del directorio actual\n",
    "#\n",
    "!rm -rf spark-warehouse/flightswithdelaystbl\n",
    "flightsWithDelays.write.format(\"orc\").saveAsTable(\"flightswithdelaystbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tablas almacenadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|flightswithdelaystbl|      false|\n",
      "|        |         flightsview|       true|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consultas en un tabla permanente\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Total|\n",
      "+-----+\n",
      "|99999|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Note que cuando la tabla está almacenada de forma\n",
    "# permanente no es necesario cargarla a la memoria\n",
    "# para poder usarla.\n",
    "#\n",
    "spark.sql(\"SELECT COUNT(1) AS Total from flightswithdelaystbl\").show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "|        |flightsview|       true|\n",
      "+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se borran las tablas para limpiar el área\n",
    "# de trabajo.\n",
    "spark.sql(\"DROP TABLE flightswithdelaystbl\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpieza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tmp/flights.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /tmp/flights.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'flights*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm flights* data.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
