{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Simple audio recognition: Recognizing keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-rayb7-3Y0I"
   },
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('data/mini_speech_commands')\n",
    "if not data_dir.exists():\n",
    "  tf.keras.utils.get_file(\n",
    "      'mini_speech_commands.zip',\n",
    "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70IBxSKxA1N9"
   },
   "outputs": [],
   "source": [
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "commands = commands[commands != 'README.md']\n",
    "print('Commands:', commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlX685l1wD9k"
   },
   "outputs": [],
   "source": [
    "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "num_samples = len(filenames)\n",
    "print('Number of total examples:', num_samples)\n",
    "print('Number of examples per label:',\n",
    "      len(tf.io.gfile.listdir(str(data_dir/commands[0]))))\n",
    "print('Example file tensor:', filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cv_wts-l3KgD"
   },
   "outputs": [],
   "source": [
    "train_files = filenames[:6400]\n",
    "val_files = filenames[6400: 6400 + 800]\n",
    "test_files = filenames[-800:]\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PjJ2iXYwftD"
   },
   "outputs": [],
   "source": [
    "def decode_audio(audio_binary):\n",
    "  audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "  return tf.squeeze(audio, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VTtX1nr3YT-"
   },
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "\n",
    "  # Note: You'll use indexing here instead of tuple unpacking to enable this \n",
    "  # to work in a TensorFlow graph.\n",
    "  return parts[-2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8Y9w_5MOsr-"
   },
   "source": [
    "Let's define a method that will take in the filename of the WAV file and output a tuple containing the audio and labels for supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdgUD5T93NyT"
   },
   "outputs": [],
   "source": [
    "def get_waveform_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  audio_binary = tf.io.read_file(file_path)\n",
    "  waveform = decode_audio(audio_binary)\n",
    "  return waveform, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvN8W_dDjYjc"
   },
   "source": [
    "You will now apply `process_path` to build your training set to extract the audio-label pairs and check the results. You'll build the validation and test sets using a similar procedure later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SQl8yXl3kNP"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voxGEwvuh2L7"
   },
   "source": [
    "Let's examine a few audio waveforms with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yuX6Nqzf6wT"
   },
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
    "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  ax.plot(audio.numpy())\n",
    "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  ax.set_title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4CK75DHz_OR"
   },
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "  # Padding for files with less than 16000 samples\n",
    "  zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
    "\n",
    "  # Concatenate audio with padding so that all audio clips will be of the \n",
    "  # same length\n",
    "  waveform = tf.cast(waveform, tf.float32)\n",
    "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "  spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length=255, frame_step=128)\n",
    "      \n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "  return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Mu6Y7Yz3C-V"
   },
   "outputs": [],
   "source": [
    "for waveform, label in waveform_ds.take(1):\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "print('Label:', label)\n",
    "print('Waveform shape:', waveform.shape)\n",
    "print('Spectrogram shape:', spectrogram.shape)\n",
    "print('Audio playback')\n",
    "display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e62jzb36-Jog"
   },
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "  # Convert to frequencies to log scale and transpose so that the time is\n",
    "  # represented in the x-axis (columns).\n",
    "  log_spec = np.log(spectrogram.T)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  Y = range(height)\n",
    "  ax.pcolormesh(X, Y, log_spec)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 16000])\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43IS2IouEV40"
   },
   "outputs": [],
   "source": [
    "def get_spectrogram_and_label_id(audio, label):\n",
    "  spectrogram = get_spectrogram(audio)\n",
    "  spectrogram = tf.expand_dims(spectrogram, -1)\n",
    "  label_id = tf.argmax(label == commands)\n",
    "  return spectrogram, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEVb_oK0oBLQ"
   },
   "outputs": [],
   "source": [
    "spectrogram_ds = waveform_ds.map(\n",
    "    get_spectrogram_and_label_id, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUbHfTuon4iF"
   },
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
    "  r = i // cols\n",
    "  c = i % cols\n",
    "  ax = axes[r][c]\n",
    "  plot_spectrogram(np.squeeze(spectrogram.numpy()), ax)\n",
    "  ax.set_title(commands[label_id.numpy()])\n",
    "  ax.axis('off')\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10UI32QH_45b"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(files):\n",
    "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "  output_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n",
    "  output_ds = output_ds.map(\n",
    "      get_spectrogram_and_label_id,  num_parallel_calls=AUTOTUNE)\n",
    "  return output_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNv4xwYkB2P6"
   },
   "outputs": [],
   "source": [
    "train_ds = spectrogram_ds\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgY9WYzn61EX"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdZ6M-F5_QzY"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALYz7PFCHblP"
   },
   "outputs": [],
   "source": [
    "for spectrogram, _ in spectrogram_ds.take(1):\n",
    "  input_shape = spectrogram.shape\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(commands)\n",
    "\n",
    "norm_layer = preprocessing.Normalization()\n",
    "norm_layer.adapt(spectrogram_ds.map(lambda x, _: x))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    preprocessing.Resizing(32, 32), \n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFjj7-EmsTD-"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttioPJVMcGtq"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,  \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzhipg3Gu2AY"
   },
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biU2MwzyAo8o"
   },
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "  test_audio.append(audio.numpy())\n",
    "  test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktUanr9mRZky"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Test set accuracy: {test_acc:.0%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvoSAOiXU3lL"
   },
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred) \n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx, xticklabels=commands, yticklabels=commands, \n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRxauKMdhofU"
   },
   "outputs": [],
   "source": [
    "sample_file = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
    "\n",
    "sample_ds = preprocess_dataset([str(sample_file)])\n",
    "\n",
    "for spectrogram, label in sample_ds.batch(1):\n",
    "  prediction = model(spectrogram)\n",
    "  plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
    "  plt.title(f'Predictions for \"{commands[label[0]]}\"')\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "simple_audio.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
