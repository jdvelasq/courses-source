{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Metric learning for image similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "y_train = np.squeeze(y_train)\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "y_test = np.squeeze(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "height_width = 32\n",
    "\n",
    "\n",
    "def show_collage(examples):\n",
    "    box_size = height_width + 2\n",
    "    num_rows, num_cols = examples.shape[:2]\n",
    "\n",
    "    collage = Image.new(\n",
    "        mode=\"RGB\",\n",
    "        size=(num_cols * box_size, num_rows * box_size),\n",
    "        color=(250, 250, 250),\n",
    "    )\n",
    "    for row_idx in range(num_rows):\n",
    "        for col_idx in range(num_cols):\n",
    "            array = (np.array(examples[row_idx, col_idx]) * 255).astype(np.uint8)\n",
    "            collage.paste(\n",
    "                Image.fromarray(array), (col_idx * box_size, row_idx * box_size)\n",
    "            )\n",
    "\n",
    "    # Double size for visualisation.\n",
    "    collage = collage.resize((2 * num_cols * box_size, 2 * num_rows * box_size))\n",
    "    return collage\n",
    "\n",
    "\n",
    "# Show a collage of 5x5 random images.\n",
    "sample_idxs = np.random.randint(0, 50000, size=(5, 5))\n",
    "examples = x_train[sample_idxs]\n",
    "show_collage(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class_idx_to_train_idxs = defaultdict(list)\n",
    "for y_train_idx, y in enumerate(y_train):\n",
    "    class_idx_to_train_idxs[y].append(y_train_idx)\n",
    "\n",
    "class_idx_to_test_idxs = defaultdict(list)\n",
    "for y_test_idx, y in enumerate(y_test):\n",
    "    class_idx_to_test_idxs[y].append(y_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "\n",
    "class AnchorPositivePairs(keras.utils.Sequence):\n",
    "    def __init__(self, num_batchs):\n",
    "        self.num_batchs = num_batchs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batchs\n",
    "\n",
    "    def __getitem__(self, _idx):\n",
    "        x = np.empty((2, num_classes, height_width, height_width, 3), dtype=np.float32)\n",
    "        for class_idx in range(num_classes):\n",
    "            examples_for_class = class_idx_to_train_idxs[class_idx]\n",
    "            anchor_idx = random.choice(examples_for_class)\n",
    "            positive_idx = random.choice(examples_for_class)\n",
    "            while positive_idx == anchor_idx:\n",
    "                positive_idx = random.choice(examples_for_class)\n",
    "            x[0, class_idx] = x_train[anchor_idx]\n",
    "            x[1, class_idx] = x_train[positive_idx]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "examples = next(iter(AnchorPositivePairs(num_batchs=1)))\n",
    "\n",
    "show_collage(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class EmbeddingModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        # Note: Workaround for open issue, to be removed.\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        anchors, positives = data[0], data[1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Run both anchors and positives through model.\n",
    "            anchor_embeddings = self(anchors, training=True)\n",
    "            positive_embeddings = self(positives, training=True)\n",
    "\n",
    "            # Calculate cosine similarity between anchors and positives. As they have\n",
    "            # been normalised this is just the pair wise dot products.\n",
    "            similarities = tf.einsum(\n",
    "                \"ae,pe->ap\", anchor_embeddings, positive_embeddings\n",
    "            )\n",
    "\n",
    "            # Since we intend to use these as logits we scale them by a temperature.\n",
    "            # This value would normally be chosen as a hyper parameter.\n",
    "            temperature = 0.2\n",
    "            similarities /= temperature\n",
    "\n",
    "            # We use these similarities as logits for a softmax. The labels for\n",
    "            # this call are just the sequence [0, 1, 2, ..., num_classes] since we\n",
    "            # want the main diagonal values, which correspond to the anchor/positive\n",
    "            # pairs, to be high. This loss will move embeddings for the\n",
    "            # anchor/positive pairs together and move all other pairs apart.\n",
    "            sparse_labels = tf.range(num_classes)\n",
    "            loss = self.compiled_loss(sparse_labels, similarities)\n",
    "\n",
    "        # Calculate gradients and apply via optimizer.\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # Update and return metrics (specifically the one for the loss value).\n",
    "        self.compiled_metrics.update_state(sparse_labels, similarities)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(height_width, height_width, 3))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, strides=2, activation=\"relu\")(inputs)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, strides=2, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, strides=2, activation=\"relu\")(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "embeddings = layers.Dense(units=8, activation=None)(x)\n",
    "embeddings = tf.nn.l2_normalize(embeddings, axis=-1)\n",
    "\n",
    "model = EmbeddingModel(inputs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "history = model.fit(AnchorPositivePairs(num_batchs=1000), epochs=20)\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "near_neighbours_per_example = 10\n",
    "\n",
    "embeddings = model.predict(x_test)\n",
    "gram_matrix = np.einsum(\"ae,be->ab\", embeddings, embeddings)\n",
    "near_neighbours = np.argsort(gram_matrix.T)[:, -(near_neighbours_per_example + 1) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "num_collage_examples = 5\n",
    "\n",
    "examples = np.empty(\n",
    "    (\n",
    "        num_collage_examples,\n",
    "        near_neighbours_per_example + 1,\n",
    "        height_width,\n",
    "        height_width,\n",
    "        3,\n",
    "    ),\n",
    "    dtype=np.float32,\n",
    ")\n",
    "for row_idx in range(num_collage_examples):\n",
    "    examples[row_idx, 0] = x_test[row_idx]\n",
    "    anchor_near_neighbours = reversed(near_neighbours[row_idx][:-1])\n",
    "    for col_idx, nn_idx in enumerate(anchor_near_neighbours):\n",
    "        examples[row_idx, col_idx + 1] = x_test[nn_idx]\n",
    "\n",
    "show_collage(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "\n",
    "# For each class.\n",
    "for class_idx in range(num_classes):\n",
    "    # Consider 10 examples.\n",
    "    example_idxs = class_idx_to_test_idxs[class_idx][:10]\n",
    "    for y_test_idx in example_idxs:\n",
    "        # And count the classes of its near neighbours.\n",
    "        for nn_idx in near_neighbours[y_test_idx][:-1]:\n",
    "            nn_class_idx = y_test[nn_idx]\n",
    "            confusion_matrix[class_idx, nn_class_idx] += 1\n",
    "\n",
    "# Display a confusion matrix.\n",
    "labels = [\n",
    "    \"Airplane\",\n",
    "    \"Automobile\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    "]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels)\n",
    "disp.plot(include_values=True, cmap=\"viridis\", ax=None, xticks_rotation=\"vertical\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "metric_learning",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
