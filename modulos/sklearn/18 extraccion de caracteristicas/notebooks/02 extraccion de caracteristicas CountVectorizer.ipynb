{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f840df1-29dd-4b41-a766-79bcb6e2d54a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Transformación de textos a características usando CountVectorizer\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752513a-607f-43c2-bb5f-1d57281e70bc",
   "metadata": {},
   "source": [
    "* Permite convertir un array de strigs a una matriz documento-término"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba770d-5ee9-4e7e-aa6e-97e3829e98ff",
   "metadata": {},
   "source": [
    "```\n",
    "This is the first document.\n",
    "This is the second second document.\n",
    "And the third one.\n",
    "Is this the first document?\n",
    "\n",
    "   and  document  first  is  one  second  the  third  this\n",
    "0    0         1      1   1    0       0    1      0     1\n",
    "1    0         1      0   1    0       2    1      0     1\n",
    "2    1         0      0   0    1       0    1      1     0\n",
    "3    0         1      1   1    0       0    1      0     1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a638c-1685-43f8-845c-05f8df003d3b",
   "metadata": {},
   "source": [
    "Creación del corpus\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f40b7a-a7c5-44a2-a158-621d1cbdbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This is the second second document.\",\n",
    "    \"And the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f8574-96cd-4ba2-8a08-d1c1f30982a5",
   "metadata": {},
   "source": [
    "Creación del transformador\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdd6dc76-d12c-4082-a3fb-2aaf48c9ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pd.set_option(\"display.notebook_repr_html\", False)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert all characters to lowercase before tokenizing.\n",
    "    lowercase=True,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Override the preprocessing (strip_accents and lowercase) stage\n",
    "    preprocessor=None,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Override the string tokenization step while preserving the preprocessing \n",
    "    # and n-grams generation steps. Only applies if analyzer == 'word'.\n",
    "    tokenizer=None,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # If ‘english’, a built-in stop word list for English is used.\n",
    "    stop_words=None,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # {‘ascii’, ‘unicode’}\n",
    "    # Remove accents and perform other character normalization during the \n",
    "    # preprocessing step. ‘ascii’ is a fast method that only works on \n",
    "    # characters that have a direct ASCII mapping. ‘unicode’ is a slightly \n",
    "    # slower method that works on any characters.\n",
    "    strip_accents=None,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # {‘word’, ‘char’, ‘char_wb’}\n",
    "    # Whether the feature should be made of word n-gram or character n-grams. \n",
    "    analyzer=\"word\",\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Regular expression denoting what constitutes a “token”, only used if \n",
    "    # analyzer == 'word'. The default regexp select tokens of 2 or more \n",
    "    # alphanumeric characters (punctuation is completely ignored and always \n",
    "    # treated as a token separator).\n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "    # -------------------------------------------------------------------------\n",
    "    # When building the vocabulary ignore terms that have a document frequency \n",
    "    # strictly higher than the given threshold (corpus-specific stop words). If \n",
    "    # float, the parameter represents a proportion of documents, integer \n",
    "    # absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "    max_df=1.0,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # When building the vocabulary ignore terms that have a document frequency\n",
    "    # strictly lower than the given threshold. This value is also called \n",
    "    # cut-off in the literature. If float, the parameter represents a \n",
    "    # proportion of documents, integer absolute counts.\n",
    "    min_df=1,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # If not None, build a vocabulary that only consider the top max_features \n",
    "    # ordered by term frequency across the corpus.\n",
    "    max_features=None,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # If True, all non zero counts are set to 1. This is useful for discrete \n",
    "    # probabilistic models that model binary events rather than integer counts.\n",
    "    binary=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0a7ee-3e38-422d-a3ba-e8e9a077873e",
   "metadata": {},
   "source": [
    "Creación de la matriz documento-termino\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1edabd7-b9cc-4889-a7e8-1b55a0b47b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   and  document  first  is  one  second  the  third  this\n",
       "0    0         1      1   1    0       0    1      0     1\n",
       "1    0         1      0   1    0       2    1      0     1\n",
       "2    1         0      0   0    1       0    1      1     0\n",
       "3    0         1      1   1    0       0    1      0     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(corpus)\n",
    "X = vectorizer.transform(corpus)\n",
    "\n",
    "pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf8670-9f76-4222-9c7c-4beedb8f1e8f",
   "metadata": {},
   "source": [
    "Transformación de un texto nuevo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af64cbc0-3dac-45f1-b715-014c2a245f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([\"Something completely new.\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a36a9d-6238-4552-8077-c580d9df34de",
   "metadata": {},
   "source": [
    "Construcción de un analizador\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff19763-f622-4094-b26e-b0a43d69c321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'completely', 'new', 'text', 'document', 'to', 'analyze']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a completely new text document to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe81e9-9e5d-49ca-8623-b61359b2d1d5",
   "metadata": {},
   "source": [
    "Posición (columna) del token en la matriz\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a326ec-3a10-4e41-992b-f49ab10ea286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get(\"document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18576c90-905b-45c6-8d69-ba0d89fbe960",
   "metadata": {},
   "source": [
    "Reconocimiento de bigramas\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f193085-4532-4535-8753-aa2f860ad2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r\"\\b\\w+\\b\",\n",
    "    min_df=1,\n",
    ")\n",
    "\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "\n",
    "analyze(\"Bi-grams are cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b628a8ef-d546-4bad-828c-47a75c8bf80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   and  and the  document  first  first document  is  is the  is this  one  \\\n",
       "0    0        0         1      1               1   1       1        0    0   \n",
       "1    0        0         1      0               0   1       1        0    0   \n",
       "2    1        1         0      0               0   0       0        0    1   \n",
       "3    0        0         1      1               1   1       0        1    0   \n",
       "\n",
       "   second  ...  second second  the  the first  the second  the third  third  \\\n",
       "0       0  ...              0    1          1           0          0      0   \n",
       "1       2  ...              1    1          0           1          0      0   \n",
       "2       0  ...              0    1          0           0          1      1   \n",
       "3       0  ...              0    1          1           0          0      0   \n",
       "\n",
       "   third one  this  this is  this the  \n",
       "0          0     1        1         0  \n",
       "1          0     1        1         0  \n",
       "2          1     0        0         0  \n",
       "3          0     1        0         1  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.fit(corpus)\n",
    "\n",
    "X_2 = bigram_vectorizer.transform(corpus)\n",
    "\n",
    "pd.DataFrame(\n",
    "    X_2.toarray(),\n",
    "    columns=bigram_vectorizer.get_feature_names_out(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42426a-6e48-4744-b4a1-9f3de36bc42b",
   "metadata": {},
   "source": [
    "Extracción de una columna de la matriz documento-término\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b40dd3a3-3c92-43c3-80ce-052fe93761b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_index = bigram_vectorizer.vocabulary_.get(\"is this\")\n",
    "\n",
    "X_2[:, feature_index].toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
