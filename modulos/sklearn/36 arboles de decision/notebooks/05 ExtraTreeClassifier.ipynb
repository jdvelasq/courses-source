{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81090f0a-74c7-46fe-9850-343233609f36",
   "metadata": {},
   "source": [
    "sklearn.tree.ExtraTreeClassifier \n",
    "===\n",
    "\n",
    "* Ultima modificación: 2023-03-11 | [YouTube](https://www.youtube.com/watch?v=2485ZJz5NZg&list=PLEFpZ3YehTnDwjNNX3D7jjBsVKshZH4Kb&index=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb39c1b-ff64-44d9-8b68-68e14c112301",
   "metadata": {},
   "source": [
    "* Es un árbol de clasificación extremadamente aleatorizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c09d11-16d1-419a-bc12-1f0e4d1ade7f",
   "metadata": {},
   "source": [
    "* Difiere del arból clásico de decisión en la forma en que es construido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f68b50-182c-403e-b4ac-3687435f7656",
   "metadata": {},
   "source": [
    "* Cuando se van a separar los ejemplos de un nodo en dos grupos, se generan particiones aleatorias para cada una de las `max_features` características seleccionadas y se escoge la mejor partición."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a36e5-8784-47f9-bfa5-8bf05e8f46ea",
   "metadata": {},
   "source": [
    "* Cuando `max_features` es igual a 1, se construye un árbol completamente aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f5df4a-8da0-4918-92a2-bee879e568ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae813ed-6930-42a2-8992-974ca299171b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "extraTreeClassifier = ExtraTreeClassifier(\n",
    "    # --------------------------------------------------------------------------\n",
    "    #\n",
    "    #    Recibe exactamente los mismos parámetros que DecisionTreeClassifier\n",
    "    #\n",
    "    # --------------------------------------------------------------------------\n",
    "    # The function to measure the quality of a split. Supported criteria are\n",
    "    # “gini” for the Gini impurity and “log_loss” and “entropy” both for the\n",
    "    # Shannon information gain\n",
    "    criterion=\"gini\",\n",
    "    # --------------------------------------------------------------------------\n",
    "    # The strategy used to choose the split at each node. Supported strategies \n",
    "    # are “best” to choose the best split and “random” to choose the best \n",
    "    # random split.\n",
    "    splitter=\"best\",\n",
    "    # --------------------------------------------------------------------------\n",
    "    # The maximum depth of the tree. If None, then nodes are expanded until all\n",
    "    # leaves are pure or until all leaves contain less than min_samples_split \n",
    "    # samples.\n",
    "    max_depth=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # The minimum number of samples required to split an internal node:\n",
    "    # * If int, then consider min_samples_split as the minimum number.\n",
    "    # * If float, then min_samples_split is a fraction and \n",
    "    #   ceil(min_samples_split * n_samples) are the minimum number of samples\n",
    "    #   for each split.\n",
    "    min_samples_split=2,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # The minimum number of samples required to be at a leaf node. A split \n",
    "    # point at any depth will only be considered if it leaves at least \n",
    "    # min_samples_leaf training samples in each of the left and right branches. \n",
    "    # This may have the effect of smoothing the model, especially in \n",
    "    # regression.\n",
    "    # * If int, then consider min_samples_leaf as the minimum number.\n",
    "    # * If float, then min_samples_leaf is a fraction and \n",
    "    #   ceil(min_samples_leaf * n_samples) are the minimum number of samples\n",
    "    #   for each node.\n",
    "    min_samples_leaf=1,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # The minimum weighted fraction of the sum total of weights (of all the\n",
    "    # input samples) required to be at a leaf node. Samples have equal weight\n",
    "    # when sample_weight is not provided.\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # The number of features to consider when looking for the best split:\n",
    "    # * If int, then consider max_features features at each split.\n",
    "    # * If float, then max_features is a fraction and \n",
    "    #   max(1, int(max_features * n_features_in_)) features are considered at \n",
    "    #   each split.\n",
    "    # * If “sqrt”, then max_features=sqrt(n_features).\n",
    "    # * If “log2”, then max_features=log2(n_features).\n",
    "    # * If None, then max_features=n_features.\n",
    "    max_features=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Controls the randomness of the estimator. The features are always \n",
    "    # randomly permuted at each split, even if splitter is set to \"best\". When\n",
    "    # max_features < n_features, the algorithm will select max_features at \n",
    "    # random at each split before finding the best split among them. But the \n",
    "    # best found split may vary across different runs, even if \n",
    "    # max_features=n_features. That is the case, if the improvement of the \n",
    "    # criterion is identical for several splits and one split has to be \n",
    "    # selected at random. To obtain a deterministic behaviour during fitting, \n",
    "    # random_state has to be fixed to an integer.\n",
    "    random_state=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are \n",
    "    # defined as relative reduction in impurity. If None then unlimited number\n",
    "    # of leaf nodes.\n",
    "    max_leaf_nodes=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # A node will be split if this split induces a decrease of the impurity\n",
    "    # greater than or equal to this value.\n",
    "    #\n",
    "    # The weighted impurity decrease equation is the following:\n",
    "    #\n",
    "    #   N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "    #                       - N_t_L / N_t * left_impurity)\n",
    "    #\n",
    "    # where N is the total number of samples, N_t is the number of samples at\n",
    "    # the current node, N_t_L is the number of samples in the left child, and \n",
    "    # N_t_R is the number of samples in the right child.\n",
    "    #\n",
    "    # N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight\n",
    "    # is passed.\n",
    "    min_impurity_decrease=0.0,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Weights associated with classes in the form {class_label: weight}. If \n",
    "    # None, all classes are supposed to have weight one.\n",
    "    class_weight=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Complexity parameter used for Minimal Cost-Complexity Pruning. The \n",
    "    # subtree with the largest cost complexity that is smaller than ccp_alpha \n",
    "    # will be chosen. By default, no pruning is performed.\n",
    "    ccp_alpha=0.0,\n",
    ")\n",
    "\n",
    "extraTreeClassifier.fit(X, y)\n",
    "extraTreeClassifier.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69936d7d-861a-4335-9901-db6c59460af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraTreeClassifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c013360d-e213-4ebc-9548-0b35d1c70f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraTreeClassifier.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b972b8-e3e3-4a5f-a6f4-70947b539f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraTreeClassifier.n_classes_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
