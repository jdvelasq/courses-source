{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegressionCV\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implementa la regresión logística con validación cruzada para el parámetro C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "logisticRegressionCV = LogisticRegressionCV(\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Each of the values in Cs describes the inverse of regularization strength.\n",
    "    # If Cs is as an int, then a grid of Cs values are chosen in a logarithmic\n",
    "    # scale between 1e-4 and 1e4. Like in support vector machines, smaller\n",
    "    # values specify stronger regularization.\n",
    "    Cs=[1e-3, 1e-2, 1e-1, 1],\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Specifies if a constant (a.k.a. bias or intercept) should be added to the\n",
    "    # decision function.\n",
    "    fit_intercept=True,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # The default cross-validation generator used is Stratified K-Folds. If an\n",
    "    # integer is provided, then it is the number of folds used.\n",
    "    cv=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Specify the norm of the penalty:\n",
    "    # * 'l2': add a L2 penalty term and it is the default choice.\n",
    "    # * 'l1': add a L1 penalty term.\n",
    "    # * 'elasticnet': both L1 and L2 penalty terms are added.\n",
    "    penalty=\"l2\",\n",
    "    # --------------------------------------------------------------------------\n",
    "    # A string (see model evaluation documentation) or a scorer callable\n",
    "    # object / function with signature scorer(estimator, X, y).\n",
    "    scoring=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Algorithm to use in the optimization problem. Default is ‘lbfgs’. To\n",
    "    # choose a solver, you might want to consider the following aspects:\n",
    "    # * For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and\n",
    "    #   ‘saga’ are faster for large ones.\n",
    "    # * For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’\n",
    "    #   handle multinomial loss.\n",
    "    # * ‘liblinear’ and is limited to one-versus-rest schemes.\n",
    "    # * ‘newton-cholesky’ is a good choice for n_samples >> n_features,\n",
    "    #   especially with one-hot encoded categorical features with rare\n",
    "    #   categories. Note that it is limited to binary classification and the\n",
    "    #   one-versus-rest reduction for multiclass classification. Be aware that\n",
    "    #   the memory usage of this solver has a quadratic dependency on n_features\n",
    "    #   because it explicitly computes the Hessian matrix.\n",
    "    solver=\"lbfgs\",\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Tolerance for stopping criteria.\n",
    "    tol=0.0001,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Maximum number of iterations taken for the solvers to converge.\n",
    "    max_iter=1000,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Weights associated with classes in the form {class_label: weight}. If not\n",
    "    # given, all classes are supposed to have weight one.\n",
    "    # * 'balanced' uses the values of y to automatically adjust weights\n",
    "    # inversely proportional to class frequencies in the input data as\n",
    "    # n_samples / (n_classes * np.bincount(y)).\n",
    "    class_weight=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # If set to True, the scores are averaged across all folds, and the coefs\n",
    "    # and the C that corresponds to the best score is taken, and a final refit\n",
    "    # is done using these parameters.\n",
    "    #\n",
    "    # Otherwise the coefs, intercepts and C that correspond to the best scores\n",
    "    # across folds are averaged.\n",
    "    refit=True,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Used when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the data.\n",
    "    random_state=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # {‘ovr’, ‘multinomial’, ‘auto’}\n",
    "    # If the option chosen is ‘ovr’, then a binary problem is fit for each\n",
    "    # label. For ‘multinomial’ the loss minimised is the multinomial loss fit\n",
    "    # across the entire probability distribution, even when the data is binary.\n",
    "    multi_class=\"auto\",\n",
    "    # --------------------------------------------------------------------------\n",
    "    # The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if\n",
    "    # penalty='elasticnet'. Setting l1_ratio=0 is equivalent to using\n",
    "    # penalty='l2', while setting l1_ratio=1 is equivalent to using penalty='l1'.\n",
    "    # For 0 < l1_ratio <1, the penalty is a combination of L1 and L2.\n",
    "    l1_ratios=None,\n",
    ")\n",
    "\n",
    "logisticRegressionCV.fit(X, y)\n",
    "\n",
    "logisticRegressionCV.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.63204311])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegressionCV.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98073972,  0.19049121, -0.29300361,  0.02408592, -0.16773977,\n",
       "        -0.22202195, -0.51221874, -0.27695865, -0.25210201, -0.03132659,\n",
       "        -0.07860476,  1.30855956,  0.13383261, -0.11150831, -0.02331991,\n",
       "         0.05411632, -0.04088007, -0.03580027, -0.03659877,  0.01168161,\n",
       "         0.09922278, -0.44981211, -0.10590221, -0.01316356, -0.33656522,\n",
       "        -0.71536982, -1.38542672, -0.57016644, -0.70482241, -0.09764186]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegressionCV.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 2.75164335e-14],\n",
       "       [9.99995346e-01, 4.65357777e-06],\n",
       "       [9.99999464e-01, 5.35566632e-07],\n",
       "       [6.86987914e-01, 3.13012086e-01],\n",
       "       [9.99745484e-01, 2.54516312e-04],\n",
       "       [7.52946748e-01, 2.47053252e-01],\n",
       "       [9.99994218e-01, 5.78244161e-06],\n",
       "       [9.89586771e-01, 1.04132294e-02],\n",
       "       [9.43836648e-01, 5.61633519e-02],\n",
       "       [9.98662627e-01, 1.33737312e-03]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegressionCV.predict_proba(X)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9595782073813708"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegressionCV.score(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nteract": {
   "version": "0.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "2482a4565599eda3cd2db35331af39775c1f78a505a9deef48c721423040da5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
