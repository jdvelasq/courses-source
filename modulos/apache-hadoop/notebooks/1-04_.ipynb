{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de palabras en Python usando Hadoop Streaming (Python avanzado)\n",
    "===\n",
    "\n",
    "* Última modificación: Noviembre 03, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición del problema\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea contar la frecuencia de ocurrencia de palabras en un conjunto de documentos. Debido a los requerimientos de diseño (gran volúmen de datos y tiempos rápidos de respuesta) se desea implementar una arquitectura Big Data. Se desea implementar una **solución computacional eficiente** en **Python**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/wordcount\n",
      "input\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se crea el directorio de entrada\n",
    "#\n",
    "!rm -rf /tmp/wordcount\n",
    "!mkdir -p /tmp/wordcount/input\n",
    "%cd /tmp/wordcount\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 1 --- Implementación del mapper\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#! /usr/bin/python3\n",
    "\n",
    "#\n",
    "# Esta es la funcion que mapea la entrada a parejas (clave, valor)\n",
    "#\n",
    "import sys\n",
    "\n",
    "\n",
    "#\n",
    "# Se usa una clase iterable para implementar el mapper.\n",
    "#\n",
    "class Mapper:\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        #\n",
    "        # almacena el flujo de entrada como una variable del objeto\n",
    "        #\n",
    "        self.stream = stream\n",
    "    \n",
    "    def emit(self, key, value):\n",
    "        #\n",
    "        # escribe al flujo estandar de salida\n",
    "        #\n",
    "        sys.stdout.write(\"{}\\t{}\\n\".format(key, value))\n",
    "        \n",
    "        \n",
    "    def status(self, message):\n",
    "        #\n",
    "        # imprime un reporte en el flujo de error\n",
    "        # no se debe usar el stdout, ya que en este \n",
    "        # unicamente deben ir las parejas (key, value)\n",
    "        #\n",
    "        sys.stderr.write('reporter:status:{}\\n'.format(message))\n",
    "\n",
    "        \n",
    "    def counter(self, counter, amount=1, group=\"ApplicationCounter\"):\n",
    "        #\n",
    "        # imprime el valor del contador\n",
    "        #\n",
    "        sys.stderr.write('reporter:counter:{},{},{}\\n'.format(group, counter, amount))\n",
    "        \n",
    "    def map(self):\n",
    "\n",
    "        word_counter = 0\n",
    "        \n",
    "        #\n",
    "        # imprime un mensaje a la entrada\n",
    "        #\n",
    "        self.status('Iniciando procesamiento ')\n",
    "            \n",
    "        for word in self:\n",
    "            #\n",
    "            # cuenta la cantidad de palabras procesadas\n",
    "            #\n",
    "            word_counter += 1\n",
    "            \n",
    "            #\n",
    "            # por cada palabra del flujo de datos\n",
    "            # emite la pareja (word, 1)\n",
    "            #\n",
    "            self.emit(key=word, value=1)\n",
    "\n",
    "        #\n",
    "        # imprime un mensaje a la salida\n",
    "        #\n",
    "        self.counter('num_words', amount=word_counter)\n",
    "        self.status('Finalizadno procesamiento ')\n",
    "\n",
    " \n",
    "            \n",
    "            \n",
    "    def __iter__(self):\n",
    "        #\n",
    "        # itera sobre cada linea de codigo recibida\n",
    "        # a traves del flujo de entrada\n",
    "        #\n",
    "        for line in self.stream:\n",
    "            #\n",
    "            # itera sobre cada palabra de la linea\n",
    "            # (en los ciclos for, retorna las palabras\n",
    "            # una a una)\n",
    "            #\n",
    "            for word in line.split():\n",
    "                #\n",
    "                # retorna la palabra siguiente en el ciclo for\n",
    "                #\n",
    "                yield word\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    #\n",
    "    # inicializa el objeto con el flujo de entrada\n",
    "    #\n",
    "    mapper = Mapper(sys.stdin)\n",
    "    \n",
    "    #\n",
    "    # ejecuta el mapper\n",
    "    #\n",
    "    mapper.map()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 2 --- Verificación\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# El programa anterior se hace ejecutable\n",
    "#\n",
    "!chmod +x /tmp/mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:status:Iniciando procesamiento \n",
      "reporter:counter:ApplicationCounter,num_words,252\n",
      "reporter:status:Finalizadno procesamiento \n",
      "Analytics\t1\n",
      "is\t1\n",
      "the\t1\n",
      "discovery,\t1\n",
      "interpretation,\t1\n",
      "and\t1\n",
      "communication\t1\n",
      "of\t1\n",
      "meaningful\t1\n",
      "patterns\t1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# la salida de la función anterior es:\n",
    "#\n",
    "!cat input/text*.txt | python3 mapper.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 3 ---- Implementación del Reducer\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El reducer recibe las parejas (key, value) a través del flujo de salida. En los ejemplos anteriores, el reducer verifica si la clave cambia de un elemento al siguiente. Sin embargo, resulta más eficiente que se pueda iterar directamente sobre elementos consecutivos que tienen la misma clave. La función `groupby` de la librería `itertools` permite hacer esto. Dicha función recibe como argumentos los datos y una función que genera la clave para cada dato. Retorna una tupla con la clave y los elementos consecutivos que contienen la misma clave. El siguiente ejemplo permite clarificar su operación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "    ('A', 1)\n",
      "B\n",
      "    ('B', 10)\n",
      "A\n",
      "    ('A', 2)\n",
      "    ('A', 3)\n",
      "    ('A', 4)\n",
      "B\n",
      "    ('B', 20)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "#\n",
    "# la letra es la clave y los números son los valores\n",
    "#\n",
    "data = [('A', 1), ('B', 10), ('A', 2), ('A', 3), ('A', 4) , ('B', 20)]\n",
    "\n",
    "#\n",
    "# retorna la parte correspondiente a la clave\n",
    "#\n",
    "def keyfun(x):\n",
    "    k, v = x\n",
    "    return k\n",
    "\n",
    "#\n",
    "# itera sobre la clave y los elementos que contiene la misma clave\n",
    "#\n",
    "for key, group in itertools.groupby(data, keyfun):\n",
    "    print(key)\n",
    "    for g in group:\n",
    "        print('   ', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se modifica el reducer para incoporar el uso de clases y de la función `groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#! /usr/bin/python3\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "class Reducer:\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.stream = stream\n",
    "        \n",
    "    def emit(self, key, value):\n",
    "        sys.stdout.write(\"{}\\t{}\\n\".format(key, value)) \n",
    "\n",
    "    def reduce(self):\n",
    "        #\n",
    "        # Esta funcion reduce los elementos que \n",
    "        # tienen la misma clave\n",
    "        #        \n",
    "        for key, group in itertools.groupby(self, lambda x: x[0]):\n",
    "            total = 0\n",
    "            for _, val in group:\n",
    "                total += val\n",
    "            \n",
    "            self.emit(key=key, value=total)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for line in self.stream:\n",
    "            #\n",
    "            # Lee el stream de datos y lo parte \n",
    "            # en (clave, valor)\n",
    "            #\n",
    "            key, val = line.split(\"\\t\") \n",
    "            val = int(val)\n",
    "            \n",
    "            #\n",
    "            # retorna la tupla (clave, valor)\n",
    "            # como el siguiente elemento del ciclo for\n",
    "            #\n",
    "            yield (key, val)\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "  \n",
    "    reducer = Reducer(sys.stdin)\n",
    "    reducer.reduce()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 4\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Se hace ejecutable el archivo\n",
    "#\n",
    "!chmod +x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 5\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se prueba la implementación localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:status:Iniciando procesamiento \n",
      "reporter:counter:ApplicationCounter,num_words,252\n",
      "reporter:status:Finalizadno procesamiento \n",
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t3\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Specifically,\t1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# La función sort hace que todos los elementos con \n",
    "# la misma clave queden en lineas consecutivas.\n",
    "# Hace el papel del módulo Shuffle & Sort\n",
    "#\n",
    "!cat input/text*.txt | python3 mapper.py | sort | python3 reducer.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 6 --- Movimiento de los archivos al HDFS\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup       1093 2022-05-26 02:31 input/text0.txt\n",
      "-rw-r--r--   1 root supergroup        352 2022-05-26 02:31 input/text1.txt\n",
      "-rw-r--r--   1 root supergroup        440 2022-05-26 02:31 input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir input\n",
    "!hadoop fs -copyFromLocal input/* input\n",
    "!hadoop fs -ls input/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 7\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.sh\n",
    "#\n",
    "# Se ejecuta en Hadoop.\n",
    "#   -input: archivo de entrada\n",
    "#   -output: directorio de salida\n",
    "#   -maper: programa que ejecuta el map\n",
    "#   -reducer: programa que ejecuta la reducción\n",
    "#\n",
    "hadoop jar \\\n",
    "    $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "    -files mapper.py,reducer.py  \\\n",
    "    -input input  \\\n",
    "    -output output \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar5327758262618317368/] [] /tmp/streamjob4110569942363235024.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!bash app.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2022-05-26 02:32 output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup       1649 2022-05-26 02:32 output/part-00000\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Contenido del directorio con los resultados de la corrida\n",
    "#\n",
    "!hadoop fs -ls output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2022-05-26 02:32 output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup       1649 2022-05-26 02:32 output/part-00000\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Contenido del directorio con los resultados de la corrida\n",
    "#\n",
    "!hadoop fs -ls output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t3\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Specifically,\t1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se visualiza el archivo con los resultados de la corrida\n",
    "#\n",
    "!hadoop fs -cat output/part-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 8 --- Movimiento de los resultados al sistema local\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/_SUCCESS  output/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal output output\n",
    "!ls output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 9 --- Limpieza del HDFS\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted input/text0.txt\n",
      "Deleted input/text1.txt\n",
      "Deleted input/text2.txt\n",
      "Deleted output/_SUCCESS\n",
      "Deleted output/part-00000\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se elimina el directorio de salida en el hdfs si existe\n",
    "#\n",
    "!hadoop fs -rm input/*\n",
    "!hadoop fs -rm output/*\n",
    "!hadoop fs -rmdir input output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpieza de la máquina local\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm reducer.py mapper.py\n",
    "!rm -rf input output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
