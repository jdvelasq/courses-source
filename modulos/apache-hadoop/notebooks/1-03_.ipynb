{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de palabras en Python usando Hadoop Streaming pseudo-distribuido\n",
    "===\n",
    "\n",
    "* Última modificación: Noviembre 05, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición del problema\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea contar la frecuencia de ocurrencia de palabras en un conjunto de documentos. Debido a los requerimientos de diseño (gran volúmen de datos y tiempos rápidos de respuesta) se desea implementar una arquitectura Big Data. Se desea implementar la solución en **Python3** y ejecutar Hadoop en **modo streamming**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archivos de prueba\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema. Puede usar directamente comandos del sistema operativo en el Terminal y el editor de texto `pico` para crear los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/wordcount\n",
      "input\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se crea el directorio de entrada\n",
    "#\n",
    "!rm -rf /tmp/wordcount\n",
    "!mkdir -p /tmp/wordcount/input\n",
    "%cd /tmp/wordcount\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba de la implementación fuera de Hadoop\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el mismo código del tutorial \"Conteo de palabras en Python usando el algortitmo MapReduce\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#! /usr/bin/python3\n",
    "\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    for line in sys.stdin:\n",
    "        for word in line.split(): \n",
    "            sys.stdout.write(\"{}\\t1\\n\".format(word))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#! /usr/bin/python3\n",
    "\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__': \n",
    "  \n",
    "    curkey = None\n",
    "    total = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        \n",
    "        key, val = line.split(\"\\t\") \n",
    "        val = int(val)\n",
    "        \n",
    "        if key == curkey: \n",
    "            total += val  \n",
    "        else:\n",
    "            if curkey is not None:\n",
    "                sys.stdout.write(\"{}\\t{}\\n\".format(curkey, total)) \n",
    "            \n",
    "            curkey = key\n",
    "            total = val\n",
    "            \n",
    "    sys.stdout.write(\"{}\\t{}\\n\".format(curkey, total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x mapper.py reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "mapper.py\n",
      "reducer.py\n"
     ]
    }
   ],
   "source": [
    "!ls  -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text0.txt\n",
      "text1.txt\n",
      "text2.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -1 input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 1. Movimiento de los datos al HDFS\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se copian los archivos de entrada del sistema local (o del datalake) al HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `input': File exists\n",
      "-rw-r--r--   1 root supergroup       1093 2022-05-26 19:08 input/text0.txt\n",
      "-rw-r--r--   1 root supergroup        352 2022-05-26 19:08 input/text1.txt\n",
      "-rw-r--r--   1 root supergroup        440 2022-05-26 19:08 input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir input\n",
    "!hadoop fs -copyFromLocal input/* input\n",
    "!hadoop fs -ls input/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vaya a la pantalla de monitoreo del NameNode (http://127.0.0.1:50070/) y ubique los archivos en el HDFS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Comando para la gestión de archivos y directorios\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gestión de archivos entre el sistema local y el HDFS se realiza mediante comandos similares a los del sistema operativo Unix en Terminal. A continuación se resumen los principales comandos.\n",
    "\n",
    "* `hadoop fs -help`:  Imprime la ayuda en pantalla para todos los comandos.\n",
    "\n",
    "* `hadoop fs -ls <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -mkdir <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -rmdir <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -cp <src> <dest>`\n",
    "\n",
    "\n",
    "* `hadoop fs -mv <src> <dest>`\n",
    "\n",
    "\n",
    "* `hadoop fs -rm <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -cat <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -head <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -tail <path>`\n",
    "\n",
    "\n",
    "* `hadoop fs -text <path>`. Imprime el arachivo en `<path>` y lo imprime en formato texto. Soporta archivos zip, TextRecordInputStream y Avro.\n",
    "\n",
    "\n",
    "* `hadoop fs -stat <path>`: Imprime estadísticos de `<path>`.\n",
    "\n",
    "\n",
    "**Transferencia de información entre el sistema local y el HDFS**.\n",
    "\n",
    "\n",
    "* `hadoop fs -get <src> <localdest>` / `hadoop fs -copyToLocal <src> <localdest>`. Copia el contenido de `<src>` en el HDFS en `<localdest>` en el sistema local.\n",
    "\n",
    "\n",
    "* `hadoop fs -put <localsrc> <dest>` / `hadoop fs -copyFromLocal <src> <localdest>`. Copia el contenido de `<localsrc>` en el sistema local a `<dest>` en el HDFS.\n",
    "\n",
    "\n",
    "* `hadoop fs -count <path>`. Cuenta el número de directorios, archivos y bytes en `<path>`.\n",
    "\n",
    "\n",
    "* `hadoop fs -appendToFile <localsrc> <dest>`: pega al final de `<dest>` el contenido de los archivos en `<localsrc>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 2. Ejecución\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.sh\n",
    "#\n",
    "# Se ejecuta en Hadoop.\n",
    "#   -files: archivos a copiar al hdfs\n",
    "#   -input: archivo de entrada\n",
    "#   -output: directorio de salida\n",
    "#   -file: archivos a copiar de la maquina local al hdfs\n",
    "#   -maper: programa que ejecuta el map\n",
    "#   -reducer: programa que ejecuta la reducción\n",
    "#\n",
    "hdfs dfs -rm -r output\n",
    "\n",
    "hadoop jar \\\n",
    "    $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "    -files mapper.py,reducer.py  \\\n",
    "    -input input  \\\n",
    "    -output output \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted output\n",
      "packageJobJar: [/tmp/hadoop-unjar1274556947240167022/] [] /tmp/streamjob4380793870891757123.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!bash app.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup          0 2022-05-26 19:08 output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup      14300 2022-05-26 19:08 output/part-00000\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Contenido del directorio con los resultados de la corrida\n",
    "#\n",
    "!hadoop fs -ls output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"*\"\t19\n",
      "\"AS\t5\n",
      "\"License\");\t5\n",
      "\"alice,bob\t19\n",
      "\"clumping\"\t1\n",
      "\"full_queue_name\"\t1\n",
      "\"priority\".\t1\n",
      "\"workflowId\"\t1\n",
      "&quot;kerberos&quot;.\t1\n",
      "&quot;simple&quot;\t1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se visualiza el archivo con los resultados de la corrida\n",
    "#\n",
    "!hadoop fs -cat output/part-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 3. Movimiento de los resultados al sistema local\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/_SUCCESS\n",
      "output/part-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal output output\n",
    "!ls -1 output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 4. Limpieza del hdfs\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted input/text0.txt\n",
      "Deleted input/text1.txt\n",
      "Deleted input/text2.txt\n",
      "Deleted output/_SUCCESS\n",
      "Deleted output/part-00000\n",
      "rmdir: `input': Directory is not empty\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se elimina el directorio de salida en el hdfs si existe\n",
    "#\n",
    "!hadoop fs -rm -r input/*\n",
    "!hadoop fs -rm -r output/*\n",
    "!hadoop fs -rmdir input output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpieza de la máquina local\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm reducer.py mapper.py\n",
    "!rm -rf input output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas.\n",
    "\n",
    "**Combiners.--** Los combiners son *reducers* que se ejecutan sobre los resultdos que produce cada mapper antes de pasar al modulo de suffle-&-sort, con el fin de reducir la carga computacional. Suelen ser identicos a los *reducers*. Una llamada típica sería:\n",
    "\n",
    "\n",
    "     $HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "        -input input \\\n",
    "        -output output  \\\n",
    "        -mapper mapper.py \\\n",
    "        -reducer reducer.py \\\n",
    "        -combiner combiner.py\n",
    "\n",
    "\n",
    "**Partitioners.--** Son rutinas que controlan como se enviar las parejas (clave, valor) a cada reducers, tal que elementos con la misma clave son enviados al mismo reducer. \n",
    "\n",
    "**Job Chain.--** Se refiere al encadenamiento de varias tareas cuando el cómputo que se desea realizar es muy complejo para que pueda realizarse en un MapReduce."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
