{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesamiento de datos con Hive\n",
    "====\n",
    "\n",
    "* Última modificación: Mayo 17, 2022 | YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tutorial esta basado en https://es.hortonworks.com/tutorial/how-to-process-data-with-apache-hive/\n",
    "\n",
    "El objetivo de este tutorial es implemetar consultas en Hive para analizar, procesar y filtrar los datos existentes en una bodega de datos, usando lenguaje SQL estándar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell magic `%%hive`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import Magics, cell_magic, line_magic, magics_class\n",
    "from pexpect import spawn\n",
    "\n",
    "TIMEOUT = 60\n",
    "PROG = \"hive\"\n",
    "PROMPT = [\"\\r\\n    > \", \"\\r\\nhive> \"]\n",
    "QUIT = \"quit;\"\n",
    "\n",
    "\n",
    "@magics_class\n",
    "class Magic(Magics):\n",
    "    def __init__(self, shell):\n",
    "        super().__init__(shell)\n",
    "        self.app = spawn(PROG, timeout=60)\n",
    "        self.app.expect(PROMPT)\n",
    "\n",
    "    @cell_magic\n",
    "    def hive(self, line, cell):\n",
    "        cell_lines = [cell_line.strip() for cell_line in cell.split(\"\\n\")]\n",
    "        cell_lines = [cell_line for cell_line in cell_lines if cell_line != \"\"]\n",
    "        for cell_line in cell_lines:\n",
    "            self.app.sendline(cell_line)\n",
    "            self.app.expect(PROMPT, timeout=TIMEOUT)\n",
    "            output = self.app.before.decode()\n",
    "            output = output.replace(\"\\r\\n\", \"\\n\")\n",
    "            output = output.split(\"\\n\")\n",
    "            output = [output_line.strip() for output_line in output]\n",
    "            for output_line in output:\n",
    "                if output_line not in cell_lines:\n",
    "                    print(output_line)\n",
    "        return None\n",
    "\n",
    "    @line_magic\n",
    "    def quit(self, line):\n",
    "        self.app.sendline(QUIT)\n",
    "\n",
    "\n",
    "def load_ipython_extension(ip):\n",
    "    ip.register_magics(Magic(ip))\n",
    "\n",
    "\n",
    "load_ipython_extension(ip=get_ipython())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos se encuentran almacenados en la carpeta `drivers` del directorio actual. A continución se procede a crear la carpeta `/tmp/drivers` en el sistema de archivos de Hadoop (HDFS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-17 14:41:50--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/drivers.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2043 (2.0K) [text/plain]\n",
      "Saving to: ‘drivers.csv’\n",
      "\n",
      "drivers.csv         100%[===================>]   2.00K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2022-05-17 14:41:51 (1.50 MB/s) - ‘drivers.csv’ saved [2043/2043]\n",
      "\n",
      "--2022-05-17 14:41:51--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/timesheet.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26205 (26K) [text/plain]\n",
      "Saving to: ‘timesheet.csv’\n",
      "\n",
      "timesheet.csv       100%[===================>]  25.59K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2022-05-17 14:41:52 (1.90 MB/s) - ‘timesheet.csv’ saved [26205/26205]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/drivers.csv\n",
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/timesheet.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp/drivers': File exists\n",
      "-rw-r--r--   1 root supergroup       2043 2022-05-17 14:41 /tmp/drivers/drivers.csv\n",
      "Found 1 items\n",
      "-rwxr-xr-x   1 root supergroup       1800 2022-05-17 13:33 /tmp/drivers/specific-columns/000000_0\n",
      "-rw-r--r--   1 root supergroup      26205 2022-05-17 14:41 /tmp/drivers/timesheet.csv\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Crea la carpeta drivers en el HDFS\n",
    "#\n",
    "!hdfs dfs -mkdir /tmp/drivers\n",
    "\n",
    "#\n",
    "# Copia los archivos al HDFS\n",
    "#\n",
    "!hdfs dfs -copyFromLocal drivers.csv  /tmp/drivers/\n",
    "!hdfs dfs -copyFromLocal timesheet.csv  /tmp/drivers/\n",
    "\n",
    "#\n",
    "# Lista los archivos al HDFS para verificar\n",
    "# que los archivos fueron copiados correctamente.\n",
    "##\n",
    "!hdfs dfs -ls /tmp/drivers/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El contenido de un archivo puede ser visualizado parcialmente usando el comando `tail`. Se usa para realizar una inspección rápida del contenido de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box 213- 8948 Nec Ave,Y,hours\n",
      "27,Mark Lochbihler,392603159,8355 Ipsum St.,Y,hours\n",
      "28,Olivier Renault,959908181,P.O. Box 243- 6509 Erat. Avenue,Y,hours\n",
      "29,Teddy Choi,185502192,P.O. Box 106- 7003 Amet Rd.,Y,hours\n",
      "30,Dan Rice,282307061,Ap #881-9267 Mollis Avenue,Y,hours\n",
      "31,Rommel Garcia,858912101,P.O. Box 945- 6015 Sociis St.,Y,hours\n",
      "32,Ryan Templeton,290304287,765-6599 Egestas. Av.,Y,hours\n",
      "33,Sridhara Sabbella,967409015,Ap #477-2507 Sagittis Avenue,Y,hours\n",
      "34,Frank Romano,391407216,Ap #753-6814 Quis Ave,Y,hours\n",
      "35,Emil Siemes,971401151,321-2976 Felis Rd.,Y,hours\n",
      "36,Andrew Grande,245303216,Ap #685-9598 Egestas Rd.,Y,hours\n",
      "37,Wes Floyd,190504074,P.O. Box 269- 9611 Nulla Street,Y,hours\n",
      "38,Scott Shaw,386411175,276 Lobortis Road,Y,hours\n",
      "39,David Kaiser,967706052,9185 At Street,Y,hours\n",
      "40,Nicolas Maillard,208510217,1027 Quis Rd.,Y,hours\n",
      "41,Greg Phillips,308103116,P.O. Box 847- 5961 Arcu. Road,Y,hours\n",
      "42,Randy Gelhausen,853302254,145-4200 In- Avenue,Y,hours\n",
      "43,Dave Patton,977706052,3028 A- St.,Y,hours"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se imprime el final del archivo drivers\n",
    "#\n",
    "!hdfs dfs -tail /tmp/drivers/drivers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42,36,56,2612\n",
      "42,37,48,2550\n",
      "42,38,55,2527\n",
      "42,39,57,2723\n",
      "42,40,55,2728\n",
      "42,41,50,2557\n",
      "42,42,53,2773\n",
      "42,43,55,2786\n",
      "42,44,54,2638\n",
      "42,45,57,2542\n",
      "42,46,48,2526\n",
      "42,47,50,2795\n",
      "42,48,53,2609\n",
      "42,49,58,2584\n",
      "42,50,48,2692\n",
      "42,51,50,2566\n",
      "42,52,48,2735\n",
      "43,1,46,2622\n",
      "43,2,47,2688\n",
      "43,3,50,2544\n",
      "43,4,56,2573\n",
      "43,5,54,2691\n",
      "43,6,52,2796\n",
      "43,7,53,2564\n",
      "43,8,58,2624\n",
      "43,9,50,2528\n",
      "43,10,57,2721\n",
      "43,11,51,2722\n",
      "43,12,59,2681\n",
      "43,13,52,2683\n",
      "43,14,46,2663\n",
      "43,15,53,2579\n",
      "43,16,56,2519\n",
      "43,17,54,2584\n",
      "43,18,47,2665\n",
      "43,19,55,2511\n",
      "43,20,60,2677\n",
      "43,21,52,2585\n",
      "43,22,60,2719\n",
      "43,23,48,2655\n",
      "43,24,48,2641\n",
      "43,25,53,2512\n",
      "43,26,48,2612\n",
      "43,27,58,2614\n",
      "43,28,60,2551\n",
      "43,29,55,2682\n",
      "43,30,49,2504\n",
      "43,31,51,2701\n",
      "43,32,57,2554\n",
      "43,33,52,2730\n",
      "43,34,54,2783\n",
      "43,35,51,2681\n",
      "43,36,51,2655\n",
      "43,37,46,2629\n",
      "43,38,58,2739\n",
      "43,39,47,2535\n",
      "43,40,50,2512\n",
      "43,41,51,2701\n",
      "43,42,55,2538\n",
      "43,43,58,2775\n",
      "43,44,56,2545\n",
      "43,45,46,2671\n",
      "43,46,57,2680\n",
      "43,47,50,2572\n",
      "43,48,52,2517\n",
      "43,49,56,2743\n",
      "43,50,59,2665\n",
      "43,51,58,2593\n",
      "43,52,48,2764"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /tmp/drivers/timesheet.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la tabla `temp_drivers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crea la tabla `temp_drivers`, que es almacenada en el disco como un archivo de texto, para almacenar la información de los conductores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 5.696 seconds\n",
      "OK\n",
      "Time taken: 0.56 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS temp_drivers;\n",
    "CREATE TABLE temp_drivers (col_value STRING) STORED AS TEXTFILE\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente, se visualizan las tablas en la base de datos actual que empiezan por t para verificar que la tabla fue creada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "temp_drivers\n",
      "Time taken: 0.11 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SHOW TABLES LIKE 't*';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos para la tabla `temp_drivers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente consulta realiza la carga de los datos del archivo `drivers.csv` en la tabla `temp_drivers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ers; DATA INPATH '/tmp/drivers/drivers.csv' OVERWRITE INTO TABLE temp_driv \n",
      "Loading data to table default.temp_drivers\n",
      "OK\n",
      "Time taken: 0.708 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "LOAD DATA INPATH '/tmp/drivers/drivers.csv' OVERWRITE INTO TABLE temp_drivers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive consume los datos, es decir, mueve los datos a la bodega de datos, de tal forma que el archivo `drivers.csv` es eliminado de la carpeta `/tmp/drivers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - root supergroup          0 2022-05-17 13:33 /tmp/drivers/specific-columns\n",
      "-rw-r--r--   1 root supergroup      26205 2022-05-17 14:41 /tmp/drivers/timesheet.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene los primeros 10 registros de la tabla para realizar una inspección rápida de los datos y verificar que los datos fueron cargados correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "10,George Vetticaden,621011971,244-4532 Nulla Rd.,N,miles\n",
      "11,Jamie Engesser,262112338,366-4125 Ac Street,N,miles\n",
      "12,Paul Coddin,198041975,Ap #622-957 Risus. Street,Y,hours\n",
      "13,Joe Niemiec,139907145,2071 Hendrerit. Ave,Y,hours\n",
      "14,Adis Cesir,820812209,Ap #810-1228 In St.,Y,hours\n",
      "15,Rohit Bakshi,239005227,648-5681 Dui- Rd.,Y,hours\n",
      "16,Tom McCuch,363303105,P.O. Box 313- 962 Parturient Rd.,Y,hours\n",
      "17,Eric Mizell,123808238,P.O. Box 579- 2191 Gravida. Street,Y,hours\n",
      "18,Grant Liu,171010151,Ap #928-3159 Vestibulum Av.,Y,hours\n",
      "19,Ajay Singh,160005158,592-9430 Nonummy Avenue,Y,hours\n",
      "Time taken: 1.005 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM temp_drivers LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de la tabla `drivers`\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crea la tabla `drivers` en donde se colocará la información extraída de la tabla `temp_drivers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 0.018 seconds\n",
      "OK\n",
      "Time taken: 0.061 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS drivers;\n",
    "\n",
    "CREATE TABLE drivers (driverId  INT, \n",
    "                      name      STRING, \n",
    "                      ssn       BIGINT,\n",
    "                      location  STRING, \n",
    "                      certified STRING, \n",
    "                      wageplan  STRING)\n",
    "\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que cada registro de la tabla `temp_drivers` es una línea de texto, se aplica una expresión regular (`regexp_extract`) para realizar la división del texto por las comas. La parte `{1}` representa la primera cadena de caracteres después de realizar la partición, `{2}` la segunda y así sucesivamente. Después de la llamada a la función `regexp_extract` se indica el nombre de la columna en la tabla `drivers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20220517144213_e23a371d-a2df-47f4-bab3-aaad8ab30155\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1652793922537_0007, Tracking URL = http://4feb4ed7d52d:8088/proxy/application_1652793922537_0007/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1652793922537_0007\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2022-05-17 14:42:20,276 Stage-1 map = 0%,  reduce = 0%\n",
      "2022-05-17 14:42:24,437 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.94 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 940 msec\n",
      "Ended Job = job_1652793922537_0007\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/drivers/.hive-staging_hive_2022-05-17_14-42-13_686_1675122652526265324-1/-ext-10000\n",
      "Loading data to table default.drivers\n",
      "MapReduce Jobs Launched:\n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 1.94 sec   HDFS Read: 6812 HDFS Write: 2036 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 1 seconds 940 msec\n",
      "OK\n",
      "Time taken: 13.097 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "INSERT OVERWRITE TABLE drivers\n",
    "SELECT\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) name,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) ssn,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) location,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){5}', 1) certified,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){6}', 1) wageplan\n",
    "FROM \n",
    "    temp_drivers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica la instrucción `SELECT` para revisar el resultado de la carga de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "11\tJamie Engesser\t262112338\t366-4125 Ac Street\tN\tmiles\n",
      "12\tPaul Coddin\t198041975\tAp #622-957 Risus. Street\tY\thours\n",
      "13\tJoe Niemiec\t139907145\t2071 Hendrerit. Ave\tY\thours\n",
      "14\tAdis Cesir\t820812209\tAp #810-1228 In St.\tY\thours\n",
      "15\tRohit Bakshi\t239005227\t648-5681 Dui- Rd.\tY\thours\n",
      "16\tTom McCuch\t363303105\tP.O. Box 313- 962 Parturient Rd.\tY\thours\n",
      "17\tEric Mizell\t123808238\tP.O. Box 579- 2191 Gravida. Street\tY\thours\n",
      "18\tGrant Liu\t171010151\tAp #928-3159 Vestibulum Av.\tY\thours\n",
      "19\tAjay Singh\t160005158\t592-9430 Nonummy Avenue\tY\thours\n",
      "20\tChris Harris\t921812303\t883-2691 Proin Avenue\tY\thours\n",
      "Time taken: 0.112 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM drivers LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de la tabla `temp_timesheet`\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a crear la tabla y cargar los datos para el archivo `time_sheet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 0.017 seconds\n",
      "OK\n",
      "Time taken: 0.216 seconds\n",
      "mesheet;A INPATH '/tmp/drivers/timesheet.csv' OVERWRITE INTO TABLE temp_ti \n",
      "Loading data to table default.temp_timesheet\n",
      "OK\n",
      "Time taken: 0.22 seconds\n",
      "OK\n",
      "10,1,70,3300\n",
      "10,2,70,3300\n",
      "10,3,60,2800\n",
      "10,4,70,3100\n",
      "10,5,70,3200\n",
      "10,6,70,3300\n",
      "10,7,70,3000\n",
      "10,8,70,3300\n",
      "10,9,70,3200\n",
      "10,10,50,2500\n",
      "Time taken: 0.096 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS temp_timesheet;\n",
    "\n",
    "CREATE TABLE temp_timesheet (col_value string) \n",
    "STORED AS TEXTFILE\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "LOAD DATA INPATH '/tmp/drivers/timesheet.csv' OVERWRITE INTO TABLE temp_timesheet;\n",
    "\n",
    "SELECT * FROM temp_timesheet LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de la tabla `timesheet`\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede igual que en las tablas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 0.018 seconds\n",
      "ogged INT)LE timesheet (driverId INT, week INT, hours_logged INT , miles_l \n",
      "OK\n",
      "Time taken: 0.042 seconds\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20220517144228_ef115f87-6300-428d-a380-9d530398d507\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1652793922537_0008, Tracking URL = http://4feb4ed7d52d:8088/proxy/application_1652793922537_0008/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1652793922537_0008\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2022-05-17 14:42:35,597 Stage-1 map = 0%,  reduce = 0%\n",
      "2022-05-17 14:42:39,707 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.85 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 850 msec\n",
      "Ended Job = job_1652793922537_0008\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/timesheet/.hive-staging_hive_2022-05-17_14-42-28_623_1595757489952936503-1/-ext-10000\n",
      "Loading data to table default.timesheet\n",
      "MapReduce Jobs Launched:\n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.85 sec   HDFS Read: 30739 HDFS Write: 24476 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 850 msec\n",
      "OK\n",
      "Time taken: 12.328 seconds\n",
      "OK\n",
      "10\t2\t70\t3300\n",
      "10\t3\t60\t2800\n",
      "10\t4\t70\t3100\n",
      "10\t5\t70\t3200\n",
      "10\t6\t70\t3300\n",
      "10\t7\t70\t3000\n",
      "10\t8\t70\t3300\n",
      "10\t9\t70\t3200\n",
      "10\t10\t50\t2500\n",
      "10\t11\t70\t2900\n",
      "Time taken: 0.093 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS timesheet;\n",
    "\n",
    "CREATE TABLE timesheet (driverId INT, week INT, hours_logged INT , miles_logged INT)\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "INSERT OVERWRITE TABLE timesheet\n",
    "SELECT\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) week,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) hours_logged,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) miles_logged\n",
    "FROM \n",
    "    temp_timesheet;\n",
    "\n",
    "SELECT * FROM timesheet LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantidad de horas y millas de cada conductor por año.\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente consulta se desea obtener para cada conductor la cantidad de horas y millas por año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20220517144241_4fe1f8e4-cd53-475e-8c16-51b039bc665a\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1652793922537_0009, Tracking URL = http://4feb4ed7d52d:8088/proxy/application_1652793922537_0009/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1652793922537_0009\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2022-05-17 14:42:50,503 Stage-1 map = 0%,  reduce = 0%\n",
      "2022-05-17 14:42:54,612 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.68 sec\n",
      "2022-05-17 14:42:59,749 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.31 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 310 msec\n",
      "Ended Job = job_1652793922537_0009\n",
      "MapReduce Jobs Launched:\n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.31 sec   HDFS Read: 33422 HDFS Write: 1005 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 310 msec\n",
      "OK\n",
      "10\t3162\t143850\n",
      "11\t3642\t179300\n",
      "12\t2639\t135962\n",
      "13\t2727\t134126\n",
      "14\t2781\t136624\n",
      "15\t2734\t138750\n",
      "16\t2746\t137205\n",
      "17\t2701\t135992\n",
      "18\t2654\t137834\n",
      "19\t2738\t137968\n",
      "20\t2644\t134564\n",
      "21\t2751\t138719\n",
      "22\t2733\t137550\n",
      "23\t2750\t137980\n",
      "24\t2647\t134461\n",
      "25\t2723\t139180\n",
      "26\t2730\t137530\n",
      "27\t2771\t137922\n",
      "28\t2723\t137469\n",
      "29\t2760\t138255\n",
      "30\t2773\t137473\n",
      "31\t2704\t137057\n",
      "32\t2736\t137422\n",
      "33\t2759\t139285\n",
      "34\t2811\t137728\n",
      "35\t2728\t138727\n",
      "36\t2795\t138025\n",
      "37\t2694\t137223\n",
      "38\t2760\t137464\n",
      "39\t2745\t138788\n",
      "40\t2700\t136931\n",
      "41\t2723\t138407\n",
      "42\t2697\t136673\n",
      "43\t2750\t136993\n",
      "Time taken: 19.249 seconds, Fetched: 34 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive \n",
    "SELECT \n",
    "    driverId, \n",
    "    sum(hours_logged), \n",
    "    sum(miles_logged) \n",
    "FROM \n",
    "    timesheet \n",
    "GROUP BY \n",
    "    driverId;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta para unir las tablas\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso final consiste en crear una consulta que agregue el nombre del conductor de la tabla `drivers` con la cantidad de horas y millas por año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20220517144301_fcf22209-53f3-42ce-9af0-d532ace4a917\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1652793922537_0010, Tracking URL = http://4feb4ed7d52d:8088/proxy/application_1652793922537_0010/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1652793922537_0010\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2022-05-17 14:43:09,601 Stage-2 map = 0%,  reduce = 0%\n",
      "2022-05-17 14:43:13,719 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.01 sec\n",
      "2022-05-17 14:43:18,828 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.63 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 630 msec\n",
      "Ended Job = job_1652793922537_0010\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "2022-05-17 14:43:26\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2022-05-17 14:43:28\tDump the side-table for tag: 0 with group count: 33 into file: file:/tmp/root/ca88b647-8729-4e04-9264-2e3e6a5c74e1/hive_2022-05-17_14-43-01_867_3518588412713534238-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable\n",
      "2022-05-17 14:43:28\tUploaded 1 File to: file:/tmp/root/ca88b647-8729-4e04-9264-2e3e6a5c74e1/hive_2022-05-17_14-43-01_867_3518588412713534238-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable (1325 bytes)\n",
      "2022-05-17 14:43:28\tEnd of local task; Time Taken: 1.4 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1652793922537_0011, Tracking URL = http://4feb4ed7d52d:8088/proxy/application_1652793922537_0011/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1652793922537_0011\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0\n",
      "2022-05-17 14:43:33,515 Stage-3 map = 0%,  reduce = 0%\n",
      "2022-05-17 14:43:37,621 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.53 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 530 msec\n",
      "Ended Job = job_1652793922537_0011\n",
      "MapReduce Jobs Launched:\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.63 sec   HDFS Read: 32521 HDFS Write: 946 SUCCESS\n",
      "Stage-Stage-3: Map: 1   Cumulative CPU: 1.53 sec   HDFS Read: 6784 HDFS Write: 1411 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 5 seconds 160 msec\n",
      "OK\n",
      "11\tJamie Engesser\t3642\t179300\n",
      "12\tPaul Coddin\t2639\t135962\n",
      "13\tJoe Niemiec\t2727\t134126\n",
      "14\tAdis Cesir\t2781\t136624\n",
      "15\tRohit Bakshi\t2734\t138750\n",
      "16\tTom McCuch\t2746\t137205\n",
      "17\tEric Mizell\t2701\t135992\n",
      "18\tGrant Liu\t2654\t137834\n",
      "19\tAjay Singh\t2738\t137968\n",
      "20\tChris Harris\t2644\t134564\n",
      "21\tJeff Markham\t2751\t138719\n",
      "22\tNadeem Asghar\t2733\t137550\n",
      "23\tAdam Diaz\t2750\t137980\n",
      "24\tDon Hilborn\t2647\t134461\n",
      "25\tJean-Philippe Playe\t2723\t139180\n",
      "26\tMichael Aube\t2730\t137530\n",
      "27\tMark Lochbihler\t2771\t137922\n",
      "28\tOlivier Renault\t2723\t137469\n",
      "29\tTeddy Choi\t2760\t138255\n",
      "30\tDan Rice\t2773\t137473\n",
      "31\tRommel Garcia\t2704\t137057\n",
      "32\tRyan Templeton\t2736\t137422\n",
      "33\tSridhara Sabbella\t2759\t139285\n",
      "34\tFrank Romano\t2811\t137728\n",
      "35\tEmil Siemes\t2728\t138727\n",
      "36\tAndrew Grande\t2795\t138025\n",
      "37\tWes Floyd\t2694\t137223\n",
      "38\tScott Shaw\t2760\t137464\n",
      "39\tDavid Kaiser\t2745\t138788\n",
      "40\tNicolas Maillard\t2700\t136931\n",
      "41\tGreg Phillips\t2723\t138407\n",
      "42\tRandy Gelhausen\t2697\t136673\n",
      "43\tDave Patton\t2750\t136993\n",
      "Time taken: 36.822 seconds, Fetched: 33 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT \n",
    "    d.driverId, \n",
    "    d.name, \n",
    "    t.total_hours, \n",
    "    t.total_miles \n",
    "FROM \n",
    "    drivers d\n",
    "JOIN (\n",
    "    SELECT \n",
    "        driverId, \n",
    "        sum(hours_logged)total_hours, \n",
    "        sum(miles_logged)total_miles \n",
    "    FROM \n",
    "        timesheet \n",
    "    GROUP BY \n",
    "        driverId \n",
    "    ) t\n",
    "ON \n",
    "    (d.driverId = t.driverId);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almacenamiento de los resultados\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se agrega una porción de codigo adicional a la consulta anterior para almacenar la tabla final obtenida en la carpeta `/tmp/drivers/summary` del HDFS para que otras aplicaciones puedan usar estos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20220517144339_0c63ca4e-baa5-46ab-9b53-25662d9d6082\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1652793922537_0012, Tracking URL = http://4feb4ed7d52d:8088/proxy/application_1652793922537_0012/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1652793922537_0012\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2022-05-17 14:43:48,364 Stage-2 map = 0%,  reduce = 0%\n",
      "2022-05-17 14:43:52,451 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.11 sec\n",
      "2022-05-17 14:43:57,573 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.44 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 440 msec\n",
      "Ended Job = job_1652793922537_0012\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "2022-05-17 14:44:04\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2022-05-17 14:44:05\tDump the side-table for tag: 0 with group count: 33 into file: file:/tmp/root/ca88b647-8729-4e04-9264-2e3e6a5c74e1/hive_2022-05-17_14-43-39_841_4775949701697963899-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile10--.hashtable\n",
      "2022-05-17 14:44:05\tUploaded 1 File to: file:/tmp/root/ca88b647-8729-4e04-9264-2e3e6a5c74e1/hive_2022-05-17_14-43-39_841_4775949701697963899-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile10--.hashtable (1325 bytes)\n",
      "2022-05-17 14:44:05\tEnd of local task; Time Taken: 1.437 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1652793922537_0013, Tracking URL = http://4feb4ed7d52d:8088/proxy/application_1652793922537_0013/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1652793922537_0013\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0\n",
      "2022-05-17 14:44:10,576 Stage-3 map = 0%,  reduce = 0%\n",
      "2022-05-17 14:44:14,682 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.69 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 690 msec\n",
      "Ended Job = job_1652793922537_0013\n",
      "Moving data to directory /tmp/drivers/summary\n",
      "MapReduce Jobs Launched:\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.44 sec   HDFS Read: 32512 HDFS Write: 946 SUCCESS\n",
      "Stage-Stage-3: Map: 1   Cumulative CPU: 1.69 sec   HDFS Read: 6345 HDFS Write: 928 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 5 seconds 130 msec\n",
      "OK\n",
      "Time taken: 35.911 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "INSERT OVERWRITE DIRECTORY '/tmp/drivers/summary' \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "SELECT \n",
    "    d.driverId, \n",
    "    d.name, \n",
    "    t.total_hours, \n",
    "    t.total_miles \n",
    "FROM \n",
    "    drivers d\n",
    "JOIN (\n",
    "    SELECT \n",
    "        driverId, \n",
    "        sum(hours_logged)total_hours, \n",
    "        sum(miles_logged)total_miles \n",
    "    FROM \n",
    "        timesheet \n",
    "    GROUP BY \n",
    "        driverId \n",
    "    ) t\n",
    "ON \n",
    "    (d.driverId = t.driverId);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxr-xr-x   1 root supergroup        928 2022-05-17 14:44 /tmp/drivers/summary/000000_0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/drivers/summary/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,Jamie Engesser,3642,179300\n",
      "12,Paul Coddin,2639,135962\n",
      "13,Joe Niemiec,2727,134126\n",
      "14,Adis Cesir,2781,136624\n",
      "15,Rohit Bakshi,2734,138750\n",
      "16,Tom McCuch,2746,137205\n",
      "17,Eric Mizell,2701,135992\n",
      "18,Grant Liu,2654,137834\n",
      "19,Ajay Singh,2738,137968\n",
      "20,Chris Harris,2644,134564\n",
      "21,Jeff Markham,2751,138719\n",
      "22,Nadeem Asghar,2733,137550\n",
      "23,Adam Diaz,2750,137980\n",
      "24,Don Hilborn,2647,134461\n",
      "25,Jean-Philippe Playe,2723,139180\n",
      "26,Michael Aube,2730,137530\n",
      "27,Mark Lochbihler,2771,137922\n",
      "28,Olivier Renault,2723,137469\n",
      "29,Teddy Choi,2760,138255\n",
      "30,Dan Rice,2773,137473\n",
      "31,Rommel Garcia,2704,137057\n",
      "32,Ryan Templeton,2736,137422\n",
      "33,Sridhara Sabbella,2759,139285\n",
      "34,Frank Romano,2811,137728\n",
      "35,Emil Siemes,2728,138727\n",
      "36,Andrew Grande,2795,138025\n",
      "37,Wes Floyd,2694,137223\n",
      "38,Scott Shaw,2760,137464\n",
      "39,David Kaiser,2745,138788\n",
      "40,Nicolas Maillard,2700,136931\n",
      "41,Greg Phillips,2723,138407\n",
      "42,Randy Gelhausen,2697,136673\n",
      "43,Dave Patton,2750,136993\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /tmp/drivers/summary/000000_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.csv *.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
