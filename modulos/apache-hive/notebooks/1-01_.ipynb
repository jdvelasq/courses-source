{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de palabras en Apache Hive (interactivo)\n",
    "===\n",
    "\n",
    "* Última modificación: Mayo 17, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell magic `%%hive`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import Magics, cell_magic, line_magic, magics_class\n",
    "from pexpect import spawn\n",
    "\n",
    "TIMEOUT = 60\n",
    "PROG = \"hive\"\n",
    "PROMPT = [\"\\r\\n    > \", \"\\r\\nhive> \"]\n",
    "QUIT = \"quit;\"\n",
    "\n",
    "\n",
    "@magics_class\n",
    "class Magic(Magics):\n",
    "    def __init__(self, shell):\n",
    "        super().__init__(shell)\n",
    "        self.app = spawn(PROG, timeout=60)\n",
    "        self.app.expect(PROMPT)\n",
    "\n",
    "    @cell_magic\n",
    "    def hive(self, line, cell):\n",
    "        cell_lines = [cell_line.strip() for cell_line in cell.split(\"\\n\")]\n",
    "        cell_lines = [cell_line for cell_line in cell_lines if cell_line != \"\"]\n",
    "        for cell_line in cell_lines:\n",
    "            self.app.sendline(cell_line)\n",
    "            self.app.expect(PROMPT, timeout=TIMEOUT)\n",
    "            output = self.app.before.decode()\n",
    "            output = output.replace(\"\\r\\n\", \"\\n\")\n",
    "            output = output.split(\"\\n\")\n",
    "            output = [output_line.strip() for output_line in output]\n",
    "            for output_line in output:\n",
    "                if output_line not in cell_lines:\n",
    "                    print(output_line)\n",
    "        return None\n",
    "\n",
    "    @line_magic\n",
    "    def quit(self, line):\n",
    "        self.app.sendline(QUIT)\n",
    "\n",
    "\n",
    "def load_ipython_extension(ip):\n",
    "    ip.register_magics(Magic(ip))\n",
    "\n",
    "\n",
    "load_ipython_extension(ip=get_ipython())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datos de prueba\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/wordcount/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/wordcount/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/wordcount/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta aplicación se usarán dos tablas:\n",
    "\n",
    "* `docs`: para cargar el contenido de los archivos de texto, donde cada línea equivale a un registro.\n",
    "\n",
    "* `word_counts`: En donde aparece cada palabra y su respectivo conteo.\n",
    "\n",
    "A continuación se elimnan dichas tablas si existen en el sistema, y luego se crea la tabla `docs` con un solo campo del tipo `STRING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Time taken: 6.001 seconds\n",
      "OK\n",
      "Time taken: 0.021 seconds\n",
      "OK\n",
      "Time taken: 0.492 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS docs;\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "CREATE TABLE docs (line STRING);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código, se hace la carga directa de todos los archivos que se encuentran en el directorio `wordcount` en la tabla `docs`. Luego, se imprimen los primeros cinco registros de la tabla para verificar que la lectura fue correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data to table default.docs\n",
      "OK\n",
      "Time taken: 1.563 seconds\n",
      "OK\n",
      "Analytics is the discovery, interpretation, and communication of meaningful patterns\n",
      "in data. Especially valuable in areas rich with recorded information, analytics relies\n",
      "on the simultaneous application of statistics, computer programming and operations research\n",
      "to quantify performance.\n",
      "\n",
      "Time taken: 1.04 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "LOAD DATA LOCAL INPATH \"/tmp/wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "SELECT * FROM docs LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargados los archivos, se procede a partir las líneas por palabras, usando la función `split(line, '\\\\s')`;  la expresión `\\\\s` indica que se realice la partición por los espacios en blanco; de esta forma, `split()` genera una lista de palabras. La función `explode(.)` de Hive en conjunto con `SELECT`, genera un nuevo registro por cada palabra en `line`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Analytics\n",
      "is\n",
      "the\n",
      "discovery,\n",
      "interpretation,\n",
      "Time taken: 0.312 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT explode(split(line, '\\\\s')) AS word FROM docs LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conteo de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar el conteo, la expresión `SELECT word, count(1) AS count ... GROUP BY word` genera una tabla con dos columnas, donde la primera columna (`word`) correspodne a cada palabra en el texto, y la segunda columna representa la cantidad de veces que aparece en los registros generados por la expresión `SELECT explode(split(line, '\\\\s')) AS word FROM docs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID = root_20220527151554_8829d099-a624-4639-8dc7-583cb62d3c87\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1653664460816_0001, Tracking URL = http://a668e4f67ef8:8088/proxy/application_1653664460816_0001/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1653664460816_0001\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2022-05-27 15:16:03,483 Stage-1 map = 0%,  reduce = 0%\n",
      "2022-05-27 15:16:07,841 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.0 sec\n",
      "2022-05-27 15:16:11,970 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.07 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 70 msec\n",
      "Ended Job = job_1653664460816_0001\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1653664460816_0002, Tracking URL = http://a668e4f67ef8:8088/proxy/application_1653664460816_0002/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1653664460816_0002\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2022-05-27 15:16:23,576 Stage-2 map = 0%,  reduce = 0%\n",
      "2022-05-27 15:16:27,696 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.16 sec\n",
      "2022-05-27 15:16:31,806 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.71 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 710 msec\n",
      "Ended Job = job_1653664460816_0002\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/word_counts\n",
      "MapReduce Jobs Launched:\n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.07 sec   HDFS Read: 10195 HDFS Write: 4345 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.71 sec   HDFS Read: 9498 HDFS Write: 1731 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 5 seconds 780 msec\n",
      "OK\n",
      "Time taken: 38.791 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "CREATE TABLE word_counts \n",
    "AS\n",
    "    SELECT word, count(1) AS count \n",
    "    FROM\n",
    "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
    "GROUP BY \n",
    "    word\n",
    "ORDER BY \n",
    "    word;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar los resultados obtenidos, se realiza un `SELECT` sobre la tabla `word_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "20\n",
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t3\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Time taken: 0.096 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM word_counts LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%quit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
