{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones básicas de texto\n",
    "\n",
    "* *30 min* | Última modificación: Sept 22, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DOI', 'Link', 'Abstract'], dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Preparacion de los datos\n",
    "## \n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/scopus-abstracts.csv\",\n",
    "    sep=\",\",\n",
    "    thousands=None,\n",
    "    decimal=\".\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mobility is one of the fundamental requirements of human life with significant societal impacts including productivity, economy, social wellbeing, adaptation to a changing climate, and so on. Although human movements follow specific patterns during normal periods, there are limited studies on how such patterns change due to extreme events. To quantify the impacts of an extreme event to human movements, we introduce the concept of mobility resilience which is defined as the ability of a mobility system to manage shocks and return to a steady state in response to an extreme event. We present a method to detect extreme events from geo-located movement data and to measure mobility resilience and transient loss of resilience due to those events. Applying this method, we measure resilience metrics from geo-located social media data for multiple types of disasters occurred all over the world. Quantifying mobility resilience may help us to assess the higher-order socio-economic impacts of extreme events and guide policies towards developing resilient infrastructures as well as a nation’s overall disaster resilience strategies. '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Abstracts\n",
    "##\n",
    "abstracts = data.Abstract.copy()\n",
    "abstracts = abstracts.map(lambda w: w[0: w.find(\"\\u00a9\")], na_action='ignore')\n",
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## HTML\n",
    "##\n",
    "from urllib import request\n",
    "\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BBC',\n",
       " 'NEWS',\n",
       " '|',\n",
       " 'Health',\n",
       " '|',\n",
       " 'Blondes',\n",
       " \"'to\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'in',\n",
       " '200',\n",
       " \"years'\",\n",
       " 'NEWS',\n",
       " 'SPORT',\n",
       " 'WEATHER',\n",
       " 'WORLD',\n",
       " 'SERVICE',\n",
       " 'A-Z',\n",
       " 'INDEX',\n",
       " 'SEARCH',\n",
       " 'You',\n",
       " 'are',\n",
       " 'in',\n",
       " ':',\n",
       " 'Health',\n",
       " 'News',\n",
       " 'Front',\n",
       " 'Page',\n",
       " 'Africa',\n",
       " 'Americas',\n",
       " 'Asia-Pacific',\n",
       " 'Europe',\n",
       " 'Middle',\n",
       " 'East',\n",
       " 'South',\n",
       " 'Asia',\n",
       " 'UK',\n",
       " 'Business',\n",
       " 'Entertainment',\n",
       " 'Science/Nature']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Limpieza del HTML\n",
    "##\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "tokens[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jdvelasq/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Mobility is one of the fundamental requirements of human life with significant societal impacts including productivity, economy, social wellbeing, adaptation to a changing climate, and so on.',\n",
       " 'Although human movements follow specific patterns during normal periods, there are limited studies on how such patterns change due to extreme events.',\n",
       " 'To quantify the impacts of an extreme event to human movements, we introduce the concept of mobility resilience which is defined as the ability of a mobility system to manage shocks and return to a steady state in response to an extreme event.',\n",
       " 'We present a method to detect extreme events from geo-located movement data and to measure mobility resilience and transient loss of resilience due to those events.',\n",
       " 'Applying this method, we measure resilience metrics from geo-located social media data for multiple types of disasters occurred all over the world.',\n",
       " 'Quantifying mobility resilience may help us to assess the higher-order socio-economic impacts of extreme events and guide policies towards developing resilient infrastructures as well as a nation’s overall disaster resilience strategies.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Sentence tokenizer\n",
    "##\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sent_tokenize = nltk.sent_tokenize\n",
    "abstracts = abstracts.map(lambda w: sent_tokenize(text=w), na_action='ignore')\n",
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "## Numero de sentencias por abstract\n",
    "##\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "abstracts.map(lambda w: len(w), na_action=\"ignore\").plot.hist(\n",
    "    color=\"darkorange\", alpha=0.6, rwidth=0.8, edgecolor=\"k\"\n",
    ")\n",
    "\n",
    "plt.gca().spines[\"left\"].set_color(\"lightgray\")\n",
    "plt.gca().spines[\"bottom\"].set_color(\"gray\")\n",
    "plt.gca().spines[\"top\"].set_visible(False)\n",
    "plt.gca().spines[\"right\"].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El proceso de leudado de los productos horneados es fundamental para desarrollar sus propiedades de\\ncalidad.',\n",
       " 'El objetivo de este estudio fue evaluar el efecto de diferentes tipos de polvos para hornear en las \\npropiedades de calidad de muffins.',\n",
       " 'Se evaluaron las propiedades físico-químicas tanto del batido como del producto \\nfinal.',\n",
       " 'Además de su influencia en las propiedades farinológicas de la harina y las propiedades texturales y \\nsensoriales del producto en el almacenamiento.',\n",
       " 'Se encontró la formulación PH16 como la más adecuada, siendo la \\nde mayor altura (47.66 ± 0.35 mm), menor contenido de humedad (24.31 ± 0.18 %), menor dureza (12.34 ± 0.34 N) y \\nfirmeza de miga más baja (1.84 ± 0.01).',\n",
       " 'El comportamiento de la muestra PH16 en almacenamiento y a nivel sensorial \\nno tuvo diferencias significativas con la muestra control seleccionada.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "##  Sentence tokenizer --- Español\n",
    "##\n",
    "text_es = \"\"\"El proceso de leudado de los productos horneados es fundamental para desarrollar sus propiedades de\n",
    "calidad. El objetivo de este estudio fue evaluar el efecto de diferentes tipos de polvos para hornear en las \n",
    "propiedades de calidad de muffins. Se evaluaron las propiedades físico-químicas tanto del batido como del producto \n",
    "final. Además de su influencia en las propiedades farinológicas de la harina y las propiedades texturales y \n",
    "sensoriales del producto en el almacenamiento. Se encontró la formulación PH16 como la más adecuada, siendo la \n",
    "de mayor altura (47.66 ± 0.35 mm), menor contenido de humedad (24.31 ± 0.18 %), menor dureza (12.34 ± 0.34 N) y \n",
    "firmeza de miga más baja (1.84 ± 0.01). El comportamiento de la muestra PH16 en almacenamiento y a nivel sensorial \n",
    "no tuvo diferencias significativas con la muestra control seleccionada.\"\"\"\n",
    "\n",
    "nltk.sent_tokenize(text=text_es, language='spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mobility',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'requirements',\n",
       " 'of',\n",
       " 'human',\n",
       " 'life',\n",
       " 'with',\n",
       " 'significant',\n",
       " 'societal',\n",
       " 'impacts',\n",
       " 'including',\n",
       " 'productivity',\n",
       " ',',\n",
       " 'economy',\n",
       " ',',\n",
       " 'social',\n",
       " 'wellbeing',\n",
       " ',',\n",
       " 'adaptation',\n",
       " 'to',\n",
       " 'a',\n",
       " 'changing',\n",
       " 'climate',\n",
       " ',',\n",
       " 'and',\n",
       " 'so',\n",
       " 'on',\n",
       " '.',\n",
       " 'Although',\n",
       " 'human',\n",
       " 'movements',\n",
       " 'follow',\n",
       " 'specific',\n",
       " 'patterns',\n",
       " 'during',\n",
       " 'normal']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Default word tokenization\n",
    "##\n",
    "\n",
    "#\n",
    "# Se extraen nuevamente los abstracts\n",
    "#\n",
    "abstracts = data.Abstract.copy()\n",
    "abstracts = abstracts.map(lambda w: w[0: w.find(\"\\u00a9\")], na_action='ignore')\n",
    "\n",
    "#\n",
    "# Default word tokenizer\n",
    "#  Es una instancia del Treebank word tokenizer\n",
    "#\n",
    "default_word_tokenize = nltk.word_tokenize\n",
    "abstracts.map(default_word_tokenize, na_action='ignore')[0][0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mobility',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'requirements',\n",
       " 'of',\n",
       " 'human',\n",
       " 'life',\n",
       " 'with',\n",
       " 'significant',\n",
       " 'societal',\n",
       " 'impacts',\n",
       " 'including',\n",
       " 'productivity',\n",
       " ',',\n",
       " 'economy',\n",
       " ',',\n",
       " 'social',\n",
       " 'wellbeing',\n",
       " ',',\n",
       " 'adaptation',\n",
       " 'to',\n",
       " 'a',\n",
       " 'changing',\n",
       " 'climate',\n",
       " ',',\n",
       " 'and',\n",
       " 'so',\n",
       " 'on.',\n",
       " 'Although',\n",
       " 'human',\n",
       " 'movements',\n",
       " 'follow',\n",
       " 'specific',\n",
       " 'patterns',\n",
       " 'during',\n",
       " 'normal',\n",
       " 'periods']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## TokTok tokenizer\n",
    "##\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "# note que no separa los puntos de sentencia intermedios en el parrafo\n",
    "toktok_word_tokenizer = nltk.ToktokTokenizer()\n",
    "abstracts.map(toktok_word_tokenizer.tokenize, na_action='ignore')[0][0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mobility',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'requirements',\n",
       " 'of',\n",
       " 'human',\n",
       " 'life',\n",
       " 'with',\n",
       " 'significant',\n",
       " 'societal',\n",
       " 'impacts',\n",
       " 'including',\n",
       " 'productivity',\n",
       " 'economy',\n",
       " 'social',\n",
       " 'wellbeing',\n",
       " 'adaptation',\n",
       " 'to',\n",
       " 'a',\n",
       " 'changing',\n",
       " 'climate',\n",
       " 'and',\n",
       " 'so',\n",
       " 'on',\n",
       " 'Although',\n",
       " 'human',\n",
       " 'movements',\n",
       " 'follow',\n",
       " 'specific',\n",
       " 'patterns',\n",
       " 'during',\n",
       " 'normal',\n",
       " 'periods',\n",
       " 'there',\n",
       " 'are',\n",
       " 'limited',\n",
       " 'studies']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Regexp tokenizer\n",
    "##\n",
    "from nltk import RegexpTokenizer\n",
    "\n",
    "TOKEN_PATTERN = r'\\w+'\n",
    "\n",
    "regex_tokenizer = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
    "abstracts.map(regex_tokenizer.tokenize, na_action='ignore')[0][0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 8),\n",
       " (9, 11),\n",
       " (12, 15),\n",
       " (16, 18),\n",
       " (19, 22),\n",
       " (23, 34),\n",
       " (35, 47),\n",
       " (48, 50),\n",
       " (51, 56),\n",
       " (57, 61),\n",
       " (62, 66),\n",
       " (67, 78),\n",
       " (79, 87),\n",
       " (88, 95),\n",
       " (96, 105),\n",
       " (106, 118),\n",
       " (120, 127),\n",
       " (129, 135),\n",
       " (136, 145),\n",
       " (147, 157),\n",
       " (158, 160),\n",
       " (161, 162),\n",
       " (163, 171),\n",
       " (172, 179),\n",
       " (181, 184),\n",
       " (185, 187),\n",
       " (188, 190),\n",
       " (192, 200),\n",
       " (201, 206),\n",
       " (207, 216),\n",
       " (217, 223),\n",
       " (224, 232),\n",
       " (233, 241),\n",
       " (242, 248),\n",
       " (249, 255),\n",
       " (256, 263),\n",
       " (265, 270),\n",
       " (271, 274),\n",
       " (275, 282),\n",
       " (283, 290)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Posiciones de los tokens en el texto\n",
    "##\n",
    "abstracts.map(lambda w: list(regex_tokenizer.span_tokenize(w)), na_action=\"ignore\")[0][\n",
    "    0:40\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mobility',\n",
       "  'is',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'fundamental',\n",
       "  'requirements',\n",
       "  'of',\n",
       "  'human',\n",
       "  'life',\n",
       "  'with',\n",
       "  'significant',\n",
       "  'societal',\n",
       "  'impacts',\n",
       "  'including',\n",
       "  'productivity',\n",
       "  ',',\n",
       "  'economy',\n",
       "  ',',\n",
       "  'social',\n",
       "  'wellbeing',\n",
       "  ',',\n",
       "  'adaptation',\n",
       "  'to',\n",
       "  'a',\n",
       "  'changing',\n",
       "  'climate',\n",
       "  ',',\n",
       "  'and',\n",
       "  'so',\n",
       "  'on',\n",
       "  '.'],\n",
       " ['Although',\n",
       "  'human',\n",
       "  'movements',\n",
       "  'follow',\n",
       "  'specific',\n",
       "  'patterns',\n",
       "  'during',\n",
       "  'normal',\n",
       "  'periods',\n",
       "  ',',\n",
       "  'there',\n",
       "  'are',\n",
       "  'limited',\n",
       "  'studies',\n",
       "  'on',\n",
       "  'how',\n",
       "  'such',\n",
       "  'patterns',\n",
       "  'change',\n",
       "  'due',\n",
       "  'to',\n",
       "  'extreme',\n",
       "  'events',\n",
       "  '.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Tokenizadores robustos\n",
    "##\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "words = abstracts.map(tokenize_text, na_action='ignore')\n",
    "\n",
    "#\n",
    "# Dos primeras lineas del primer abstract\n",
    "#\n",
    "words[0][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoción de acentos y caracteres especiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AEIOUNaeiouaiou'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Remocion de acentos\n",
    "##\n",
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "remove_accented_chars('ÁÉÍÓÚÑáéíóúäïöü')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun What do you think '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Remoción de caracterires especiales\n",
    "## \n",
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]' \n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "remove_special_characters(\"Well this was fun! What do you think? 123#@!\", remove_digits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrección de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finally'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Correccion de texto --- usando textblob\n",
    "##     (Otras librerias: PyEnchant, aspell-python)\n",
    "##\n",
    "# !pip3 install --user textblob\n",
    "\n",
    "from textblob import Word\n",
    "\n",
    "w = Word('fianlly') \n",
    "w.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('finally', 1.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check suggestions \n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flat', 0.85), ('float', 0.15)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('flaot') \n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump', 'lie', 'strang')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Stemming\n",
    "##\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped'), ps.stem('lying'), ps.stem('strange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump', 'lying', 'strange')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'), ls.stem('lying'), ls.stem('strange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump', 'ly', 'strange')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped'), rs.stem('lying'), rs.stem('strange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump', 'lie', 'strang')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "ss = SnowballStemmer(\"english\")\n",
    "ss.stem('jumping'), ss.stem('jumps'), ss.stem('jumped'), ss.stem('lying'), ss.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n",
      "running\n",
      "ate\n",
      "saddest\n",
      "fancier\n",
      "----\n",
      "car\n",
      "men\n",
      "run\n",
      "eat\n",
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Lemmatization\n",
    "## \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "print(wnl.lemmatize('cars'))\n",
    "print(wnl.lemmatize('men'))\n",
    "print(wnl.lemmatize('running'))\n",
    "print(wnl.lemmatize('ate'))\n",
    "print(wnl.lemmatize('saddest'))\n",
    "print(wnl.lemmatize('fancier'))\n",
    "print('----')\n",
    "print(wnl.lemmatize('cars', 'n'))  # n --> nouns\n",
    "print(wnl.lemmatize('men', 'n'))\n",
    "print(wnl.lemmatize('running', 'v')) # v --> verbs\n",
    "print(wnl.lemmatize('ate', 'v'))\n",
    "print(wnl.lemmatize('saddest', 'a')) # --> adjectves\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash ! his crash yesterday , ours crash daily'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## spaCy hace la lematizaction basado en speech tagging\n",
    "##   !pip3 install spacy\n",
    "##   !python3 -m spacy download en_core_web_sm\n",
    "##\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "\n",
    "def lemmatize_text(text): \n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esquema basico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"',\n",
       " \"'\",\n",
       " ',',\n",
       " ',\"',\n",
       " '-',\n",
       " '-------------',\n",
       " '----------------------------------------------------------------------------------',\n",
       " '.',\n",
       " '.\"',\n",
       " '/',\n",
       " '01',\n",
       " '02',\n",
       " '09',\n",
       " '11',\n",
       " '12',\n",
       " '17',\n",
       " '200',\n",
       " '2002',\n",
       " '2202',\n",
       " '27']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Clean html\n",
    "##\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# captura\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "\n",
    "# se remueven las etiquetas\n",
    "\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "\n",
    "# tokenizer\n",
    "tokens = nltk.wordpunct_tokenize(raw)\n",
    "\n",
    "# tokens --> text\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "# normalizacion\n",
    "#  remocion de puntuacion, acentos, numeros, puntuacion, ....\n",
    "words = [w.lower() for w in text]\n",
    "\n",
    "# vocabulario\n",
    "sorted(set(words))[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
