{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging y Shallow Parsing\n",
    "\n",
    "* *30 min* | Última modificación: Sept 22, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/book/\n",
    "\n",
    "Text Analytics with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts-of-Speech (POS) tagging / Categorización léxica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TAG   Descripción      Ejemplo\n",
    "------------------------------------------------------------\n",
    "CC    Coordination conjuntion                and, or\n",
    "CD    Cardinal number                        one, two, 3\n",
    "DT    Determiner                             a, the\n",
    "EX    Existential there                      there were two cars \n",
    "FW    Foreign word                           hola mundo cruel \n",
    "IN    Preposition/subordinating conjunction  of, in, on, that\n",
    "JJ    Adjective                              quick, lazy\n",
    "JJR   Adjective, comparative                 quicker, lazier\n",
    "JJS   Adjective, superlative                 quickest, laziest\n",
    "NN    Noun, singular or mass                 fox, dog\n",
    "NNS   Noun, plural                           foxes, dogs\n",
    "NNPS  Noun, proper singular                  John, Alice  \n",
    "NNP   Noun, proper plural                    Vikings, Indians, Germans\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "##\n",
    "## Para ver todos los posibles tags ejecute el siguiente codigo\n",
    "##\n",
    "# nltk.download('tagsets')\n",
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS tag</th>\n",
       "      <th>Tag type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unveils</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'s</td>\n",
       "      <td>POS</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>most</td>\n",
       "      <td>RBS</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>powerful</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>supercomputer</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>beats</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>China</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word POS tag Tag type\n",
       "0              US     NNP    PROPN\n",
       "1         unveils     VBZ     VERB\n",
       "2           world      NN     NOUN\n",
       "3              's     POS     PART\n",
       "4            most     RBS      ADV\n",
       "5        powerful      JJ      ADJ\n",
       "6   supercomputer      NN     NOUN\n",
       "7               ,       ,    PUNCT\n",
       "8           beats     VBZ     VERB\n",
       "9           China     NNP    PROPN\n",
       "10              .       .    PUNCT"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Ejemplo --- POS tagging usando spaCy\n",
    "##\n",
    "sentence = \"US unveils world's most powerful supercomputer, beats China.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", parse=True, tag=True, entity=True)\n",
    "\n",
    "sentence_nlp = nlp(sentence)\n",
    "\n",
    "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
    "pd.DataFrame(spacy_pos_tagged, columns=[\"Word\", \"POS tag\", \"Tag type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jdvelasq/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unveils</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'s</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>most</td>\n",
       "      <td>RBS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>powerful</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>supercomputer</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>beats</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>China</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word POS tag\n",
       "0              US     NNP\n",
       "1         unveils      JJ\n",
       "2           world      NN\n",
       "3              's     POS\n",
       "4            most     RBS\n",
       "5        powerful      JJ\n",
       "6   supercomputer      NN\n",
       "7               ,       ,\n",
       "8           beats     VBZ\n",
       "9           China     NNP\n",
       "10              .       ."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Ejemplo --- POS tagging usando NLTK\n",
    "## \n",
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk_pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence)) \n",
    "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Noun phrase (NP): El sustantivo (noun) encabeza la frase. Una NP atua como el sujeto u objeto de un verbo.\n",
    "\n",
    "\n",
    "* Verb phrase (VP): El verbo (verb) encabeza la frase. \n",
    "\n",
    "\n",
    "* Adjective phrase (ADJP): El adjetivo es la cabeza. Califica sustantivsos y pronombres en la sentencia.\n",
    "\n",
    "\n",
    "* Adverb phrase (ADVP): frases que actuan como adverbios\n",
    "\n",
    "\n",
    "* Preprositional phrase (PP): tienen una preposicion al inicio de la frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Ejemplo del resultado de una shallow parser\n",
    "##\n",
    "from nltk.corpus import treebank_chunk\n",
    "\n",
    "# nltk.download('treebank')\n",
    "\n",
    "data = treebank_chunk.chunked_sents()\n",
    "\n",
    "train_data = data[:3500] \n",
    "test_data = data[3500:]\n",
    "\n",
    "print(train_data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('US', 'NNP'), ('unveils', 'JJ'), ('world', 'NN'), (\"'s\", 'POS'), ('most', 'RBS'), ('powerful', 'JJ'), ('supercomputer', 'NN'), (',', ','), ('beats', 'VBZ'), ('China', 'NNP'), ('.', '.')]\n",
      "\n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  (NP unveils/JJ world/NN)\n",
      "  's/POS\n",
      "  most/RBS\n",
      "  (NP powerful/JJ supercomputer/NN)\n",
      "  ,/,\n",
      "  beats/VBZ\n",
      "  (NP China/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Especificacion de un regexp parser\n",
    "##\n",
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "sentence = \"US unveils world's most powerful supercomputer, beats China.\"\n",
    "\n",
    "tagged_simple_sent = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print('POS Tags:', tagged_simple_sent)\n",
    "\n",
    "chunk_grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN.*>}\n",
    "\"\"\"\n",
    "\n",
    "rc = RegexpParser(chunk_grammar) \n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP US/NNP)\n",
      "  unveils/JJ\n",
      "  (NP world/NN 's/POS most/RBS)\n",
      "  powerful/JJ\n",
      "  (NP supercomputer/NN ,/,)\n",
      "  beats/VBZ\n",
      "  (NP China/NNP ./.))\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Chink --- lo que no es reconocido\n",
    "##\n",
    "chink_grammar = \"\"\"\n",
    "NP:\n",
    "   {<.*>+}                # Chunk everything as NP\n",
    "   }<VBZ|VBD|JJ|IN>+{     # Chink sequences of VBD\\VBZ\\JJ\\IN\n",
    "\"\"\"\n",
    "\n",
    "rc = RegexpParser(chink_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP US/NNP)\n",
      "  (NP unveils/JJ world/NN)\n",
      "  's/POS\n",
      "  (ADVP most/RBS)\n",
      "  (NP powerful/JJ supercomputer/NN)\n",
      "  ,/,\n",
      "  (VP beats/VBZ)\n",
      "  (NP China/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Mejoras\n",
    "##\n",
    "grammar = \"\"\"\n",
    "NP:   {<DT>?<JJ>?<NN.*>}\n",
    "ADJP: {<JJ>}\n",
    "ADVP: {<RB.*>}\n",
    "PP:   {<IN>}\n",
    "VP:   {<MD>?<VB.*>+}\n",
    "\"\"\"\n",
    "\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  46.1%%\n",
      "    Precision:     19.9%%\n",
      "    Recall:        43.3%%\n",
      "    F-Measure:     27.3%%\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Evaluacion\n",
    "##\n",
    "print(rc.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## B- begining of the chunk\n",
    "## I- inside a chunk\n",
    "## O- no pertenece a ningun chunk\n",
    "##\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "\n",
    "train_sent = train_data[7]\n",
    "print(train_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT', 'B-NP'),\n",
       " ('Lorillard', 'NNP', 'I-NP'),\n",
       " ('spokewoman', 'NN', 'I-NP'),\n",
       " ('said', 'VBD', 'O'),\n",
       " (',', ',', 'O'),\n",
       " ('``', '``', 'O'),\n",
       " ('This', 'DT', 'B-NP'),\n",
       " ('is', 'VBZ', 'O'),\n",
       " ('an', 'DT', 'B-NP'),\n",
       " ('old', 'JJ', 'I-NP'),\n",
       " ('story', 'NN', 'I-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtc = tree2conlltags(train_sent)\n",
    "wtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "tree = conlltags2tree(wtc)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  97.2%%\n",
      "    Precision:     91.4%%\n",
      "    Recall:        94.3%%\n",
      "    F-Measure:     92.8%%\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    def __init__(self, train_sentences, tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "        train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "        self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "\n",
    "    def parse(self, tagged_sentence):\n",
    "        if not tagged_sentence:\n",
    "            return None\n",
    "        \n",
    "        pos_tags = [tag for word, tag in tagged_sentence]\n",
    "        chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "        chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "        wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag) in zip(tagged_sentence, chunk_tags)]\n",
    "        return conlltags2tree(wpc_tags)\n",
    "    \n",
    "ntc = NGramTagChunker(train_data)\n",
    "print(ntc.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-833c8b4a0527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtagged_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_nlp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mntc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "sentence_nlp = nlp(sentence)\n",
    "tagged_sentence = [(word.text, word.tag_) for word in sentence_nlp]\n",
    "tree = ntc.parse(tagged_sentence)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:10000]\n",
    "test_wsj_data = wsj_data[10000:]\n",
    "\n",
    "print(train_wsj_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.1%%\n",
      "    Precision:     80.3%%\n",
      "    Recall:        86.1%%\n",
      "    F-Measure:     83.1%%\n"
     ]
    }
   ],
   "source": [
    "tc = NGramTagChunker(train_wsj_data)\n",
    "print(tc.evaluate(test_wsj_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagged_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-029c411ee035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tagged_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "tree = tc.parse(tagged_sentence)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:10000]\n",
    "test_wsj_data = wsj_data[10000:]\n",
    "print(train_wsj_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.1%%\n",
      "    Precision:     80.3%%\n",
      "    Recall:        86.1%%\n",
      "    F-Measure:     83.1%%\n"
     ]
    }
   ],
   "source": [
    "tc = NGramTagChunker(train_wsj_data)\n",
    "\n",
    "print(tc.evaluate(test_wsj_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagged_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-029c411ee035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tagged_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "tree = tc.parse(tagged_sentence)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
