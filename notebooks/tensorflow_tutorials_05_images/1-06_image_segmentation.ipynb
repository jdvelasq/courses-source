{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88123ee0-467c-4466-be2a-51c77039c123",
   "metadata": {
    "tags": []
   },
   "source": [
    "Segmentación de imágenes --- 0:00 min\n",
    "===\n",
    "\n",
    "* Última modificación: Marzo 1, 2022 | YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886b6c7-6883-4b2e-972e-0c118bc95529",
   "metadata": {
    "tags": []
   },
   "source": [
    "Importación de librerías\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7ff3e-98a4-471e-985d-9cfa265b0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4ac54-d823-4a75-bee4-5915bfa9d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407202c-3fd7-4bfb-ae5b-3ce9e5b7da30",
   "metadata": {},
   "source": [
    "Descarga del dataset\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bb053-4908-4342-a44a-b947d4ee969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load(\n",
    "    \"oxford_iiit_pet:3.*.*\",\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69b750-71ee-4957-bbd6-6b9e0362c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_image, input_mask):\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    input_mask -= 1\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b7dbf-7374-4819-91c2-eead0253b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(datapoint):\n",
    "    input_image = tf.image.resize(datapoint[\"image\"], (128, 128))\n",
    "    input_mask = tf.image.resize(datapoint[\"segmentation_mask\"], (128, 128))\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9aa6a-b00b-438b-ad4b-f59bc89a54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = info.splits[\"train\"].num_examples\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79a40b-c703-41a3-ba42-49a78a0ad089",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = dataset[\"train\"].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_images = dataset[\"test\"].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e55591e-86fd-47fe-87d6-23274cf2c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augment(tf.keras.layers.Layer):\n",
    "    def __init__(self, seed=42):\n",
    "        super().__init__()\n",
    "        # both use the same seed, so they'll make the same random changes.\n",
    "        self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "        self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "\n",
    "    def call(self, inputs, labels):\n",
    "        inputs = self.augment_inputs(inputs)\n",
    "        labels = self.augment_labels(labels)\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f2120-c701-4e0a-807c-81323d4bedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = (\n",
    "    train_images.cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .map(Augment())\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_batches = test_images.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1133a26-472e-4d39-aebd-c1137c88be32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = [\"Input Image\", \"True Mask\", \"Predicted Mask\"]\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e888e-24d0-4d41-b8e3-eda54ed4c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, masks in train_batches.take(2):\n",
    "    sample_image, sample_mask = images[0], masks[0]\n",
    "    display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c485c948-c278-43ec-b802-baa42aaee249",
   "metadata": {},
   "source": [
    "Definición del modelo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27454e1-2d2e-45db-a7b2-2e763c76d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=[128, 128, 3],\n",
    "    include_top=False,\n",
    ")\n",
    "\n",
    "\n",
    "layer_names = [\n",
    "    \"block_1_expand_relu\",  # 64x64\n",
    "    \"block_3_expand_relu\",  # 32x32\n",
    "    \"block_6_expand_relu\",  # 16x16\n",
    "    \"block_13_expand_relu\",  # 8x8\n",
    "    \"block_16_project\",  # 4x4\n",
    "]\n",
    "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "\n",
    "down_stack = tf.keras.Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=base_model_outputs,\n",
    ")\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b9de8-148e-4dc1-b136-46619346fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "    pix2pix.upsample(64, 3),  # 32x32 -> 64x64\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacefdb6-07c4-4523-8f69-36ea2c98b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(output_channels: int):\n",
    "    inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(inputs)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = tf.keras.layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    # This is the last layer of the model\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=output_channels, kernel_size=3, strides=2, padding=\"same\"\n",
    "    )  # 64x64 -> 128x128\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7b68a-945a-44a0-b72a-57169a5a9d69",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8c46a-0be5-4861-85fb-0d0ab21785c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CLASSES = 3\n",
    "\n",
    "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973fba0-b8c2-461e-bf6a-3034392e8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e928e31-5c5f-4277-9bea-8681cbc2bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76875763-eacd-4c07-bcd4-c129076d7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display([image[0], mask[0], create_mask(pred_mask)])\n",
    "    else:\n",
    "        display(\n",
    "            [\n",
    "                sample_image,\n",
    "                sample_mask,\n",
    "                create_mask(model.predict(sample_image[tf.newaxis, ...])),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42380db-482d-494e-bf81-70daec046c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c343f8-f47a-4fb3-bef7-b8ed2c2afc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print(\"\\nSample Prediction after epoch {}\\n\".format(epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed8b5c-aba4-4afd-b315-7f6fa94d6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = info.splits[\"test\"].num_examples // BATCH_SIZE // VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(\n",
    "    train_batches,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_steps=VALIDATION_STEPS,\n",
    "    validation_data=test_batches,\n",
    "    callbacks=[DisplayCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e4801-1b64-4913-a633-31e8e46d3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model_history.history[\"loss\"]\n",
    "val_loss = model_history.history[\"val_loss\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model_history.epoch, loss, \"r\", label=\"Training loss\")\n",
    "plt.plot(model_history.epoch, val_loss, \"bo\", label=\"Validation loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb055c-04d6-4c68-b53c-2e4d147819e3",
   "metadata": {},
   "source": [
    "Pronóstico\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0a7a0-4925-444c-8d8d-24db895ee02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(test_batches, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82bf1bb-6ab5-4b72-9d2b-642345cab73b",
   "metadata": {},
   "source": [
    "Clases no balanceadas\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4878f6-c508-4b0a-9f07-f1d7cf3e2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_history = model.fit(\n",
    "        train_batches,\n",
    "        epochs=EPOCHS,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        class_weight={0: 2.0, 1: 2.0, 2: 1.0},\n",
    "    )\n",
    "    assert False\n",
    "except Exception as e:\n",
    "    print(f\"Expected {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4524f-1bf6-4f95-8162-d287e976303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [0, 0]\n",
    "prediction = [[-3.0, 0], [-3, 0]]\n",
    "sample_weight = [1, 10]\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=tf.losses.Reduction.NONE\n",
    ")\n",
    "loss(label, prediction, sample_weight).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa0cdc-1ae1-4db0-87ac-b8492082075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sample_weights(image, label):\n",
    "    # The weights for each class, with the constraint that:\n",
    "    #     sum(class_weights) == 1.0\n",
    "    class_weights = tf.constant([2.0, 2.0, 1.0])\n",
    "    class_weights = class_weights / tf.reduce_sum(class_weights)\n",
    "\n",
    "    # Create an image of `sample_weights` by using the label at each pixel as an\n",
    "    # index into the `class weights` .\n",
    "    sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
    "\n",
    "    return image, label, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba214001-b72a-4c1a-bd4d-d74d104bc097",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches.map(add_sample_weights).element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d92da-26a1-40a2-b05f-0a20db6a1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model = unet_model(OUTPUT_CLASSES)\n",
    "weighted_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca69da-3420-4dc9-8453-00562c05b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model.fit(train_batches.map(add_sample_weights), epochs=1, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4799a-e3f4-4cc2-ac0b-829bad49a62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
