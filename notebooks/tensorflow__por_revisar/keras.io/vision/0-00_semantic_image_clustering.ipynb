{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Semantic Image Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_data = np.concatenate([x_train, x_test])\n",
    "y_data = np.concatenate([y_train, y_test])\n",
    "\n",
    "print(\"x_data shape:\", x_data.shape, \"- y_data shape:\", y_data.shape)\n",
    "\n",
    "classes = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "target_size = 32  # Resize the input images.\n",
    "representation_dim = 512  # The dimensions of the features vector.\n",
    "projection_units = 128  # The projection head of the representation learner.\n",
    "num_clusters = 20  # Number of clusters.\n",
    "k_neighbours = 5  # Number of neighbours to consider during cluster learning.\n",
    "tune_encoder_during_clustering = False  # Freeze the encoder in the cluster learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "data_preprocessing = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Resizing(target_size, target_size),\n",
    "        layers.experimental.preprocessing.Normalization(),\n",
    "    ]\n",
    ")\n",
    "# Compute the mean and the variance from the data for normalization.\n",
    "data_preprocessing.layers[-1].adapt(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomTranslation(\n",
    "            height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2), fill_mode=\"nearest\"\n",
    "        ),\n",
    "        layers.experimental.preprocessing.RandomFlip(mode=\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(\n",
    "            factor=0.15, fill_mode=\"nearest\"\n",
    "        ),\n",
    "        layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor=(-0.3, 0.1), width_factor=(-0.3, 0.1), fill_mode=\"nearest\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "image_idx = np.random.choice(range(x_data.shape[0]))\n",
    "image = x_data[image_idx]\n",
    "image_class = classes[y_data[image_idx][0]]\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(x_data[image_idx].astype(\"uint8\"))\n",
    "plt.title(image_class)\n",
    "_ = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    augmented_images = data_augmentation(np.array([image]))\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def create_encoder(representation_dim):\n",
    "    encoder = keras.Sequential(\n",
    "        [\n",
    "            keras.applications.ResNet50V2(\n",
    "                include_top=False, weights=None, pooling=\"avg\"\n",
    "            ),\n",
    "            layers.Dense(representation_dim),\n",
    "        ]\n",
    "    )\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class RepresentationLearner(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        projection_units,\n",
    "        num_augmentations,\n",
    "        temperature=1.0,\n",
    "        dropout_rate=0.1,\n",
    "        l2_normalize=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(RepresentationLearner, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        # Create projection head.\n",
    "        self.projector = keras.Sequential(\n",
    "            [\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(units=projection_units, use_bias=False),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.ReLU(),\n",
    "            ]\n",
    "        )\n",
    "        self.num_augmentations = num_augmentations\n",
    "        self.temperature = temperature\n",
    "        self.l2_normalize = l2_normalize\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def compute_contrastive_loss(self, feature_vectors, batch_size):\n",
    "        num_augmentations = tf.shape(feature_vectors)[0] // batch_size\n",
    "        if self.l2_normalize:\n",
    "            feature_vectors = tf.math.l2_normalize(feature_vectors, -1)\n",
    "        # The logits shape is [num_augmentations * batch_size, num_augmentations * batch_size].\n",
    "        logits = (\n",
    "            tf.linalg.matmul(feature_vectors, feature_vectors, transpose_b=True)\n",
    "            / self.temperature\n",
    "        )\n",
    "        # Apply log-max trick for numerical stability.\n",
    "        logits_max = tf.math.reduce_max(logits, axis=1)\n",
    "        logits = logits - logits_max\n",
    "        # The shape of targets is [num_augmentations * batch_size, num_augmentations * batch_size].\n",
    "        # targets is a matrix consits of num_augmentations submatrices of shape [batch_size * batch_size].\n",
    "        # Each [batch_size * batch_size] submatrix is an identity matrix (diagonal entries are ones).\n",
    "        targets = tf.tile(tf.eye(batch_size), [num_augmentations, num_augmentations])\n",
    "        # Compute cross entropy loss\n",
    "        return keras.losses.categorical_crossentropy(\n",
    "            y_true=targets, y_pred=logits, from_logits=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Preprocess the input images.\n",
    "        preprocessed = data_preprocessing(inputs)\n",
    "        # Create augmented versions of the images.\n",
    "        augmented = []\n",
    "        for _ in range(self.num_augmentations):\n",
    "            augmented.append(data_augmentation(preprocessed))\n",
    "        augmented = layers.Concatenate(axis=0)(augmented)\n",
    "        # Generate embedding representations of the images.\n",
    "        features = self.encoder(augmented)\n",
    "        # Apply projection head.\n",
    "        return self.projector(features)\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        # Run the forward pass and compute the contrastive loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            feature_vectors = self(inputs, training=True)\n",
    "            loss = self.compute_contrastive_loss(feature_vectors, batch_size)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update loss tracker metric\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        feature_vectors = self(inputs, training=False)\n",
    "        loss = self.compute_contrastive_loss(feature_vectors, batch_size)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create vision encoder.\n",
    "encoder = create_encoder(representation_dim)\n",
    "# Create representation learner.\n",
    "representation_learner = RepresentationLearner(\n",
    "    encoder, projection_units, num_augmentations=2, temperature=0.1\n",
    ")\n",
    "# Create a a Cosine decay learning rate scheduler.\n",
    "lr_scheduler = keras.experimental.CosineDecay(\n",
    "    initial_learning_rate=0.001, decay_steps=500, alpha=0.1\n",
    ")\n",
    "# Compile the model.\n",
    "representation_learner.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(learning_rate=lr_scheduler, weight_decay=0.0001),\n",
    ")\n",
    "# Fit the model.\n",
    "history = representation_learner.fit(\n",
    "    x=x_data,\n",
    "    batch_size=512,\n",
    "    epochs=50,  # for better results, increase the number of epochs to 500.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "# Get the feature vector representations of the images.\n",
    "feature_vectors = encoder.predict(x_data, batch_size=batch_size, verbose=1)\n",
    "# Normalize the feature vectores.\n",
    "feature_vectors = tf.math.l2_normalize(feature_vectors, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "neighbours = []\n",
    "num_batches = feature_vectors.shape[0] // batch_size\n",
    "for batch_idx in tqdm(range(num_batches)):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    current_batch = feature_vectors[start_idx:end_idx]\n",
    "    # Compute the dot similarity.\n",
    "    similarities = tf.linalg.matmul(current_batch, feature_vectors, transpose_b=True)\n",
    "    # Get the indices of most similar vectors.\n",
    "    _, indices = tf.math.top_k(similarities, k=k_neighbours + 1, sorted=True)\n",
    "    # Add the indices to the neighbours.\n",
    "    neighbours.append(indices[..., 1:])\n",
    "\n",
    "neighbours = np.reshape(np.array(neighbours), (-1, k_neighbours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "nrows = 4\n",
    "ncols = k_neighbours + 1\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "position = 1\n",
    "for _ in range(nrows):\n",
    "    anchor_idx = np.random.choice(range(x_data.shape[0]))\n",
    "    neighbour_indicies = neighbours[anchor_idx]\n",
    "    indices = [anchor_idx] + neighbour_indicies.tolist()\n",
    "    for j in range(ncols):\n",
    "        plt.subplot(nrows, ncols, position)\n",
    "        plt.imshow(x_data[indices[j]].astype(\"uint8\"))\n",
    "        plt.title(classes[y_data[indices[j]][0]])\n",
    "        plt.axis(\"off\")\n",
    "        position += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class ClustersConsistencyLoss(keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(ClustersConsistencyLoss, self).__init__()\n",
    "\n",
    "    def __call__(self, target, similarity, sample_weight=None):\n",
    "        # Set targets to be ones.\n",
    "        target = tf.ones_like(similarity)\n",
    "        # Compute cross entropy loss.\n",
    "        loss = keras.losses.binary_crossentropy(\n",
    "            y_true=target, y_pred=similarity, from_logits=True\n",
    "        )\n",
    "        return tf.math.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class ClustersEntropyLoss(keras.losses.Loss):\n",
    "    def __init__(self, entropy_loss_weight=1.0):\n",
    "        super(ClustersEntropyLoss, self).__init__()\n",
    "        self.entropy_loss_weight = entropy_loss_weight\n",
    "\n",
    "    def __call__(self, target, cluster_probabilities, sample_weight=None):\n",
    "        # Ideal entropy = log(num_clusters).\n",
    "        num_clusters = tf.cast(tf.shape(cluster_probabilities)[-1], tf.dtypes.float32)\n",
    "        target = tf.math.log(num_clusters)\n",
    "        # Compute the overall clusters distribution.\n",
    "        cluster_probabilities = tf.math.reduce_mean(cluster_probabilities, axis=0)\n",
    "        # Replacing zero probabilities - if any - with a very small value.\n",
    "        cluster_probabilities = tf.clip_by_value(\n",
    "            cluster_probabilities, clip_value_min=1e-8, clip_value_max=1.0\n",
    "        )\n",
    "        # Compute the entropy over the clusters.\n",
    "        entropy = -tf.math.reduce_sum(\n",
    "            cluster_probabilities * tf.math.log(cluster_probabilities)\n",
    "        )\n",
    "        # Compute the difference between the target and the actual.\n",
    "        loss = target - entropy\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def create_clustering_model(encoder, num_clusters, name=None):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Preprocess the input images.\n",
    "    preprocessed = data_preprocessing(inputs)\n",
    "    # Apply data augmentation to the images.\n",
    "    augmented = data_augmentation(preprocessed)\n",
    "    # Generate embedding representations of the images.\n",
    "    features = encoder(augmented)\n",
    "    # Assign the images to clusters.\n",
    "    outputs = layers.Dense(units=num_clusters, activation=\"softmax\")(features)\n",
    "    # Create the model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def create_clustering_learner(clustering_model):\n",
    "    anchor = keras.Input(shape=input_shape, name=\"anchors\")\n",
    "    neighbours = keras.Input(\n",
    "        shape=tuple([k_neighbours]) + input_shape, name=\"neighbours\"\n",
    "    )\n",
    "    # Changes neighbours shape to [batch_size * k_neighbours, width, height, channels]\n",
    "    neighbours_reshaped = tf.reshape(neighbours, shape=tuple([-1]) + input_shape)\n",
    "    # anchor_clustering shape: [batch_size, num_clusters]\n",
    "    anchor_clustering = clustering_model(anchor)\n",
    "    # neighbours_clustering shape: [batch_size * k_neighbours, num_clusters]\n",
    "    neighbours_clustering = clustering_model(neighbours_reshaped)\n",
    "    # Convert neighbours_clustering shape to [batch_size, k_neighbours, num_clusters]\n",
    "    neighbours_clustering = tf.reshape(\n",
    "        neighbours_clustering,\n",
    "        shape=(-1, k_neighbours, tf.shape(neighbours_clustering)[-1]),\n",
    "    )\n",
    "    # similarity shape: [batch_size, 1, k_neighbours]\n",
    "    similarity = tf.linalg.einsum(\n",
    "        \"bij,bkj->bik\", tf.expand_dims(anchor_clustering, axis=1), neighbours_clustering\n",
    "    )\n",
    "    # similarity shape:  [batch_size, k_neighbours]\n",
    "    similarity = layers.Lambda(lambda x: tf.squeeze(x, axis=1), name=\"similarity\")(\n",
    "        similarity\n",
    "    )\n",
    "    # Create the model.\n",
    "    model = keras.Model(\n",
    "        inputs=[anchor, neighbours],\n",
    "        outputs=[similarity, anchor_clustering],\n",
    "        name=\"clustering_learner\",\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# If tune_encoder_during_clustering is set to False,\n",
    "# then freeze the encoder weights.\n",
    "for layer in encoder.layers:\n",
    "    layer.trainable = tune_encoder_during_clustering\n",
    "# Create the clustering model and learner.\n",
    "clustering_model = create_clustering_model(encoder, num_clusters, name=\"clustering\")\n",
    "clustering_learner = create_clustering_learner(clustering_model)\n",
    "# Instantiate the model losses.\n",
    "losses = [ClustersConsistencyLoss(), ClustersEntropyLoss(entropy_loss_weight=5)]\n",
    "# Create the model inputs and labels.\n",
    "inputs = {\"anchors\": x_data, \"neighbours\": tf.gather(x_data, neighbours)}\n",
    "labels = tf.ones(shape=(x_data.shape[0]))\n",
    "# Compile the model.\n",
    "clustering_learner.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(learning_rate=0.0005, weight_decay=0.0001),\n",
    "    loss=losses,\n",
    ")\n",
    "\n",
    "# Begin training the model.\n",
    "clustering_learner.fit(x=inputs, y=labels, batch_size=512, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Get the cluster probability distribution of the input images.\n",
    "clustering_probs = clustering_model.predict(x_data, batch_size=batch_size, verbose=1)\n",
    "# Get the cluster of the highest probability.\n",
    "cluster_assignments = tf.math.argmax(clustering_probs, axis=-1).numpy()\n",
    "# Store the clustering confidence.\n",
    "# Images with the highest clustering confidence are considered the 'prototypes'\n",
    "# of the clusters.\n",
    "cluster_confidence = tf.math.reduce_max(clustering_probs, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "clusters = defaultdict(list)\n",
    "for idx, c in enumerate(cluster_assignments):\n",
    "    clusters[c].append((idx, cluster_confidence[idx]))\n",
    "\n",
    "for c in range(num_clusters):\n",
    "    print(\"cluster\", c, \":\", len(clusters[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "num_images = 8\n",
    "plt.figure(figsize=(15, 15))\n",
    "position = 1\n",
    "for c in range(num_clusters):\n",
    "    cluster_instances = sorted(clusters[c], key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    for j in range(num_images):\n",
    "        image_idx = cluster_instances[j][0]\n",
    "        plt.subplot(num_clusters, num_images, position)\n",
    "        plt.imshow(x_data[image_idx].astype(\"uint8\"))\n",
    "        plt.title(classes[y_data[image_idx][0]])\n",
    "        plt.axis(\"off\")\n",
    "        position += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "cluster_label_counts = dict()\n",
    "\n",
    "for c in range(num_clusters):\n",
    "    cluster_label_counts[c] = [0] * num_classes\n",
    "    instances = clusters[c]\n",
    "    for i, _ in instances:\n",
    "        cluster_label_counts[c][y_data[i][0]] += 1\n",
    "\n",
    "    cluster_label_idx = np.argmax(cluster_label_counts[c])\n",
    "    correct_count = np.max(cluster_label_counts[c])\n",
    "    cluster_size = len(clusters[c])\n",
    "    accuracy = (\n",
    "        np.round((correct_count / cluster_size) * 100, 2) if cluster_size > 0 else 0\n",
    "    )\n",
    "    cluster_label = classes[cluster_label_idx]\n",
    "    print(\"cluster\", c, \"label is:\", cluster_label, \" -  accuracy:\", accuracy, \"%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "semantic_image_clustering",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
