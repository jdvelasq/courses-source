{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Memory-efficient embeddings for recommendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-1m.zip\", \"movielens.zip\")\n",
    "ZipFile(\"movielens.zip\", \"r\").extractall()\n",
    "\n",
    "ratings_data = pd.read_csv(\n",
    "    \"ml-1m/ratings.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],\n",
    ")\n",
    "\n",
    "ratings_data[\"movie_id\"] = ratings_data[\"movie_id\"].apply(lambda x: f\"movie_{x}\")\n",
    "ratings_data[\"user_id\"] = ratings_data[\"user_id\"].apply(lambda x: f\"user_{x}\")\n",
    "ratings_data[\"rating\"] = ratings_data[\"rating\"].apply(lambda x: float(x))\n",
    "del ratings_data[\"unix_timestamp\"]\n",
    "\n",
    "print(f\"Number of users: {len(ratings_data.user_id.unique())}\")\n",
    "print(f\"Number of movies: {len(ratings_data.movie_id.unique())}\")\n",
    "print(f\"Number of ratings: {len(ratings_data.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "random_selection = np.random.rand(len(ratings_data.index)) <= 0.85\n",
    "train_data = ratings_data[random_selection]\n",
    "eval_data = ratings_data[~random_selection]\n",
    "\n",
    "train_data.to_csv(\"train_data.csv\", index=False, sep=\"|\", header=False)\n",
    "eval_data.to_csv(\"eval_data.csv\", index=False, sep=\"|\", header=False)\n",
    "print(f\"Train data split: {len(train_data.index)}\")\n",
    "print(f\"Eval data split: {len(eval_data.index)}\")\n",
    "print(\"Train and eval data files are saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "csv_header = list(ratings_data.columns)\n",
    "user_vocabulary = list(ratings_data.user_id.unique())\n",
    "movie_vocabulary = list(ratings_data.movie_id.unique())\n",
    "target_feature_name = \"rating\"\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "num_epochs = 3\n",
    "base_embedding_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=True):\n",
    "    return tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=csv_header,\n",
    "        label_name=target_feature_name,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        field_delim=\"|\",\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    "    )\n",
    "    # Read the training data.\n",
    "    train_dataset = get_dataset_from_csv(\"train_data.csv\", batch_size)\n",
    "    # Read the test data.\n",
    "    eval_dataset = get_dataset_from_csv(\"eval_data.csv\", batch_size, shuffle=False)\n",
    "    # Fit the model with the training data.\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=eval_dataset,\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def embedding_encoder(vocabulary, embedding_dim, num_oov_indices=0, name=None):\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            StringLookup(\n",
    "                vocabulary=vocabulary, mask_token=None, num_oov_indices=num_oov_indices\n",
    "            ),\n",
    "            layers.Embedding(\n",
    "                input_dim=len(vocabulary) + num_oov_indices, output_dim=embedding_dim\n",
    "            ),\n",
    "        ],\n",
    "        name=f\"{name}_embedding\" if name else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def create_baseline_model():\n",
    "    # Receive the user as an input.\n",
    "    user_input = layers.Input(name=\"user_id\", shape=(), dtype=tf.string)\n",
    "    # Get user embedding.\n",
    "    user_embedding = embedding_encoder(\n",
    "        vocabulary=user_vocabulary, embedding_dim=base_embedding_dim, name=\"user\"\n",
    "    )(user_input)\n",
    "\n",
    "    # Receive the movie as an input.\n",
    "    movie_input = layers.Input(name=\"movie_id\", shape=(), dtype=tf.string)\n",
    "    # Get embedding.\n",
    "    movie_embedding = embedding_encoder(\n",
    "        vocabulary=movie_vocabulary, embedding_dim=base_embedding_dim, name=\"movie\"\n",
    "    )(movie_input)\n",
    "\n",
    "    # Compute dot product similarity between user and movie embeddings.\n",
    "    logits = layers.Dot(axes=1, name=\"dot_similarity\")(\n",
    "        [user_embedding, movie_embedding]\n",
    "    )\n",
    "    # Convert to rating scale.\n",
    "    prediction = keras.activations.sigmoid(logits) * 5\n",
    "    # Create the model.\n",
    "    model = keras.Model(\n",
    "        inputs=[user_input, movie_input], outputs=prediction, name=\"baseline_model\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "history = run_experiment(baseline_model)\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"eval\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class QREmbedding(keras.layers.Layer):\n",
    "    def __init__(self, vocabulary, embedding_dim, num_buckets, name=None):\n",
    "        super(QREmbedding, self).__init__(name=name)\n",
    "        self.num_buckets = num_buckets\n",
    "\n",
    "        self.index_lookup = StringLookup(\n",
    "            vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
    "        )\n",
    "        self.q_embeddings = layers.Embedding(\n",
    "            num_buckets,\n",
    "            embedding_dim,\n",
    "        )\n",
    "        self.r_embeddings = layers.Embedding(\n",
    "            num_buckets,\n",
    "            embedding_dim,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get the item index.\n",
    "        embedding_index = self.index_lookup(inputs)\n",
    "        # Get the quotient index.\n",
    "        quotient_index = tf.math.floordiv(embedding_index, self.num_buckets)\n",
    "        # Get the reminder index.\n",
    "        remainder_index = tf.math.floormod(embedding_index, self.num_buckets)\n",
    "        # Lookup the quotient_embedding using the quotient_index.\n",
    "        quotient_embedding = self.q_embeddings(quotient_index)\n",
    "        # Lookup the remainder_embedding using the remainder_index.\n",
    "        remainder_embedding = self.r_embeddings(remainder_index)\n",
    "        # Use multiplication as a combiner operation\n",
    "        return quotient_embedding * remainder_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class MDEmbedding(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, blocks_vocabulary, blocks_embedding_dims, base_embedding_dim, name=None\n",
    "    ):\n",
    "        super(MDEmbedding, self).__init__(name=name)\n",
    "        self.num_blocks = len(blocks_vocabulary)\n",
    "\n",
    "        # Create vocab to block lookup.\n",
    "        keys = []\n",
    "        values = []\n",
    "        for block_idx, block_vocab in enumerate(blocks_vocabulary):\n",
    "            keys.extend(block_vocab)\n",
    "            values.extend([block_idx] * len(block_vocab))\n",
    "        self.vocab_to_block = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(keys, values), default_value=-1\n",
    "        )\n",
    "\n",
    "        self.block_embedding_encoders = []\n",
    "        self.block_embedding_projectors = []\n",
    "\n",
    "        # Create block embedding encoders and projectors.\n",
    "        for idx in range(self.num_blocks):\n",
    "            vocabulary = blocks_vocabulary[idx]\n",
    "            embedding_dim = blocks_embedding_dims[idx]\n",
    "            block_embedding_encoder = embedding_encoder(\n",
    "                vocabulary, embedding_dim, num_oov_indices=1\n",
    "            )\n",
    "            self.block_embedding_encoders.append(block_embedding_encoder)\n",
    "            if embedding_dim == base_embedding_dim:\n",
    "                self.block_embedding_projectors.append(layers.Lambda(lambda x: x))\n",
    "            else:\n",
    "                self.block_embedding_projectors.append(\n",
    "                    layers.Dense(units=base_embedding_dim)\n",
    "                )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get block index for each input item.\n",
    "        block_indicies = self.vocab_to_block.lookup(inputs)\n",
    "        # Initialize output embeddings to zeros.\n",
    "        embeddings = tf.zeros(shape=(tf.shape(inputs)[0], base_embedding_dim))\n",
    "        # Generate embeddings from blocks.\n",
    "        for idx in range(self.num_blocks):\n",
    "            # Lookup embeddings from the current block.\n",
    "            block_embeddings = self.block_embedding_encoders[idx](inputs)\n",
    "            # Project embeddings to base_embedding_dim.\n",
    "            block_embeddings = self.block_embedding_projectors[idx](block_embeddings)\n",
    "            # Create a mask to filter out embeddings of items that do not belong to the current block.\n",
    "            mask = tf.expand_dims(tf.cast(block_indicies == idx, tf.dtypes.float32), 1)\n",
    "            # Set the embeddings for the items not belonging to the current block to zeros.\n",
    "            block_embeddings = block_embeddings * mask\n",
    "            # Add the block embeddings to the final embeddings.\n",
    "            embeddings += block_embeddings\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "movie_frequencies = ratings_data[\"movie_id\"].value_counts()\n",
    "movie_frequencies.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "sorted_movie_vocabulary = list(movie_frequencies.keys())\n",
    "\n",
    "movie_blocks_vocabulary = [\n",
    "    sorted_movie_vocabulary[:400],  # high popularity movies block\n",
    "    sorted_movie_vocabulary[400:1700],  # normal popularity movies block\n",
    "    sorted_movie_vocabulary[1700:],  # low popularity movies block\n",
    "]\n",
    "\n",
    "movie_blocks_embedding_dims = [64, 32, 16]\n",
    "\n",
    "user_embedding_num_buckets = len(user_vocabulary) // 50\n",
    "\n",
    "\n",
    "def create_memory_efficient_model():\n",
    "    # Take the user as an input.\n",
    "    user_input = layers.Input(name=\"user_id\", shape=(), dtype=tf.string)\n",
    "    # Get user embedding.\n",
    "    user_embedding = QREmbedding(\n",
    "        vocabulary=user_vocabulary,\n",
    "        embedding_dim=base_embedding_dim,\n",
    "        num_buckets=user_embedding_num_buckets,\n",
    "        name=\"user_embedding\",\n",
    "    )(user_input)\n",
    "\n",
    "    # Take the movie as an input.\n",
    "    movie_input = layers.Input(name=\"movie_id\", shape=(), dtype=tf.string)\n",
    "    # Get embedding.\n",
    "    movie_embedding = MDEmbedding(\n",
    "        blocks_vocabulary=movie_blocks_vocabulary,\n",
    "        blocks_embedding_dims=movie_blocks_embedding_dims,\n",
    "        base_embedding_dim=base_embedding_dim,\n",
    "        name=\"movie_embedding\",\n",
    "    )(movie_input)\n",
    "\n",
    "    # Compute dot product similarity between user and movie embeddings.\n",
    "    logits = layers.Dot(axes=1, name=\"dot_similarity\")(\n",
    "        [user_embedding, movie_embedding]\n",
    "    )\n",
    "    # Convert to rating scale.\n",
    "    prediction = keras.activations.sigmoid(logits) * 5\n",
    "    # Create the model.\n",
    "    model = keras.Model(\n",
    "        inputs=[user_input, movie_input], outputs=prediction, name=\"baseline_model\"\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "memory_efficient_model = create_memory_efficient_model()\n",
    "memory_efficient_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "history = run_experiment(memory_efficient_model)\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"eval\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "memory_efficient_embeddings",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
