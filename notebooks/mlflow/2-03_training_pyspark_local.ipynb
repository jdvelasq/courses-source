{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8fcfa0-86dc-4739-959d-fe5dde256310",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tracking con PySpark usando el sistema local de archivos\n",
    "===\n",
    "\n",
    "* Ultima modificación: Mayo 14, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32fe06c-1819-4818-9550-ba85a63b9191",
   "metadata": {},
   "source": [
    "Código base\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6659faa0-360e-4384-827d-74ed53c69c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "    df = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "    y = df[\"quality\"]\n",
    "    x = df.copy()\n",
    "    x.pop(\"quality\")\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def make_train_test_split(x, y):\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (x_train, x_test, y_train, y_test) = train_test_split(\n",
    "        x,\n",
    "        y,\n",
    "        test_size=0.25,\n",
    "        random_state=123456,\n",
    "    )\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def eval_metrics(df):\n",
    "\n",
    "    from pyspark.mllib.evaluation import RegressionMetrics as rmtrcs\n",
    "\n",
    "    metrics = rmtrcs(df.rdd.map(lambda x: (x.quality, x.prediction)))\n",
    "\n",
    "    mse = metrics.meanSquaredError\n",
    "    mae = metrics.meanAbsoluteError\n",
    "    r2 = metrics.r2\n",
    "\n",
    "    return mse, mae, r2\n",
    "\n",
    "\n",
    "def report(estimator, mse, mae, r2):\n",
    "\n",
    "    print(estimator, \":\", sep=\"\")\n",
    "    print(f\"  MSE: {mse}\")\n",
    "    print(f\"  MAE: {mae}\")\n",
    "    print(f\"  R2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc01e26-3bfa-47e5-9874-ec9fc40f9f47",
   "metadata": {},
   "source": [
    "MLflow Tracking\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b8e4ef-0638-4d86-a1c5-95bc424c4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_experiment(experiment_name, regParam, elasticNetParam, verbose=1):\n",
    "\n",
    "    import os\n",
    "\n",
    "    import pandas as pd\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.ml.regression import LinearRegression\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    import mlflow\n",
    "\n",
    "    x, y = load_data()\n",
    "    x_train, x_test, y_train, y_test = make_train_test_split(x, y)\n",
    "\n",
    "    pdf_train = pd.concat(\n",
    "        [x_train, pd.to_numeric(y_train, downcast=\"float\")], axis=\"columns\"\n",
    "    )\n",
    "    pdf_test = pd.concat(\n",
    "        [x_test, pd.to_numeric(y_test, downcast=\"float\")], axis=\"columns\"\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # Spark\n",
    "    #\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    df_train = spark.createDataFrame(pdf_train)\n",
    "    df_test = spark.createDataFrame(pdf_test)\n",
    "\n",
    "    vectorAssembler = VectorAssembler(\n",
    "        inputCols=list(pdf_train.columns[:-1]),\n",
    "        outputCol=\"features\",\n",
    "    )\n",
    "    df_train = vectorAssembler.transform(df_train)\n",
    "    df_test = vectorAssembler.transform(df_test)\n",
    "\n",
    "    lr = LinearRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"quality\",\n",
    "        predictionCol=\"prediction\",\n",
    "        maxIter=1000,\n",
    "        regParam=regParam,\n",
    "        elasticNetParam=elasticNetParam,\n",
    "        fitIntercept=True,\n",
    "        standardization=True,\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(\"mlruns\"):\n",
    "        os.makedirs(\"mlruns\")\n",
    "    mlflow.set_tracking_uri(\"file:///workspace/mlflow/corridas\")\n",
    "    print(\"Tracking directory:\", mlflow.get_tracking_uri())\n",
    "\n",
    "    #\n",
    "    # Establece el directorio de tracking. Esta es la dirección absoluta al\n",
    "    # directorio actual en este ejemplo.\n",
    "    #\n",
    "    mlflow.pyspark.ml.autolog()\n",
    "\n",
    "    #\n",
    "    # Almancena las corridas  en el experimento indicado\n",
    "    #\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "\n",
    "        run = mlflow.active_run()\n",
    "        print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "        model = lr.fit(df_train)\n",
    "\n",
    "        df_test = model.transform(df_test)\n",
    "\n",
    "        mse, mae, r2 = eval_metrics(df_test)\n",
    "        if verbose > 0:\n",
    "            report(model, mse, mae, r2)\n",
    "\n",
    "        mlflow.log_metric(\"mse\", mse)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f62be02-483f-4d1a-b693-a94e9979e40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking directory: file:///workspace/mlflow/corridas\n",
      "Active run_id: 2e5f343a65104d28866207a10a55334a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionModel: uid=LinearRegression_7c8a85716278, numFeatures=11:\n",
      "  MSE: 0.45526360186470766\n",
      "  MAE: 0.5292753571424285\n",
      "  R2: -1.114457075123866\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se realizar el primer tanteo\n",
    "#\n",
    "make_experiment(\n",
    "    experiment_name=\"red-wine\",\n",
    "    regParam=0.00001,\n",
    "    elasticNetParam=0.00001,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade21620-150c-45f5-a524-a6c8ef6fd722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking directory: file:///workspace/mlflow/corridas\n",
      "Active run_id: 5b49f614696847119bcfac1e8390121a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionModel: uid=LinearRegression_c74fc1588d38, numFeatures=11:\n",
      "  MSE: 0.4552568409331781\n",
      "  MAE: 0.5292824166305712\n",
      "  R2: -1.116331285203061\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se realizar el segundo tanteo\n",
    "#\n",
    "make_experiment(\n",
    "    experiment_name=\"red-wine\",\n",
    "    regParam=0.0005,\n",
    "    elasticNetParam=0.0001,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232297e-8232-4b91-af6e-22a8d05d92b4",
   "metadata": {},
   "source": [
    "MLflow ui\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229102d0-3581-47f3-a1b7-76328450ff07",
   "metadata": {},
   "source": [
    "Para visualizar la interfase use:\n",
    "\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "**Nota:** En docker usar:\n",
    "\n",
    "```bash\n",
    "mlflow ui --host 0.0.0.0 \n",
    "``` \n",
    "\n",
    "con: \n",
    "\n",
    "http://127.0.0.1:5001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23698113-ff0c-473d-a99b-9cba037379fe",
   "metadata": {},
   "source": [
    "![assets/mlflow-tracking-3-pyspark-part-0.png](assets/mlflow-tracking-3-pyspark-part-0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0e9f2-ea39-4296-a4a0-46f040b825f9",
   "metadata": {},
   "source": [
    "**Detalles de la corrida**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3d550-1b36-4c90-91b8-c68178f0ea2a",
   "metadata": {},
   "source": [
    "![assets/mlflow-tracking-3-pyspark-part-1.png](assets/mlflow-tracking-3-pyspark-part-1.png)\n",
    "![assets/mlflow-tracking-3-pyspark-part-2.png](assets/mlflow-tracking-3-pyspark-part-2.png)\n",
    "![assets/mlflow-tracking-3-pyspark-part-3.png](assets/mlflow-tracking-3-pyspark-part-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928de67d-5d48-484c-abad-6278224f488b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Chequeo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f524f13f-de2d-41f2-8030-c92fca1bdaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_estimator():\n",
    "#     \n",
    "#     def local_eval_metrics(y_true, y_pred):\n",
    "#         \n",
    "#         from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# \n",
    "#         mse = mean_squared_error(y_true, y_pred)\n",
    "#         mae = mean_absolute_error(y_true, y_pred)\n",
    "#         r2 = r2_score(y_true, y_pred)\n",
    "# \n",
    "#         return mse, mae, r2\n",
    "# \n",
    "#     import pandas as pd\n",
    "#     from pyspark.ml.feature import VectorAssembler\n",
    "#     from pyspark.sql import SparkSession\n",
    "# \n",
    "#     import mlflow\n",
    "# \n",
    "#     x, y = load_data()\n",
    "#     x_train, x_test, y_train, y_test = make_train_test_split(x, y)\n",
    "# \n",
    "#     # -----------------------------------------------------------------------------------\n",
    "#     # Debe prepararse la data. Este es un buen ejemplo para mostrar las dificultades\n",
    "#     # asociadas a no tener un feature store.\n",
    "#     #\n",
    "#     pdf_test = pd.concat(\n",
    "#         [x_test, pd.to_numeric(y_test, downcast=\"float\")], axis=\"columns\"\n",
    "#     )\n",
    "# \n",
    "#     spark = SparkSession.builder.getOrCreate()\n",
    "#     df_test = spark.createDataFrame(pdf_test)\n",
    "# \n",
    "#     vectorAssembler = VectorAssembler(\n",
    "#         inputCols=list(pdf_test.columns[:-1]),\n",
    "#         outputCol=\"features\",\n",
    "#     )\n",
    "#     df_test = vectorAssembler.transform(df_test)\n",
    "#     x_test = df_test.toPandas()\n",
    "#     spark.stop()\n",
    "#     # -----------------------------------------------------------------------------------\n",
    "# \n",
    "#     # NOTA: este parámetro es copiado directamente de la interfase de MLflow\n",
    "#     estimator_path = \"runs:/02298313752742d6ae411626d5aff18e/model\"\n",
    "#     estimator = mlflow.pyfunc.load_model(estimator_path)\n",
    "#     \n",
    "#     y_pred = estimator.predict(pd.DataFrame(x_test))\n",
    "#     \n",
    "#     mse, mae, r2 = local_eval_metrics(y_test, y_pred)\n",
    "#     report(estimator, mse, mae, r2)\n",
    "# \n",
    "# \n",
    "# #\n",
    "# # Debe coincidir con el mejor modelo encontrado en la celdas anteriores\n",
    "# #\n",
    "# check_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f93621-61f9-4821-aa51-eee958d2febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# No se borran las corridas para comparar resultados con otras librerías\n",
    "# -----------------------------------------------------------------------------\n",
    "# %%bash\n",
    "# rm -rf outputs mlruns models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
