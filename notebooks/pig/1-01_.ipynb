{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de palabras en Apache Pig\n",
    "===\n",
    "\n",
    "* Última modificación: Noviembre 05, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Archivos de prueba\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema. Puede usar directamente comandos del sistema operativo en el Terminal y el editor de texto `pico` para crear los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Se crea el directorio de entrada\n",
    "#\n",
    "!rm -rf input output\n",
    "!mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación del contendor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el Terminal ejecute el siguiente comando:\n",
    "\n",
    "```bash\n",
    "$ docker run --rm -it -v \"$PWD\":/workspace  --name pig -p 50070:50070 -p 8088:8088 -p 8888:8888  jdvelasq/pig:0.17.0\n",
    "```\n",
    "\n",
    "Como resultado debe ver el siguiente mensaje:\n",
    "\n",
    "```\n",
    "======================================\n",
    "\n",
    "Hadoop NameNode at:\n",
    "\n",
    "    http://127.0.0.1:50070/\n",
    "\n",
    "Yarn ResourceManager at:\n",
    "\n",
    "     http://127.0.0.1:8088/\n",
    "\n",
    "======================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificación de que Hadoop está ejecutándose correctamente\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que Hadoop se está ejecutando correctamente abra las siguiente direcciones en un explorador de internet:\n",
    "\n",
    "* http://127.0.0.1:50070/\n",
    "\n",
    "\n",
    "* http://127.0.0.1:8088/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código en Apache Pig\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota.** Se usan los dos guiones `--` para comentario de una línea y `/*` ... `*/` para comentarios de varias líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.pig\n",
    "\n",
    "-- crea la carpeta input in el HDFS\n",
    "fs -mkdir input\n",
    "\n",
    "-- copia los archivos del sistema local al HDFS\n",
    "fs -put input/ .\n",
    "\n",
    "-- carga de datos\n",
    "lines = LOAD 'input/text*.txt' AS (line:CHARARRAY);\n",
    "\n",
    "-- genera una tabla llamada words con una palabra por registro\n",
    "words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\n",
    "\n",
    "-- agrupa los registros que tienen la misma palabra\n",
    "grouped = GROUP words BY word;\n",
    "\n",
    "-- genera una variable que cuenta las ocurrencias por cada grupo\n",
    "wordcount = FOREACH grouped GENERATE group, COUNT(words);\n",
    "\n",
    "-- selecciona las primeras 15 palabras\n",
    "s = LIMIT wordcount 15;\n",
    "\n",
    "-- escribe el archivo de salida \n",
    "STORE s INTO 'output';\n",
    "\n",
    "-- copia los archivos del HDFS al sistema local\n",
    "fs -get output/ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-12 19:49:21,209 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2022-05-12 19:49:23,182 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2022-05-12 19:49:23,394 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:49:23,494 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2022-05-12 19:49:23,496 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2022-05-12 19:49:23,503 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2022-05-12 19:49:23,508 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.submit.replication is deprecated. Instead, use mapreduce.client.submit.file.replication\n",
      "2022-05-12 19:49:25,297 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2022-05-12 19:49:25,301 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:49:25,309 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2022-05-12 19:49:25,372 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2022-05-12 19:49:25,394 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2022-05-12 19:49:25,865 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2022-05-12 19:49:26,348 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1652384900661_0001\n",
      "2022-05-12 19:49:26,487 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2022-05-12 19:49:26,687 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1652384900661_0001\n",
      "2022-05-12 19:49:26,713 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://650556b976c1:8088/proxy/application_1652384900661_0001/\n",
      "2022-05-12 19:49:46,875 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:49:46,883 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:49:47,411 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:49:47,416 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:49:47,442 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:49:47,445 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:49:48,822 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:49:48,839 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2022-05-12 19:49:48,855 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2022-05-12 19:49:49,287 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2022-05-12 19:49:49,322 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1652384900661_0002\n",
      "2022-05-12 19:49:49,325 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2022-05-12 19:49:49,352 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1652384900661_0002\n",
      "2022-05-12 19:49:49,356 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://650556b976c1:8088/proxy/application_1652384900661_0002/\n",
      "2022-05-12 19:50:04,466 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:04,470 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:04,579 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:04,583 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:04,607 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:04,610 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:04,647 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:04,650 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:04,675 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:04,678 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:04,705 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:04,709 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:04,738 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:04,741 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:04,768 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:04,771 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:04,794 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:04,797 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n"
     ]
    }
   ],
   "source": [
    "!pig -execute 'run script.pig'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de los resultados en el HDFS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup          0 2022-05-12 19:50 output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         81 2022-05-12 19:50 output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\n",
      "DA\t1\n",
      "be\t1\n",
      "by\t2\n",
      "in\t5\n",
      "is\t3\n",
      "of\t8\n",
      "on\t1\n",
      "or\t5\n",
      "to\t12\n",
      "Big\t1\n",
      "The\t2\n",
      "aid\t1\n",
      "and\t15\n",
      "are\t1\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movimiento de los resultados al sistema local\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/_SUCCESS  output/part-r-00000\n",
      "\n",
      "output/output:\n",
      "_SUCCESS  part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal output output\n",
    "!ls output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualilzación de resultados en el sistema local\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/_SUCCESS\n",
      "output/part-r-00000\n",
      "\n",
      "output/output:\n",
      "_SUCCESS\n",
      "part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!ls -1 output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\n",
      "DA\t1\n",
      "be\t1\n",
      "by\t2\n",
      "in\t5\n",
      "is\t3\n",
      "of\t8\n",
      "on\t1\n",
      "or\t5\n",
      "to\t12\n",
      "Big\t1\n",
      "The\t2\n",
      "aid\t1\n",
      "and\t15\n",
      "are\t1\n"
     ]
    }
   ],
   "source": [
    "!cat output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notas\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejecución de programas en Grunt**\n",
    "\n",
    "Se realiza con los comandos `exec` y `run`. \n",
    "\n",
    "    grunt> exec script\n",
    "    \n",
    "    grunt> run script\n",
    "    \n",
    "La diferencia entre estos comandos es que `exec` ejecuta el script sin importalo a `grunt`  mientras que `run` si lo hace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejecución de Pig en la consola de comandos**\n",
    "\n",
    "Pig es comúnmente usado en la línea de comandos para ejecutar tareas de ETL, análisis de datos y procesamiento interactivo. Para ejecutar Pig en la línea de comandos digite:\n",
    "\n",
    "    pig -x local\n",
    "    \n",
    "Si no usa la opción `x -local`, Pig usará el sistema de archivos HDFS.\n",
    "\n",
    "\n",
    "La opción `-e` o `-execute` permite ejecutar un comandos simple sin entrar a Grunt:\n",
    "\n",
    "    pig -e comando\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejecución de comandos del sistema operativo desde Pig**\n",
    "\n",
    "`Pig` permite la ejecución de comandos del sistema operativo; por ejemplo:\n",
    "\n",
    "     grunt> ls\n",
    "     \n",
    "También es posible usar comandos del sistema HDFS; el comando `hadoop dfs -ls /` se escribiría en `hive` como\n",
    "\n",
    "     grunt> fs -ls / ;\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supresion de información detallada\n",
    "\n",
    "Apache Pig imprime mucha información en pantalla relacionada con su ejecución y la Hadoop. Para regular el nivel de información entregada, se puede realizar una copia al directorio actual del archivo `./conf/log4j.properties` ubicado en la carpeta de instalación de Pig. \n",
    "    \n",
    "El archivo `log4j.properties` se modifica para que se impriman únicamente los mensajes de error de Pig y Hadoop. Para ello, se modifica la línea correspondiente para que quede así:\n",
    "\n",
    "    log4j.logger.org.apache.pig=error, A\n",
    "    \n",
    "y se agrega la siguiente para modificar el nivel de información entregado por Hadoop:\n",
    "\n",
    "    log4j.logger.org.apache.hadoop=error, A\n",
    "    \n",
    "Se invoca Pig con:\n",
    "\n",
    "    pig -4 log4j.properties\n",
    "    \n",
    "para indicar que se usa el archivo ubicado en la carpeta actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución de Pig en Jupyter\n",
    "\n",
    "A continuación se describe como ejecutar comandos de Pig en Jupyter usando la extensión de Jupyter `bigdata`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordCount en Apache Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los archivos en Apache Pig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lines = LOAD 'input/text*.txt' AS (line:CHARARRAY);\n",
      " DUMP lines;\n",
      "2022-05-12 19:50:14,604 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:14,720 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2022-05-12 19:50:14,722 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2022-05-12 19:50:14,728 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.submit.replication is deprecated. Instead, use mapreduce.client.submit.file.replication\n",
      "2022-05-12 19:50:14,891 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2022-05-12 19:50:14,894 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:14,902 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2022-05-12 19:50:14,951 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2022-05-12 19:50:14,971 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2022-05-12 19:50:15,825 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2022-05-12 19:50:16,178 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1652384900661_0003\n",
      "2022-05-12 19:50:16,445 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2022-05-12 19:50:16,502 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1652384900661_0003\n",
      "2022-05-12 19:50:16,523 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://650556b976c1:8088/proxy/application_1652384900661_0003/\n",
      "2022-05-12 19:50:26,632 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:26,639 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:26,780 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:26,784 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:26,802 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2022-05-12 19:50:26,803 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:26,806 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:26,843 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:26,846 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:26,868 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:26,871 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:26,888 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:26,891 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:26,925 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(Analytics is the discovery, interpretation, and communication of meaningful patterns )\n",
      "(in data. Especially valuable in areas rich with recorded information, analytics relies )\n",
      "(on the simultaneous application of statistics, computer programming and operations research )\n",
      "(to quantify performance.)\n",
      "()\n",
      "(Organizations may apply analytics to business data to describe, predict, and improve business )\n",
      "(performance. Specifically, areas within analytics include predictive analytics, prescriptive )\n",
      "(analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big )\n",
      "(Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, )\n",
      "(marketing optimization and marketing mix modeling, web analytics, call analytics, speech )\n",
      "(analytics, sales force sizing and optimization, price and promotion modeling, predictive )\n",
      "(science, credit risk analysis, and fraud analytics. Since analytics can require extensive )\n",
      "(computation (see big data), the algorithms and software used for analytics harness the most )\n",
      "(current methods in computer science, statistics, and mathematics.)\n",
      "(Data analytics (DA) is the process of examining data sets in order to draw conclusions )\n",
      "(about the information they contain, increasingly with the aid of specialized systems )\n",
      "(and software. Data analytics technologies and techniques are widely used in commercial )\n",
      "(industries to enable organizations to make more-informed business decisions and by )\n",
      "(scientists and researchers to verify or disprove scientific models, theories and )\n",
      "(hypotheses.)\n",
      "(The field of data analysis. Analytics often involves studying past historical data to )\n",
      "(research potential trends, to analyze the effects of certain decisions or events, or to )\n",
      "(evaluate the performance of a given tool or scenario. The goal of analytics is to improve )\n",
      "(the business by gaining knowledge which can be used to make improvements or changes.)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "lines = LOAD 'input/text*.txt' AS (line:CHARARRAY);\n",
    "DUMP lines;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realiza el conteo de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- genera una tabla llamada words con una palabra por registro\n",
      " words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\n",
      " DUMP words;\n",
      "2022-05-12 19:50:27,234 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:27,769 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:27,780 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2022-05-12 19:50:27,789 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2022-05-12 19:50:27,814 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2022-05-12 19:50:27,843 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1652384900661_0004\n",
      "2022-05-12 19:50:27,845 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2022-05-12 19:50:27,874 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1652384900661_0004\n",
      "2022-05-12 19:50:27,878 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://650556b976c1:8088/proxy/application_1652384900661_0004/\n",
      "2022-05-12 19:50:42,888 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:42,892 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:42,952 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:42,957 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:42,978 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:42,982 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:43,005 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:43,008 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:43,032 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:43,035 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:43,056 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:43,060 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:50:43,087 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(Analytics)\n",
      "(is)\n",
      "(the)\n",
      "(discovery)\n",
      "(interpretation)\n",
      "(and)\n",
      "(communication)\n",
      "(of)\n",
      "(meaningful)\n",
      "(patterns)\n",
      "(in)\n",
      "(data.)\n",
      "(Especially)\n",
      "(valuable)\n",
      "(in)\n",
      "(areas)\n",
      "(rich)\n",
      "(with)\n",
      "(recorded)\n",
      "(information)\n",
      "(analytics)\n",
      "(relies)\n",
      "(on)\n",
      "(the)\n",
      "(simultaneous)\n",
      "(application)\n",
      "(of)\n",
      "(statistics)\n",
      "(computer)\n",
      "(programming)\n",
      "(and)\n",
      "(operations)\n",
      "(research)\n",
      "(to)\n",
      "(quantify)\n",
      "(performance.)\n",
      "()\n",
      "(Organizations)\n",
      "(may)\n",
      "(apply)\n",
      "(analytics)\n",
      "(to)\n",
      "(business)\n",
      "(data)\n",
      "(to)\n",
      "(describe)\n",
      "(predict)\n",
      "(and)\n",
      "(improve)\n",
      "(business)\n",
      "(performance.)\n",
      "(Specifically)\n",
      "(areas)\n",
      "(within)\n",
      "(analytics)\n",
      "(include)\n",
      "(predictive)\n",
      "(analytics)\n",
      "(prescriptive)\n",
      "(analytics)\n",
      "(enterprise)\n",
      "(decision)\n",
      "(management)\n",
      "(descriptive)\n",
      "(analytics)\n",
      "(cognitive)\n",
      "(analytics)\n",
      "(Big)\n",
      "(Data)\n",
      "(Analytics)\n",
      "(retail)\n",
      "(analytics)\n",
      "(store)\n",
      "(assortment)\n",
      "(and)\n",
      "(stock-keeping)\n",
      "(unit)\n",
      "(optimization)\n",
      "(marketing)\n",
      "(optimization)\n",
      "(and)\n",
      "(marketing)\n",
      "(mix)\n",
      "(modeling)\n",
      "(web)\n",
      "(analytics)\n",
      "(call)\n",
      "(analytics)\n",
      "(speech)\n",
      "(analytics)\n",
      "(sales)\n",
      "(force)\n",
      "(sizing)\n",
      "(and)\n",
      "(optimization)\n",
      "(price)\n",
      "(and)\n",
      "(promotion)\n",
      "(modeling)\n",
      "(predictive)\n",
      "(science)\n",
      "(credit)\n",
      "(risk)\n",
      "(analysis)\n",
      "(and)\n",
      "(fraud)\n",
      "(analytics.)\n",
      "(Since)\n",
      "(analytics)\n",
      "(can)\n",
      "(require)\n",
      "(extensive)\n",
      "(computation)\n",
      "(see)\n",
      "(big)\n",
      "(data)\n",
      "(the)\n",
      "(algorithms)\n",
      "(and)\n",
      "(software)\n",
      "(used)\n",
      "(for)\n",
      "(analytics)\n",
      "(harness)\n",
      "(the)\n",
      "(most)\n",
      "(current)\n",
      "(methods)\n",
      "(in)\n",
      "(computer)\n",
      "(science)\n",
      "(statistics)\n",
      "(and)\n",
      "(mathematics.)\n",
      "(Data)\n",
      "(analytics)\n",
      "(DA)\n",
      "(is)\n",
      "(the)\n",
      "(process)\n",
      "(of)\n",
      "(examining)\n",
      "(data)\n",
      "(sets)\n",
      "(in)\n",
      "(order)\n",
      "(to)\n",
      "(draw)\n",
      "(conclusions)\n",
      "(about)\n",
      "(the)\n",
      "(information)\n",
      "(they)\n",
      "(contain)\n",
      "(increasingly)\n",
      "(with)\n",
      "(the)\n",
      "(aid)\n",
      "(of)\n",
      "(specialized)\n",
      "(systems)\n",
      "(and)\n",
      "(software.)\n",
      "(Data)\n",
      "(analytics)\n",
      "(technologies)\n",
      "(and)\n",
      "(techniques)\n",
      "(are)\n",
      "(widely)\n",
      "(used)\n",
      "(in)\n",
      "(commercial)\n",
      "(industries)\n",
      "(to)\n",
      "(enable)\n",
      "(organizations)\n",
      "(to)\n",
      "(make)\n",
      "(more-informed)\n",
      "(business)\n",
      "(decisions)\n",
      "(and)\n",
      "(by)\n",
      "(scientists)\n",
      "(and)\n",
      "(researchers)\n",
      "(to)\n",
      "(verify)\n",
      "(or)\n",
      "(disprove)\n",
      "(scientific)\n",
      "(models)\n",
      "(theories)\n",
      "(and)\n",
      "(hypotheses.)\n",
      "(The)\n",
      "(field)\n",
      "(of)\n",
      "(data)\n",
      "(analysis.)\n",
      "(Analytics)\n",
      "(often)\n",
      "(involves)\n",
      "(studying)\n",
      "(past)\n",
      "(historical)\n",
      "(data)\n",
      "(to)\n",
      "(research)\n",
      "(potential)\n",
      "(trends)\n",
      "(to)\n",
      "(analyze)\n",
      "(the)\n",
      "(effects)\n",
      "(of)\n",
      "(certain)\n",
      "(decisions)\n",
      "(or)\n",
      "(events)\n",
      "(or)\n",
      "(to)\n",
      "(evaluate)\n",
      "(the)\n",
      "(performance)\n",
      "(of)\n",
      "(a)\n",
      "(given)\n",
      "(tool)\n",
      "(or)\n",
      "(scenario.)\n",
      "(The)\n",
      "(goal)\n",
      "(of)\n",
      "(analytics)\n",
      "(is)\n",
      "(to)\n",
      "(improve)\n",
      "(the)\n",
      "(business)\n",
      "(by)\n",
      "(gaining)\n",
      "(knowledge)\n",
      "(which)\n",
      "(can)\n",
      "(be)\n",
      "(used)\n",
      "(to)\n",
      "(make)\n",
      "(improvements)\n",
      "(or)\n",
      "(changes.)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "-- genera una tabla llamada words con una palabra por registro\n",
    "words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\n",
    "DUMP words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- agrupa los registros que tienen la misma palabra\n",
      " grouped = GROUP words BY word;\n",
      " DUMP grouped;\n",
      "2022-05-12 19:50:43,357 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:44,687 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:50:44,697 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2022-05-12 19:50:44,704 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2022-05-12 19:50:45,131 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2022-05-12 19:50:45,552 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1652384900661_0005\n",
      "2022-05-12 19:50:45,554 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2022-05-12 19:50:45,574 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1652384900661_0005\n",
      "2022-05-12 19:50:45,577 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://650556b976c1:8088/proxy/application_1652384900661_0005/\n",
      "2022-05-12 19:51:00,711 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:00,724 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:00,778 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:00,781 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:00,803 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:00,805 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:00,825 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:00,828 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:00,846 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:00,849 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:00,870 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:00,872 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:00,897 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(a,{(a)})\n",
      "(DA,{(DA)})\n",
      "(be,{(be)})\n",
      "(by,{(by),(by)})\n",
      "(in,{(in),(in),(in),(in),(in)})\n",
      "(is,{(is),(is),(is)})\n",
      "(of,{(of),(of),(of),(of),(of),(of),(of),(of)})\n",
      "(on,{(on)})\n",
      "(or,{(or),(or),(or),(or),(or)})\n",
      "(to,{(to),(to),(to),(to),(to),(to),(to),(to),(to),(to),(to),(to)})\n",
      "(Big,{(Big)})\n",
      "(The,{(The),(The)})\n",
      "(aid,{(aid)})\n",
      "(and,{(and),(and),(and),(and),(and),(and),(and),(and),(and),(and),(and),(and),(and),(and),(and)})\n",
      "(are,{(are)})\n",
      "(big,{(big)})\n",
      "(can,{(can),(can)})\n",
      "(for,{(for)})\n",
      "(may,{(may)})\n",
      "(mix,{(mix)})\n",
      "(see,{(see)})\n",
      "(the,{(the),(the),(the),(the),(the),(the),(the),(the),(the),(the)})\n",
      "(web,{(web)})\n",
      "(Data,{(Data),(Data),(Data)})\n",
      "(call,{(call)})\n",
      "(data,{(data),(data),(data),(data),(data)})\n",
      "(draw,{(draw)})\n",
      "(goal,{(goal)})\n",
      "(make,{(make),(make)})\n",
      "(most,{(most)})\n",
      "(past,{(past)})\n",
      "(rich,{(rich)})\n",
      "(risk,{(risk)})\n",
      "(sets,{(sets)})\n",
      "(they,{(they)})\n",
      "(tool,{(tool)})\n",
      "(unit,{(unit)})\n",
      "(used,{(used),(used),(used)})\n",
      "(with,{(with),(with)})\n",
      "(Since,{(Since)})\n",
      "(about,{(about)})\n",
      "(apply,{(apply)})\n",
      "(areas,{(areas),(areas)})\n",
      "(data.,{(data.)})\n",
      "(field,{(field)})\n",
      "(force,{(force)})\n",
      "(fraud,{(fraud)})\n",
      "(given,{(given)})\n",
      "(often,{(often)})\n",
      "(order,{(order)})\n",
      "(price,{(price)})\n",
      "(sales,{(sales)})\n",
      "(store,{(store)})\n",
      "(which,{(which)})\n",
      "(credit,{(credit)})\n",
      "(enable,{(enable)})\n",
      "(events,{(events)})\n",
      "(models,{(models)})\n",
      "(relies,{(relies)})\n",
      "(retail,{(retail)})\n",
      "(sizing,{(sizing)})\n",
      "(speech,{(speech)})\n",
      "(trends,{(trends)})\n",
      "(verify,{(verify)})\n",
      "(widely,{(widely)})\n",
      "(within,{(within)})\n",
      "(analyze,{(analyze)})\n",
      "(certain,{(certain)})\n",
      "(contain,{(contain)})\n",
      "(current,{(current)})\n",
      "(effects,{(effects)})\n",
      "(gaining,{(gaining)})\n",
      "(harness,{(harness)})\n",
      "(improve,{(improve),(improve)})\n",
      "(include,{(include)})\n",
      "(methods,{(methods)})\n",
      "(predict,{(predict)})\n",
      "(process,{(process)})\n",
      "(require,{(require)})\n",
      "(science,{(science),(science)})\n",
      "(systems,{(systems)})\n",
      "(analysis,{(analysis)})\n",
      "(business,{(business),(business),(business),(business)})\n",
      "(changes.,{(changes.)})\n",
      "(computer,{(computer),(computer)})\n",
      "(decision,{(decision)})\n",
      "(describe,{(describe)})\n",
      "(disprove,{(disprove)})\n",
      "(evaluate,{(evaluate)})\n",
      "(involves,{(involves)})\n",
      "(modeling,{(modeling),(modeling)})\n",
      "(patterns,{(patterns)})\n",
      "(quantify,{(quantify)})\n",
      "(recorded,{(recorded)})\n",
      "(research,{(research),(research)})\n",
      "(software,{(software)})\n",
      "(studying,{(studying)})\n",
      "(theories,{(theories)})\n",
      "(valuable,{(valuable)})\n",
      "(Analytics,{(Analytics),(Analytics),(Analytics)})\n",
      "(analysis.,{(analysis.)})\n",
      "(analytics,{(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics)})\n",
      "(cognitive,{(cognitive)})\n",
      "(decisions,{(decisions),(decisions)})\n",
      "(discovery,{(discovery)})\n",
      "(examining,{(examining)})\n",
      "(extensive,{(extensive)})\n",
      "(knowledge,{(knowledge)})\n",
      "(marketing,{(marketing),(marketing)})\n",
      "(potential,{(potential)})\n",
      "(promotion,{(promotion)})\n",
      "(scenario.,{(scenario.)})\n",
      "(software.,{(software.)})\n",
      "(Especially,{(Especially)})\n",
      "(algorithms,{(algorithms)})\n",
      "(analytics.,{(analytics.)})\n",
      "(assortment,{(assortment)})\n",
      "(commercial,{(commercial)})\n",
      "(enterprise,{(enterprise)})\n",
      "(historical,{(historical)})\n",
      "(industries,{(industries)})\n",
      "(management,{(management)})\n",
      "(meaningful,{(meaningful)})\n",
      "(operations,{(operations)})\n",
      "(predictive,{(predictive),(predictive)})\n",
      "(scientific,{(scientific)})\n",
      "(scientists,{(scientists)})\n",
      "(statistics,{(statistics),(statistics)})\n",
      "(techniques,{(techniques)})\n",
      "(application,{(application)})\n",
      "(computation,{(computation)})\n",
      "(conclusions,{(conclusions)})\n",
      "(descriptive,{(descriptive)})\n",
      "(hypotheses.,{(hypotheses.)})\n",
      "(information,{(information),(information)})\n",
      "(performance,{(performance)})\n",
      "(programming,{(programming)})\n",
      "(researchers,{(researchers)})\n",
      "(specialized,{(specialized)})\n",
      "(Specifically,{(Specifically)})\n",
      "(improvements,{(improvements)})\n",
      "(increasingly,{(increasingly)})\n",
      "(mathematics.,{(mathematics.)})\n",
      "(optimization,{(optimization),(optimization),(optimization)})\n",
      "(performance.,{(performance.),(performance.)})\n",
      "(prescriptive,{(prescriptive)})\n",
      "(simultaneous,{(simultaneous)})\n",
      "(technologies,{(technologies)})\n",
      "(Organizations,{(Organizations)})\n",
      "(communication,{(communication)})\n",
      "(more-informed,{(more-informed)})\n",
      "(organizations,{(organizations)})\n",
      "(stock-keeping,{(stock-keeping)})\n",
      "(interpretation,{(interpretation)})\n",
      "(,{()})\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "-- agrupa los registros que tienen la misma palabra\n",
    "grouped = GROUP words BY word;\n",
    "DUMP grouped;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- genera una variable que cuenta las ocurrencias por cada grupo\n",
      " wordcount = FOREACH grouped GENERATE group, COUNT(words);\n",
      " DUMP wordcount;\n",
      "2022-05-12 19:51:01,164 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:02,456 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:02,468 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2022-05-12 19:51:02,476 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2022-05-12 19:51:02,902 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2022-05-12 19:51:03,296 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1652384900661_0006\n",
      "2022-05-12 19:51:03,301 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2022-05-12 19:51:03,322 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1652384900661_0006\n",
      "2022-05-12 19:51:03,325 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://650556b976c1:8088/proxy/application_1652384900661_0006/\n",
      "2022-05-12 19:51:23,428 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:23,434 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:23,487 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:23,490 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:23,511 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:23,514 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:23,533 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:23,536 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:23,554 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:23,556 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:23,577 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:23,580 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:23,604 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(a,1)\n",
      "(DA,1)\n",
      "(be,1)\n",
      "(by,2)\n",
      "(in,5)\n",
      "(is,3)\n",
      "(of,8)\n",
      "(on,1)\n",
      "(or,5)\n",
      "(to,12)\n",
      "(Big,1)\n",
      "(The,2)\n",
      "(aid,1)\n",
      "(and,15)\n",
      "(are,1)\n",
      "(big,1)\n",
      "(can,2)\n",
      "(for,1)\n",
      "(may,1)\n",
      "(mix,1)\n",
      "(see,1)\n",
      "(the,10)\n",
      "(web,1)\n",
      "(Data,3)\n",
      "(call,1)\n",
      "(data,5)\n",
      "(draw,1)\n",
      "(goal,1)\n",
      "(make,2)\n",
      "(most,1)\n",
      "(past,1)\n",
      "(rich,1)\n",
      "(risk,1)\n",
      "(sets,1)\n",
      "(they,1)\n",
      "(tool,1)\n",
      "(unit,1)\n",
      "(used,3)\n",
      "(with,2)\n",
      "(Since,1)\n",
      "(about,1)\n",
      "(apply,1)\n",
      "(areas,2)\n",
      "(data.,1)\n",
      "(field,1)\n",
      "(force,1)\n",
      "(fraud,1)\n",
      "(given,1)\n",
      "(often,1)\n",
      "(order,1)\n",
      "(price,1)\n",
      "(sales,1)\n",
      "(store,1)\n",
      "(which,1)\n",
      "(credit,1)\n",
      "(enable,1)\n",
      "(events,1)\n",
      "(models,1)\n",
      "(relies,1)\n",
      "(retail,1)\n",
      "(sizing,1)\n",
      "(speech,1)\n",
      "(trends,1)\n",
      "(verify,1)\n",
      "(widely,1)\n",
      "(within,1)\n",
      "(analyze,1)\n",
      "(certain,1)\n",
      "(contain,1)\n",
      "(current,1)\n",
      "(effects,1)\n",
      "(gaining,1)\n",
      "(harness,1)\n",
      "(improve,2)\n",
      "(include,1)\n",
      "(methods,1)\n",
      "(predict,1)\n",
      "(process,1)\n",
      "(require,1)\n",
      "(science,2)\n",
      "(systems,1)\n",
      "(analysis,1)\n",
      "(business,4)\n",
      "(changes.,1)\n",
      "(computer,2)\n",
      "(decision,1)\n",
      "(describe,1)\n",
      "(disprove,1)\n",
      "(evaluate,1)\n",
      "(involves,1)\n",
      "(modeling,2)\n",
      "(patterns,1)\n",
      "(quantify,1)\n",
      "(recorded,1)\n",
      "(research,2)\n",
      "(software,1)\n",
      "(studying,1)\n",
      "(theories,1)\n",
      "(valuable,1)\n",
      "(Analytics,3)\n",
      "(analysis.,1)\n",
      "(analytics,16)\n",
      "(cognitive,1)\n",
      "(decisions,2)\n",
      "(discovery,1)\n",
      "(examining,1)\n",
      "(extensive,1)\n",
      "(knowledge,1)\n",
      "(marketing,2)\n",
      "(potential,1)\n",
      "(promotion,1)\n",
      "(scenario.,1)\n",
      "(software.,1)\n",
      "(Especially,1)\n",
      "(algorithms,1)\n",
      "(analytics.,1)\n",
      "(assortment,1)\n",
      "(commercial,1)\n",
      "(enterprise,1)\n",
      "(historical,1)\n",
      "(industries,1)\n",
      "(management,1)\n",
      "(meaningful,1)\n",
      "(operations,1)\n",
      "(predictive,2)\n",
      "(scientific,1)\n",
      "(scientists,1)\n",
      "(statistics,2)\n",
      "(techniques,1)\n",
      "(application,1)\n",
      "(computation,1)\n",
      "(conclusions,1)\n",
      "(descriptive,1)\n",
      "(hypotheses.,1)\n",
      "(information,2)\n",
      "(performance,1)\n",
      "(programming,1)\n",
      "(researchers,1)\n",
      "(specialized,1)\n",
      "(Specifically,1)\n",
      "(improvements,1)\n",
      "(increasingly,1)\n",
      "(mathematics.,1)\n",
      "(optimization,3)\n",
      "(performance.,2)\n",
      "(prescriptive,1)\n",
      "(simultaneous,1)\n",
      "(technologies,1)\n",
      "(Organizations,1)\n",
      "(communication,1)\n",
      "(more-informed,1)\n",
      "(organizations,1)\n",
      "(stock-keeping,1)\n",
      "(interpretation,1)\n",
      "(,0)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "-- genera una variable que cuenta las ocurrencias por cada grupo\n",
    "wordcount = FOREACH grouped GENERATE group, COUNT(words);\n",
    "DUMP wordcount;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- selecciona las primeras 15 palabras\n",
      " s = LIMIT wordcount 15;\n",
      " DUMP s;\n",
      "2022-05-12 19:51:23,855 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:25,146 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:25,156 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2022-05-12 19:51:25,164 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2022-05-12 19:51:26,001 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2022-05-12 19:51:26,428 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1652384900661_0007\n",
      "2022-05-12 19:51:26,432 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2022-05-12 19:51:26,457 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1652384900661_0007\n",
      "2022-05-12 19:51:26,460 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://650556b976c1:8088/proxy/application_1652384900661_0007/\n",
      "2022-05-12 19:51:41,563 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:41,569 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:41,602 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:41,606 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:41,623 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:41,625 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:43,348 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:43,360 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2022-05-12 19:51:43,366 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2022-05-12 19:51:43,789 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2022-05-12 19:51:44,212 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1652384900661_0008\n",
      "2022-05-12 19:51:44,217 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2022-05-12 19:51:44,440 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1652384900661_0008\n",
      "2022-05-12 19:51:44,443 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://650556b976c1:8088/proxy/application_1652384900661_0008/\n",
      "2022-05-12 19:51:59,558 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:59,565 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:59,617 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:59,620 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:59,640 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:59,642 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:59,665 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:59,667 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:59,688 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:59,690 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:59,710 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:59,712 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:59,732 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:59,734 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:59,754 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:59,757 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:59,777 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-12 19:51:59,779 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-12 19:51:59,801 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(a,1)\n",
      "(DA,1)\n",
      "(be,1)\n",
      "(by,2)\n",
      "(in,5)\n",
      "(is,3)\n",
      "(of,8)\n",
      "(on,1)\n",
      "(or,5)\n",
      "(to,12)\n",
      "(Big,1)\n",
      "(The,2)\n",
      "(aid,1)\n",
      "(and,15)\n",
      "(are,1)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "-- selecciona las primeras 15 palabras\n",
    "s = LIMIT wordcount 15;\n",
    "DUMP s;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_quit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza del HDFS y de la máquina local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted input/text0.txt\n",
      "Deleted input/text1.txt\n",
      "Deleted input/text2.txt\n",
      "Deleted output/_SUCCESS\n",
      "rm: `output/output': No such file or directory\n",
      "Deleted output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "## Se elimina el directorio de salida en el hdfs si existe\n",
    "!hadoop fs -rm input/*\n",
    "!hadoop fs -rm output/*\n",
    "!hadoop fs -rmdir input output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf input output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
