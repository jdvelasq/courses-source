{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordCount en Apache Pig\n",
    "===\n",
    "\n",
    "* 30 min | Última modificación: Noviembre 05, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea contar la frecuencia de ocurrencia de palabras en un conjunto de documentos usando Apache Pig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicio de la máquina virtual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa linux o macOS puede pasar directamente al siguiente paso. Inicie la VM con:\n",
    "\n",
    "```bash\n",
    "vagrant up\n",
    "```\n",
    "\n",
    "y luego vaya a la carpeta de trabajo:\n",
    "\n",
    "```\n",
    "cd /vagrant\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecución del contendor de Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si va a iniciar el contendor de Hadoop en la carpeta compartida con su máquina local use:\n",
    "\n",
    "```\n",
    "docker run --rm -it -v \"$PWD\":/datalake  --name pig -p 50070:50070 -p 8088:8088 -p 8888:8888 -p 5000:5000 jdvelasq/pig:0.17.0-pseudo\n",
    "```\n",
    "\n",
    "Si desea iniciar la sesión en el `datalake` use:\n",
    "\n",
    "```\n",
    "docker run --rm -it -v datalake:/datalake --name pig  -p 50070:50070 -p 8088:8088 -p 8888:8888 -p 5000:5000 jdvelasq/pig:0.17.0-pseudo\n",
    "```\n",
    "\n",
    "\n",
    "Si un contenedor ya se está ejecutando puede abrir un nuevo terminal con:\n",
    "\n",
    "```\n",
    "docker exec -it pig bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivos de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema. Puede usar directamente comandos del sistema operativo en el Terminal y el editor de texto `pico` para crear los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se crea el directorio de entrada\n",
    "!rm -rf input output\n",
    "!mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código en Apache Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota.** Se usan los dos guiones `--` para comentario de una línea y `/*` ... `*/` para comentarios de varias líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.pig\n",
    "\n",
    "-- crea la carpeta input in el HDFS\n",
    "fs -mkdir input\n",
    "\n",
    "-- copia los archivos del sistema local al HDFS\n",
    "fs -put input/ .\n",
    "\n",
    "-- carga de datos\n",
    "lines = LOAD 'input/text*.txt' AS (line:CHARARRAY);\n",
    "\n",
    "-- genera una tabla llamada words con una palabra por registro\n",
    "words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\n",
    "\n",
    "-- agrupa los registros que tienen la misma palabra\n",
    "grouped = GROUP words BY word;\n",
    "\n",
    "-- genera una variable que cuenta las ocurrencias por cada grupo\n",
    "wordcount = FOREACH grouped GENERATE group, COUNT(words);\n",
    "\n",
    "-- selecciona las primeras 15 palabras\n",
    "s = LIMIT wordcount 15;\n",
    "\n",
    "-- escribe el archivo de salida \n",
    "STORE s INTO 'output';\n",
    "\n",
    "-- copia los archivos del HDFS al sistema local\n",
    "fs -get output/ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución del script en modo batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique que Hadoop se esta ejecutando correctamente. Vaya a las direcciones:\n",
    "\n",
    "* NameNode: http://127.0.0.1:50070/\n",
    "\n",
    "* Yarn ResourceManager: http://127.0.0.1:8088/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-14 22:38:01,763 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "mkdir: `input': File exists\n",
      "2019-11-14 22:38:02,438 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2997: Encountered IOException. fs command '-mkdir input' failed. Please check output logs for details\n",
      "Details at logfile: /datalake/pig/pig_1573771081267.log\n"
     ]
    }
   ],
   "source": [
    "!pig -execute 'run script.pig'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de los resultados en el HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup          0 2019-11-14 22:37 output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         81 2019-11-14 22:37 output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\n",
      "DA\t1\n",
      "be\t1\n",
      "by\t2\n",
      "in\t5\n",
      "is\t3\n",
      "of\t8\n",
      "on\t1\n",
      "or\t5\n",
      "to\t12\n",
      "Big\t1\n",
      "The\t2\n",
      "aid\t1\n",
      "and\t15\n",
      "are\t1\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movimiento de los resultados al sistema local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/_SUCCESS  output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal output output\n",
    "!ls output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualilzación de resultados en el sistema local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/_SUCCESS\n",
      "output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!ls -1 output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\n",
      "DA\t1\n",
      "be\t1\n",
      "by\t2\n",
      "in\t5\n",
      "is\t3\n",
      "of\t8\n",
      "on\t1\n",
      "or\t5\n",
      "to\t12\n",
      "Big\t1\n",
      "The\t2\n",
      "aid\t1\n",
      "and\t15\n",
      "are\t1\n"
     ]
    }
   ],
   "source": [
    "!cat output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejecución de programas en Grunt**\n",
    "\n",
    "Se realiza con los comandos `exec` y `run`. \n",
    "\n",
    "    grunt> exec script\n",
    "    \n",
    "    grunt> run script\n",
    "    \n",
    "La diferencia entre estos comandos es que `exec` ejecuta el script sin importalo a `grunt`  mientras que `run` si lo hace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejecución de Pig en la consola de comandos**\n",
    "\n",
    "Pig es comúnmente usado en la línea de comandos para ejecutar tareas de ETL, análisis de datos y procesamiento interactivo. Para ejecutar Pig en la línea de comandos digite:\n",
    "\n",
    "    pig -x local\n",
    "    \n",
    "Si no usa la opción `x -local`, Pig usará el sistema de archivos HDFS.\n",
    "\n",
    "\n",
    "La opción `-e` o `-execute` permite ejecutar un comandos simple sin entrar a Grunt:\n",
    "\n",
    "    pig -e comando\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejecución de comandos del sistema operativo desde Pig**\n",
    "\n",
    "`Pig` permite la ejecución de comandos del sistema operativo; por ejemplo:\n",
    "\n",
    "     grunt> ls\n",
    "     \n",
    "También es posible usar comandos del sistema HDFS; el comando `hadoop dfs -ls /` se escribiría en `hive` como\n",
    "\n",
    "     grunt> fs -ls / ;\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supresion de información detallada\n",
    "\n",
    "Apache Pig imprime mucha información en pantalla relacionada con su ejecución y la Hadoop. Para regular el nivel de información entregada, se puede realizar una copia al directorio actual del archivo `./conf/log4j.properties` ubicado en la carpeta de instalación de Pig. \n",
    "    \n",
    "El archivo `log4j.properties` se modifica para que se impriman únicamente los mensajes de error de Pig y Hadoop. Para ello, se modifica la línea correspondiente para que quede así:\n",
    "\n",
    "    log4j.logger.org.apache.pig=error, A\n",
    "    \n",
    "y se agrega la siguiente para modificar el nivel de información entregado por Hadoop:\n",
    "\n",
    "    log4j.logger.org.apache.hadoop=error, A\n",
    "    \n",
    "Se invoca Pig con:\n",
    "\n",
    "    pig -4 log4j.properties\n",
    "    \n",
    "para indicar que se usa el archivo ubicado en la carpeta actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución de Pig en Jupyter\n",
    "\n",
    "A continuación se describe como ejecutar comandos de Pig en Jupyter usando la extensión de Jupyter `bigdata`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordCount en Apache Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan los archivos en Apache Pig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lines = LOAD 'input/text*.txt' AS (line:CHARARRAY);\n",
      " DUMP lines;\n",
      "2019-11-14 22:38:15,586 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:15,723 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2019-11-14 22:38:15,727 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2019-11-14 22:38:15,735 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.submit.replication is deprecated. Instead, use mapreduce.client.submit.file.replication\n",
      "2019-11-14 22:38:16,004 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2019-11-14 22:38:16,009 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:16,021 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2019-11-14 22:38:16,094 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2019-11-14 22:38:16,115 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2019-11-14 22:38:16,169 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2019-11-14 22:38:16,264 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1573770721210_0013\n",
      "2019-11-14 22:38:16,427 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2019-11-14 22:38:16,492 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1573770721210_0013\n",
      "2019-11-14 22:38:16,523 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://0f49bf10403a:8088/proxy/application_1573770721210_0013/\n",
      "2019-11-14 22:38:26,592 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:26,597 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:26,772 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:26,777 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:26,800 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2019-11-14 22:38:26,802 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:26,806 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:26,859 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:26,863 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:26,891 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:26,895 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:26,917 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:26,921 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:26,976 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(Analytics is the discovery, interpretation, and communication of meaningful patterns )\n",
      "(in data. Especially valuable in areas rich with recorded information, analytics relies )\n",
      "(on the simultaneous application of statistics, computer programming and operations research )\n",
      "(to quantify performance.)\n",
      "()\n",
      "(Organizations may apply analytics to business data to describe, predict, and improve business )\n",
      "(performance. Specifically, areas within analytics include predictive analytics, prescriptive )\n",
      "(analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big )\n",
      "(Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, )\n",
      "(marketing optimization and marketing mix modeling, web analytics, call analytics, speech )\n",
      "(analytics, sales force sizing and optimization, price and promotion modeling, predictive )\n",
      "(science, credit risk analysis, and fraud analytics. Since analytics can require extensive )\n",
      "(computation (see big data), the algorithms and software used for analytics harness the most )\n",
      "(current methods in computer science, statistics, and mathematics.)\n",
      "(Data analytics (DA) is the process of examining data sets in order to draw conclusions )\n",
      "(about the information they contain, increasingly with the aid of specialized systems )\n",
      "(and software. Data analytics technologies and techniques are widely used in commercial )\n",
      "(industries to enable organizations to make more-informed business decisions and by )\n",
      "(scientists and researchers to verify or disprove scientific models, theories and )\n",
      "(hypotheses.)\n",
      "(The field of data analysis. Analytics often involves studying past historical data to )\n",
      "(research potential trends, to analyze the effects of certain decisions or events, or to )\n",
      "(evaluate the performance of a given tool or scenario. The goal of analytics is to improve )\n",
      "(the business by gaining knowledge which can be used to make improvements or changes.)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "lines = LOAD 'input/text*.txt' AS (line:CHARARRAY);\n",
    "DUMP lines;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realiza el conteo de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- genera una tabla llamada words con una palabra por registro\n",
      " words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\n",
      " DUMP words;\n",
      "2019-11-14 22:38:27,350 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:27,525 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:27,541 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2019-11-14 22:38:27,559 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2019-11-14 22:38:27,597 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2019-11-14 22:38:27,631 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1573770721210_0014\n",
      "2019-11-14 22:38:27,634 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2019-11-14 22:38:27,875 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1573770721210_0014\n",
      "2019-11-14 22:38:27,880 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://0f49bf10403a:8088/proxy/application_1573770721210_0014/\n",
      "2019-11-14 22:38:43,124 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:43,128 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:43,216 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:43,220 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:43,244 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:43,247 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:43,279 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:43,283 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:43,311 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:43,315 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:43,336 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:43,341 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:38:43,373 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(Analytics)\n",
      "(is)\n",
      "(the)\n",
      "(discovery)\n",
      "(interpretation)\n",
      "(and)\n",
      "(communication)\n",
      "(of)\n",
      "(meaningful)\n",
      "(patterns)\n",
      "(in)\n",
      "(data.)\n",
      "(Especially)\n",
      "(valuable)\n",
      "(in)\n",
      "(areas)\n",
      "(rich)\n",
      "(with)\n",
      "(recorded)\n",
      "(information)\n",
      "(analytics)\n",
      "(relies)\n",
      "(on)\n",
      "(the)\n",
      "(simultaneous)\n",
      "(application)\n",
      "(of)\n",
      "(statistics)\n",
      "(computer)\n",
      "(programming)\n",
      "(and)\n",
      "(operations)\n",
      "(research)\n",
      "(to)\n",
      "(quantify)\n",
      "(performance.)\n",
      "()\n",
      "(Organizations)\n",
      "(may)\n",
      "(apply)\n",
      "(analytics)\n",
      "(to)\n",
      "(business)\n",
      "(data)\n",
      "(to)\n",
      "(describe)\n",
      "(predict)\n",
      "(and)\n",
      "(improve)\n",
      "(business)\n",
      "(performance.)\n",
      "(Specifically)\n",
      "(areas)\n",
      "(within)\n",
      "(analytics)\n",
      "(include)\n",
      "(predictive)\n",
      "(analytics)\n",
      "(prescriptive)\n",
      "(analytics)\n",
      "(enterprise)\n",
      "(decision)\n",
      "(management)\n",
      "(descriptive)\n",
      "(analytics)\n",
      "(cognitive)\n",
      "(analytics)\n",
      "(Big)\n",
      "(Data)\n",
      "(Analytics)\n",
      "(retail)\n",
      "(analytics)\n",
      "(store)\n",
      "(assortment)\n",
      "(and)\n",
      "(stock-keeping)\n",
      "(unit)\n",
      "(optimization)\n",
      "(marketing)\n",
      "(optimization)\n",
      "(and)\n",
      "(marketing)\n",
      "(mix)\n",
      "(modeling)\n",
      "(web)\n",
      "(analytics)\n",
      "(call)\n",
      "(analytics)\n",
      "(speech)\n",
      "(analytics)\n",
      "(sales)\n",
      "(force)\n",
      "(sizing)\n",
      "(and)\n",
      "(optimization)\n",
      "(price)\n",
      "(and)\n",
      "(promotion)\n",
      "(modeling)\n",
      "(predictive)\n",
      "(science)\n",
      "(credit)\n",
      "(risk)\n",
      "(analysis)\n",
      "(and)\n",
      "(fraud)\n",
      "(analytics.)\n",
      "(Since)\n",
      "(analytics)\n",
      "(can)\n",
      "(require)\n",
      "(extensive)\n",
      "(computation)\n",
      "(see)\n",
      "(big)\n",
      "(data)\n",
      "(the)\n",
      "(algorithms)\n",
      "(and)\n",
      "(software)\n",
      "(used)\n",
      "(for)\n",
      "(analytics)\n",
      "(harness)\n",
      "(the)\n",
      "(most)\n",
      "(current)\n",
      "(methods)\n",
      "(in)\n",
      "(computer)\n",
      "(science)\n",
      "(statistics)\n",
      "(and)\n",
      "(mathematics.)\n",
      "(Data)\n",
      "(analytics)\n",
      "(DA)\n",
      "(is)\n",
      "(the)\n",
      "(process)\n",
      "(of)\n",
      "(examining)\n",
      "(data)\n",
      "(sets)\n",
      "(in)\n",
      "(order)\n",
      "(to)\n",
      "(draw)\n",
      "(conclusions)\n",
      "(about)\n",
      "(the)\n",
      "(information)\n",
      "(they)\n",
      "(contain)\n",
      "(increasingly)\n",
      "(with)\n",
      "(the)\n",
      "(aid)\n",
      "(of)\n",
      "(specialized)\n",
      "(systems)\n",
      "(and)\n",
      "(software.)\n",
      "(Data)\n",
      "(analytics)\n",
      "(technologies)\n",
      "(and)\n",
      "(techniques)\n",
      "(are)\n",
      "(widely)\n",
      "(used)\n",
      "(in)\n",
      "(commercial)\n",
      "(industries)\n",
      "(to)\n",
      "(enable)\n",
      "(organizations)\n",
      "(to)\n",
      "(make)\n",
      "(more-informed)\n",
      "(business)\n",
      "(decisions)\n",
      "(and)\n",
      "(by)\n",
      "(scientists)\n",
      "(and)\n",
      "(researchers)\n",
      "(to)\n",
      "(verify)\n",
      "(or)\n",
      "(disprove)\n",
      "(scientific)\n",
      "(models)\n",
      "(theories)\n",
      "(and)\n",
      "(hypotheses.)\n",
      "(The)\n",
      "(field)\n",
      "(of)\n",
      "(data)\n",
      "(analysis.)\n",
      "(Analytics)\n",
      "(often)\n",
      "(involves)\n",
      "(studying)\n",
      "(past)\n",
      "(historical)\n",
      "(data)\n",
      "(to)\n",
      "(research)\n",
      "(potential)\n",
      "(trends)\n",
      "(to)\n",
      "(analyze)\n",
      "(the)\n",
      "(effects)\n",
      "(of)\n",
      "(certain)\n",
      "(decisions)\n",
      "(or)\n",
      "(events)\n",
      "(or)\n",
      "(to)\n",
      "(evaluate)\n",
      "(the)\n",
      "(performance)\n",
      "(of)\n",
      "(a)\n",
      "(given)\n",
      "(tool)\n",
      "(or)\n",
      "(scenario.)\n",
      "(The)\n",
      "(goal)\n",
      "(of)\n",
      "(analytics)\n",
      "(is)\n",
      "(to)\n",
      "(improve)\n",
      "(the)\n",
      "(business)\n",
      "(by)\n",
      "(gaining)\n",
      "(knowledge)\n",
      "(which)\n",
      "(can)\n",
      "(be)\n",
      "(used)\n",
      "(to)\n",
      "(make)\n",
      "(improvements)\n",
      "(or)\n",
      "(changes.)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "-- genera una tabla llamada words con una palabra por registro\n",
    "words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\n",
    "DUMP words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- agrupa los registros que tienen la misma palabra\n",
      " grouped = GROUP words BY word;\n",
      " DUMP grouped;\n",
      "2019-11-14 22:38:43,664 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:43,820 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:38:43,835 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2019-11-14 22:38:43,849 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2019-11-14 22:38:44,693 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2019-11-14 22:38:44,721 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1573770721210_0015\n",
      "2019-11-14 22:38:44,724 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2019-11-14 22:38:44,753 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1573770721210_0015\n",
      "2019-11-14 22:38:44,759 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://0f49bf10403a:8088/proxy/application_1573770721210_0015/\n",
      "2019-11-14 22:39:04,871 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:04,883 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:04,951 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:04,955 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:04,985 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:04,988 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:05,016 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:05,019 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:05,045 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:05,048 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:05,075 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:05,078 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:05,122 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(a,{(a)})\n",
      "(DA,{(DA)})\n",
      "(be,{(be)})\n",
      "(by,{(by),(by)})\n",
      "(in,{(in),(in),(in),(in),(in)})\n",
      "(is,{(is),(is),(is)})\n",
      "(of,{(of),(of),(of),(of),(of),(of),(of),(of)})\n",
      "(on,{(on)})\n",
      "(or,{(or),(or),(or),(or),(or)})\n",
      "(to,{(to),(to),(to),(to),(to),(to),(to),(to),(to),(to),(to),(to)})\n",
      "(Big,{(Big)})\n",
      "(The,{(The),(The)})\n",
      "(aid,{(aid)})\n",
      "(and,{(and),(and),(and),(and),(and),(and),(and),(and),(and),(and),(and),(and),(and),(and),(and)})\n",
      "(are,{(are)})\n",
      "(big,{(big)})\n",
      "(can,{(can),(can)})\n",
      "(for,{(for)})\n",
      "(may,{(may)})\n",
      "(mix,{(mix)})\n",
      "(see,{(see)})\n",
      "(the,{(the),(the),(the),(the),(the),(the),(the),(the),(the),(the)})\n",
      "(web,{(web)})\n",
      "(Data,{(Data),(Data),(Data)})\n",
      "(call,{(call)})\n",
      "(data,{(data),(data),(data),(data),(data)})\n",
      "(draw,{(draw)})\n",
      "(goal,{(goal)})\n",
      "(make,{(make),(make)})\n",
      "(most,{(most)})\n",
      "(past,{(past)})\n",
      "(rich,{(rich)})\n",
      "(risk,{(risk)})\n",
      "(sets,{(sets)})\n",
      "(they,{(they)})\n",
      "(tool,{(tool)})\n",
      "(unit,{(unit)})\n",
      "(used,{(used),(used),(used)})\n",
      "(with,{(with),(with)})\n",
      "(Since,{(Since)})\n",
      "(about,{(about)})\n",
      "(apply,{(apply)})\n",
      "(areas,{(areas),(areas)})\n",
      "(data.,{(data.)})\n",
      "(field,{(field)})\n",
      "(force,{(force)})\n",
      "(fraud,{(fraud)})\n",
      "(given,{(given)})\n",
      "(often,{(often)})\n",
      "(order,{(order)})\n",
      "(price,{(price)})\n",
      "(sales,{(sales)})\n",
      "(store,{(store)})\n",
      "(which,{(which)})\n",
      "(credit,{(credit)})\n",
      "(enable,{(enable)})\n",
      "(events,{(events)})\n",
      "(models,{(models)})\n",
      "(relies,{(relies)})\n",
      "(retail,{(retail)})\n",
      "(sizing,{(sizing)})\n",
      "(speech,{(speech)})\n",
      "(trends,{(trends)})\n",
      "(verify,{(verify)})\n",
      "(widely,{(widely)})\n",
      "(within,{(within)})\n",
      "(analyze,{(analyze)})\n",
      "(certain,{(certain)})\n",
      "(contain,{(contain)})\n",
      "(current,{(current)})\n",
      "(effects,{(effects)})\n",
      "(gaining,{(gaining)})\n",
      "(harness,{(harness)})\n",
      "(improve,{(improve),(improve)})\n",
      "(include,{(include)})\n",
      "(methods,{(methods)})\n",
      "(predict,{(predict)})\n",
      "(process,{(process)})\n",
      "(require,{(require)})\n",
      "(science,{(science),(science)})\n",
      "(systems,{(systems)})\n",
      "(analysis,{(analysis)})\n",
      "(business,{(business),(business),(business),(business)})\n",
      "(changes.,{(changes.)})\n",
      "(computer,{(computer),(computer)})\n",
      "(decision,{(decision)})\n",
      "(describe,{(describe)})\n",
      "(disprove,{(disprove)})\n",
      "(evaluate,{(evaluate)})\n",
      "(involves,{(involves)})\n",
      "(modeling,{(modeling),(modeling)})\n",
      "(patterns,{(patterns)})\n",
      "(quantify,{(quantify)})\n",
      "(recorded,{(recorded)})\n",
      "(research,{(research),(research)})\n",
      "(software,{(software)})\n",
      "(studying,{(studying)})\n",
      "(theories,{(theories)})\n",
      "(valuable,{(valuable)})\n",
      "(Analytics,{(Analytics),(Analytics),(Analytics)})\n",
      "(analysis.,{(analysis.)})\n",
      "(analytics,{(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics),(analytics)})\n",
      "(cognitive,{(cognitive)})\n",
      "(decisions,{(decisions),(decisions)})\n",
      "(discovery,{(discovery)})\n",
      "(examining,{(examining)})\n",
      "(extensive,{(extensive)})\n",
      "(knowledge,{(knowledge)})\n",
      "(marketing,{(marketing),(marketing)})\n",
      "(potential,{(potential)})\n",
      "(promotion,{(promotion)})\n",
      "(scenario.,{(scenario.)})\n",
      "(software.,{(software.)})\n",
      "(Especially,{(Especially)})\n",
      "(algorithms,{(algorithms)})\n",
      "(analytics.,{(analytics.)})\n",
      "(assortment,{(assortment)})\n",
      "(commercial,{(commercial)})\n",
      "(enterprise,{(enterprise)})\n",
      "(historical,{(historical)})\n",
      "(industries,{(industries)})\n",
      "(management,{(management)})\n",
      "(meaningful,{(meaningful)})\n",
      "(operations,{(operations)})\n",
      "(predictive,{(predictive),(predictive)})\n",
      "(scientific,{(scientific)})\n",
      "(scientists,{(scientists)})\n",
      "(statistics,{(statistics),(statistics)})\n",
      "(techniques,{(techniques)})\n",
      "(application,{(application)})\n",
      "(computation,{(computation)})\n",
      "(conclusions,{(conclusions)})\n",
      "(descriptive,{(descriptive)})\n",
      "(hypotheses.,{(hypotheses.)})\n",
      "(information,{(information),(information)})\n",
      "(performance,{(performance)})\n",
      "(programming,{(programming)})\n",
      "(researchers,{(researchers)})\n",
      "(specialized,{(specialized)})\n",
      "(Specifically,{(Specifically)})\n",
      "(improvements,{(improvements)})\n",
      "(increasingly,{(increasingly)})\n",
      "(mathematics.,{(mathematics.)})\n",
      "(optimization,{(optimization),(optimization),(optimization)})\n",
      "(performance.,{(performance.),(performance.)})\n",
      "(prescriptive,{(prescriptive)})\n",
      "(simultaneous,{(simultaneous)})\n",
      "(technologies,{(technologies)})\n",
      "(Organizations,{(Organizations)})\n",
      "(communication,{(communication)})\n",
      "(more-informed,{(more-informed)})\n",
      "(organizations,{(organizations)})\n",
      "(stock-keeping,{(stock-keeping)})\n",
      "(interpretation,{(interpretation)})\n",
      "(,{()})\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "-- agrupa los registros que tienen la misma palabra\n",
    "grouped = GROUP words BY word;\n",
    "DUMP grouped;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- genera una variable que cuenta las ocurrencias por cada grupo\n",
      " wordcount = FOREACH grouped GENERATE group, COUNT(words);\n",
      " DUMP wordcount;\n",
      "2019-11-14 22:39:05,464 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:06,417 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:06,437 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2019-11-14 22:39:06,456 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2019-11-14 22:39:06,489 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2019-11-14 22:39:06,909 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1573770721210_0016\n",
      "2019-11-14 22:39:06,913 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2019-11-14 22:39:07,142 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1573770721210_0016\n",
      "2019-11-14 22:39:07,146 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://0f49bf10403a:8088/proxy/application_1573770721210_0016/\n",
      "2019-11-14 22:39:27,278 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:27,285 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:27,334 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:27,337 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:27,358 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:27,360 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:27,380 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:27,383 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:27,405 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:27,408 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:27,428 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:27,431 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:27,459 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(a,1)\n",
      "(DA,1)\n",
      "(be,1)\n",
      "(by,2)\n",
      "(in,5)\n",
      "(is,3)\n",
      "(of,8)\n",
      "(on,1)\n",
      "(or,5)\n",
      "(to,12)\n",
      "(Big,1)\n",
      "(The,2)\n",
      "(aid,1)\n",
      "(and,15)\n",
      "(are,1)\n",
      "(big,1)\n",
      "(can,2)\n",
      "(for,1)\n",
      "(may,1)\n",
      "(mix,1)\n",
      "(see,1)\n",
      "(the,10)\n",
      "(web,1)\n",
      "(Data,3)\n",
      "(call,1)\n",
      "(data,5)\n",
      "(draw,1)\n",
      "(goal,1)\n",
      "(make,2)\n",
      "(most,1)\n",
      "(past,1)\n",
      "(rich,1)\n",
      "(risk,1)\n",
      "(sets,1)\n",
      "(they,1)\n",
      "(tool,1)\n",
      "(unit,1)\n",
      "(used,3)\n",
      "(with,2)\n",
      "(Since,1)\n",
      "(about,1)\n",
      "(apply,1)\n",
      "(areas,2)\n",
      "(data.,1)\n",
      "(field,1)\n",
      "(force,1)\n",
      "(fraud,1)\n",
      "(given,1)\n",
      "(often,1)\n",
      "(order,1)\n",
      "(price,1)\n",
      "(sales,1)\n",
      "(store,1)\n",
      "(which,1)\n",
      "(credit,1)\n",
      "(enable,1)\n",
      "(events,1)\n",
      "(models,1)\n",
      "(relies,1)\n",
      "(retail,1)\n",
      "(sizing,1)\n",
      "(speech,1)\n",
      "(trends,1)\n",
      "(verify,1)\n",
      "(widely,1)\n",
      "(within,1)\n",
      "(analyze,1)\n",
      "(certain,1)\n",
      "(contain,1)\n",
      "(current,1)\n",
      "(effects,1)\n",
      "(gaining,1)\n",
      "(harness,1)\n",
      "(improve,2)\n",
      "(include,1)\n",
      "(methods,1)\n",
      "(predict,1)\n",
      "(process,1)\n",
      "(require,1)\n",
      "(science,2)\n",
      "(systems,1)\n",
      "(analysis,1)\n",
      "(business,4)\n",
      "(changes.,1)\n",
      "(computer,2)\n",
      "(decision,1)\n",
      "(describe,1)\n",
      "(disprove,1)\n",
      "(evaluate,1)\n",
      "(involves,1)\n",
      "(modeling,2)\n",
      "(patterns,1)\n",
      "(quantify,1)\n",
      "(recorded,1)\n",
      "(research,2)\n",
      "(software,1)\n",
      "(studying,1)\n",
      "(theories,1)\n",
      "(valuable,1)\n",
      "(Analytics,3)\n",
      "(analysis.,1)\n",
      "(analytics,16)\n",
      "(cognitive,1)\n",
      "(decisions,2)\n",
      "(discovery,1)\n",
      "(examining,1)\n",
      "(extensive,1)\n",
      "(knowledge,1)\n",
      "(marketing,2)\n",
      "(potential,1)\n",
      "(promotion,1)\n",
      "(scenario.,1)\n",
      "(software.,1)\n",
      "(Especially,1)\n",
      "(algorithms,1)\n",
      "(analytics.,1)\n",
      "(assortment,1)\n",
      "(commercial,1)\n",
      "(enterprise,1)\n",
      "(historical,1)\n",
      "(industries,1)\n",
      "(management,1)\n",
      "(meaningful,1)\n",
      "(operations,1)\n",
      "(predictive,2)\n",
      "(scientific,1)\n",
      "(scientists,1)\n",
      "(statistics,2)\n",
      "(techniques,1)\n",
      "(application,1)\n",
      "(computation,1)\n",
      "(conclusions,1)\n",
      "(descriptive,1)\n",
      "(hypotheses.,1)\n",
      "(information,2)\n",
      "(performance,1)\n",
      "(programming,1)\n",
      "(researchers,1)\n",
      "(specialized,1)\n",
      "(Specifically,1)\n",
      "(improvements,1)\n",
      "(increasingly,1)\n",
      "(mathematics.,1)\n",
      "(optimization,3)\n",
      "(performance.,2)\n",
      "(prescriptive,1)\n",
      "(simultaneous,1)\n",
      "(technologies,1)\n",
      "(Organizations,1)\n",
      "(communication,1)\n",
      "(more-informed,1)\n",
      "(organizations,1)\n",
      "(stock-keeping,1)\n",
      "(interpretation,1)\n",
      "(,0)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "-- genera una variable que cuenta las ocurrencias por cada grupo\n",
    "wordcount = FOREACH grouped GENERATE group, COUNT(words);\n",
    "DUMP wordcount;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- selecciona las primeras 15 palabras\n",
      " s = LIMIT wordcount 15;\n",
      " DUMP s;\n",
      "2019-11-14 22:39:27,731 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:28,246 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:28,259 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2019-11-14 22:39:28,271 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2019-11-14 22:39:28,316 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2019-11-14 22:39:28,370 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1573770721210_0017\n",
      "2019-11-14 22:39:28,372 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2019-11-14 22:39:28,398 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1573770721210_0017\n",
      "2019-11-14 22:39:28,401 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://0f49bf10403a:8088/proxy/application_1573770721210_0017/\n",
      "2019-11-14 22:39:48,857 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:48,865 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:48,933 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:48,937 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:48,958 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:48,961 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:39:49,514 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:39:49,526 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2019-11-14 22:39:49,540 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2019-11-14 22:39:49,969 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2019-11-14 22:39:49,995 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1573770721210_0018\n",
      "2019-11-14 22:39:49,998 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2019-11-14 22:39:50,027 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1573770721210_0018\n",
      "2019-11-14 22:39:50,031 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://0f49bf10403a:8088/proxy/application_1573770721210_0018/\n",
      "2019-11-14 22:40:10,125 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:40:10,135 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:40:10,204 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:40:10,210 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:40:10,230 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:40:10,233 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:40:10,261 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:40:10,264 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:40:10,286 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:40:10,289 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:40:10,308 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:40:10,311 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:40:10,332 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:40:10,335 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:40:10,356 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:40:10,358 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:40:10,379 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2019-11-14 22:40:10,381 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2019-11-14 22:40:10,404 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(a,1)\n",
      "(DA,1)\n",
      "(be,1)\n",
      "(by,2)\n",
      "(in,5)\n",
      "(is,3)\n",
      "(of,8)\n",
      "(on,1)\n",
      "(or,5)\n",
      "(to,12)\n",
      "(Big,1)\n",
      "(The,2)\n",
      "(aid,1)\n",
      "(and,15)\n",
      "(are,1)\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "-- selecciona las primeras 15 palabras\n",
    "s = LIMIT wordcount 15;\n",
    "DUMP s;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_quit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza del HDFS y de la máquina local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted input/text0.txt\n",
      "Deleted input/text1.txt\n",
      "Deleted input/text2.txt\n",
      "Deleted output/_SUCCESS\n",
      "Deleted output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "## Se elimina el directorio de salida en el hdfs si existe\n",
    "!hadoop fs -rm input/*\n",
    "!hadoop fs -rm output/*\n",
    "!hadoop fs -rmdir input output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf input output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
