{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de palabras en Apache Pig\n",
    "===\n",
    "\n",
    "* Última modificación: Mayo 16, 2021 | YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Archivos de prueba\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema. Puede usar directamente comandos del sistema operativo en el Terminal y el editor de texto `pico` para crear los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/input /tmp/output\n",
    "!mkdir /tmp/input /tmp/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código en Apache Pig\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota.** Se usan los dos guiones `--` para comentario de una línea y `/*` ... `*/` para comentarios de varias líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/script.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/script.pig\n",
    "\n",
    "-- crea la carpeta input in el HDFS\n",
    "fs -mkdir input\n",
    "\n",
    "-- copia los archivos del sistema local al HDFS\n",
    "fs -put /tmp/input/ .\n",
    "\n",
    "-- carga de datos\n",
    "lines = LOAD 'input/text*.txt' AS (line:CHARARRAY);\n",
    "\n",
    "-- genera una tabla llamada words con una palabra por registro\n",
    "words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\n",
    "\n",
    "-- agrupa los registros que tienen la misma palabra\n",
    "grouped = GROUP words BY word;\n",
    "\n",
    "-- genera una variable que cuenta las ocurrencias por cada grupo\n",
    "wordcount = FOREACH grouped GENERATE group, COUNT(words);\n",
    "\n",
    "-- selecciona las primeras 15 palabras\n",
    "s = LIMIT wordcount 15;\n",
    "\n",
    "-- escribe el archivo de salida \n",
    "STORE s INTO 'output';\n",
    "\n",
    "-- copia los archivos del HDFS al sistema local\n",
    "fs -get output/ /tmp/output/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 15:02:07,185 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:02:08,742 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:02:08,830 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2022-05-27 15:02:08,854 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2022-05-27 15:02:08,922 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2022-05-27 15:02:09,123 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1653663693519_0001\n",
      "2022-05-27 15:02:09,223 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2022-05-27 15:02:09,272 [JobControl] INFO  org.apache.hadoop.conf.Configuration - resource-types.xml not found\n",
      "2022-05-27 15:02:09,272 [JobControl] INFO  org.apache.hadoop.yarn.util.resource.ResourceUtils - Unable to find 'resource-types.xml'.\n",
      "2022-05-27 15:02:09,276 [JobControl] INFO  org.apache.hadoop.yarn.util.resource.ResourceUtils - Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n",
      "2022-05-27 15:02:09,276 [JobControl] INFO  org.apache.hadoop.yarn.util.resource.ResourceUtils - Adding resource type - name = vcores, units = , type = COUNTABLE\n",
      "2022-05-27 15:02:09,480 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1653663693519_0001\n",
      "2022-05-27 15:02:09,508 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://64ccca48977e:8088/proxy/application_1653663693519_0001/\n",
      "2022-05-27 15:03:10,083 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:10,111 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-27 15:03:11,099 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:11,102 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-27 15:03:11,132 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:11,142 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-27 15:03:11,802 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:11,821 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2022-05-27 15:03:11,840 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2022-05-27 15:03:12,708 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2022-05-27 15:03:13,171 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1653663693519_0002\n",
      "2022-05-27 15:03:13,178 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2022-05-27 15:03:13,218 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1653663693519_0002\n",
      "2022-05-27 15:03:13,224 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://64ccca48977e:8088/proxy/application_1653663693519_0002/\n",
      "2022-05-27 15:03:28,344 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:28,348 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-27 15:03:28,459 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:28,465 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-27 15:03:28,503 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:28,509 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-27 15:03:28,550 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:28,553 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-27 15:03:28,575 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:28,578 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-27 15:03:28,596 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:28,599 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-27 15:03:28,624 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:28,628 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-27 15:03:28,648 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:28,652 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2022-05-27 15:03:28,668 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2022-05-27 15:03:28,673 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n"
     ]
    }
   ],
   "source": [
    "!pig -execute 'run /tmp/script.pig'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de los resultados en el HDFS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup          0 2022-05-27 15:03 output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         81 2022-05-27 15:03 output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\n",
      "DA\t1\n",
      "be\t1\n",
      "by\t2\n",
      "in\t5\n",
      "is\t3\n",
      "of\t8\n",
      "on\t1\n",
      "or\t5\n",
      "to\t12\n",
      "Big\t1\n",
      "The\t2\n",
      "aid\t1\n",
      "and\t15\n",
      "are\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualilzación de resultados en el sistema local\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-r-00000\n"
     ]
    }
   ],
   "source": [
    "ls -1 /tmp/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\n",
      "DA\t1\n",
      "be\t1\n",
      "by\t2\n",
      "in\t5\n",
      "is\t3\n",
      "of\t8\n",
      "on\t1\n",
      "or\t5\n",
      "to\t12\n",
      "Big\t1\n",
      "The\t2\n",
      "aid\t1\n",
      "and\t15\n",
      "are\t1\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/output/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notas\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejecución de programas en Grunt**\n",
    "\n",
    "Se realiza con los comandos `exec` y `run`. \n",
    "\n",
    "    grunt> exec script\n",
    "    \n",
    "    grunt> run script\n",
    "    \n",
    "La diferencia entre estos comandos es que `exec` ejecuta el script sin importalo a `grunt`  mientras que `run` si lo hace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejecución de Pig en la consola de comandos**\n",
    "\n",
    "Pig es comúnmente usado en la línea de comandos para ejecutar tareas de ETL, análisis de datos y procesamiento interactivo. Para ejecutar Pig en la línea de comandos digite:\n",
    "\n",
    "    pig -x local\n",
    "    \n",
    "Si no usa la opción `x -local`, Pig usará el sistema de archivos HDFS.\n",
    "\n",
    "\n",
    "La opción `-e` o `-execute` permite ejecutar un comandos simple sin entrar a Grunt:\n",
    "\n",
    "    pig -e comando\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejecución de comandos del sistema operativo desde Pig**\n",
    "\n",
    "`Pig` permite la ejecución de comandos del sistema operativo; por ejemplo:\n",
    "\n",
    "     grunt> ls\n",
    "     \n",
    "También es posible usar comandos del sistema HDFS; el comando `hadoop dfs -ls /` se escribiría en `hive` como\n",
    "\n",
    "     grunt> fs -ls / ;\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supresion de información detallada\n",
    "\n",
    "Apache Pig imprime mucha información en pantalla relacionada con su ejecución y la Hadoop. Para regular el nivel de información entregada, se puede realizar una copia al directorio actual del archivo `./conf/log4j.properties` ubicado en la carpeta de instalación de Pig. \n",
    "    \n",
    "El archivo `log4j.properties` se modifica para que se impriman únicamente los mensajes de error de Pig y Hadoop. Para ello, se modifica la línea correspondiente para que quede así:\n",
    "\n",
    "    log4j.logger.org.apache.pig=error, A\n",
    "    \n",
    "y se agrega la siguiente para modificar el nivel de información entregado por Hadoop:\n",
    "\n",
    "    log4j.logger.org.apache.hadoop=error, A\n",
    "    \n",
    "Se invoca Pig con:\n",
    "\n",
    "    pig -4 log4j.properties\n",
    "    \n",
    "para indicar que se usa el archivo ubicado en la carpeta actual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
