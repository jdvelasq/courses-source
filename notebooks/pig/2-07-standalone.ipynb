{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordCount en Apache Pig en modo standalone\n",
    "===\n",
    "\n",
    "* 30 min | Última modificación: Noviembre 15, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea contar la frecuencia de ocurrencia de palabras en un conjunto de documentos usando Apache Pig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicio de la máquina virtual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa linux o macOS puede pasar directamente al siguiente paso. Inicie la VM con:\n",
    "\n",
    "```bash\n",
    "vagrant up\n",
    "```\n",
    "\n",
    "y luego vaya a la carpeta de trabajo:\n",
    "\n",
    "```\n",
    "cd /vagrant\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecución del contendor de Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si va a iniciar el contendor de Hadoop en la carpeta compartida con su máquina local use:\n",
    "\n",
    "```\n",
    "docker run --rm -it -v \"$PWD\":/datalake  --name pig -p 8888:8888 jdvelasq/pig:0.17.0-standalone\n",
    "```\n",
    "\n",
    "Si desea iniciar la sesión en el `datalake` use:\n",
    "\n",
    "```\n",
    "docker run --rm -it -v datalake:/datalake --name pig  -p 8888:8888 jdvelasq/pig:0.17.0-standalone\n",
    "```\n",
    "\n",
    "\n",
    "Si un contenedor ya se está ejecutando puede abrir un nuevo terminal con:\n",
    "\n",
    "```\n",
    "docker exec -it pig bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivos de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema. Puede usar directamente comandos del sistema operativo en el Terminal y el editor de texto `pico` para crear los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se crea el directorio de entrada\n",
    "!rm -rf input tmp\n",
    "!mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código en Apache Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota.** Se usan los dos guiones `--` para comentario de una línea y `/*` ... `*/` para comentarios de varias líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.pig\n",
    "\n",
    "-- crea la carpeta input in el HDFS\n",
    "fs -mkdir tmp\n",
    "fs -mkdir tmp/input\n",
    "\n",
    "-- copia los archivos del sistema local al HDFS\n",
    "fs -put input/ tmp/\n",
    "\n",
    "-- carga de datos\n",
    "lines = LOAD 'tmp/input/text*.txt' AS (line:CHARARRAY);\n",
    "\n",
    "-- genera una tabla llamada words con una palabra por registro\n",
    "words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\n",
    "\n",
    "-- agrupa los registros que tienen la misma palabra\n",
    "grouped = GROUP words BY word;\n",
    "\n",
    "-- genera una variable que cuenta las ocurrencias por cada grupo\n",
    "wordcount = FOREACH grouped GENERATE group, COUNT(words);\n",
    "\n",
    "-- selecciona las primeras 15 palabras\n",
    "s = LIMIT wordcount 15;\n",
    "\n",
    "-- escribe el archivo de salida \n",
    "STORE s INTO 'tmp/output';\n",
    "\n",
    "-- copia los archivos del HDFS al sistema local (genera la carpeta output en el directorio actual)\n",
    "fs -get tmp/output/ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución del script en modo standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-15 02:31:43,492 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2019-11-15 02:31:44,405 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2019-11-15 02:31:44,678 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "2019-11-15 02:31:44,679 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "2019-11-15 02:31:44,701 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2019-11-15 02:31:44,704 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2019-11-15 02:31:44,736 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2019-11-15 02:31:44,743 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.submit.replication is deprecated. Instead, use mapreduce.client.submit.file.replication\n",
      "2019-11-15 02:31:44,851 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2019-11-15 02:31:44,861 [JobControl] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:44,872 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2019-11-15 02:31:44,924 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2019-11-15 02:31:45,026 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 3\n",
      "2019-11-15 02:31:45,057 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2019-11-15 02:31:45,138 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1059171614_0001\n",
      "2019-11-15 02:31:45,342 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1573785105218/pig-0.17.0-core-h2.jar <- /datalake/pig/pig-0.17.0-core-h2.jar\n",
      "2019-11-15 02:31:45,352 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1327976925/tmp-681383038/pig-0.17.0-core-h2.jar as file:/tmp/hadoop-root/mapred/local/1573785105218/pig-0.17.0-core-h2.jar\n",
      "2019-11-15 02:31:45,366 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1573785105219/automaton-1.11-8.jar <- /datalake/pig/automaton-1.11-8.jar\n",
      "2019-11-15 02:31:45,369 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1327976925/tmp1410257452/automaton-1.11-8.jar as file:/tmp/hadoop-root/mapred/local/1573785105219/automaton-1.11-8.jar\n",
      "2019-11-15 02:31:45,369 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1573785105220/antlr-runtime-3.4.jar <- /datalake/pig/antlr-runtime-3.4.jar\n",
      "2019-11-15 02:31:45,372 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1327976925/tmp208607162/antlr-runtime-3.4.jar as file:/tmp/hadoop-root/mapred/local/1573785105220/antlr-runtime-3.4.jar\n",
      "2019-11-15 02:31:45,372 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1573785105221/joda-time-2.9.3.jar <- /datalake/pig/joda-time-2.9.3.jar\n",
      "2019-11-15 02:31:45,375 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1327976925/tmp-1341981989/joda-time-2.9.3.jar as file:/tmp/hadoop-root/mapred/local/1573785105221/joda-time-2.9.3.jar\n",
      "2019-11-15 02:31:45,443 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1573785105218/pig-0.17.0-core-h2.jar\n",
      "2019-11-15 02:31:45,444 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1573785105219/automaton-1.11-8.jar\n",
      "2019-11-15 02:31:45,444 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1573785105220/antlr-runtime-3.4.jar\n",
      "2019-11-15 02:31:45,444 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1573785105221/joda-time-2.9.3.jar\n",
      "2019-11-15 02:31:45,448 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
      "2019-11-15 02:31:45,450 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
      "2019-11-15 02:31:45,487 [Thread-14] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2019-11-15 02:31:45,487 [Thread-14] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2019-11-15 02:31:45,490 [Thread-14] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2019-11-15 02:31:45,490 [Thread-14] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2019-11-15 02:31:45,491 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
      "2019-11-15 02:31:45,547 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
      "2019-11-15 02:31:45,547 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1059171614_0001_m_000000_0\n",
      "2019-11-15 02:31:45,587 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2019-11-15 02:31:45,587 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2019-11-15 02:31:45,603 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2019-11-15 02:31:45,608 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :3\n",
      "Total Length = 1885\n",
      "Input split[0]:\n",
      "   Length = 1093\n",
      "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
      "   Locations:\n",
      "\n",
      "-----------------------\n",
      "Input split[1]:\n",
      "   Length = 440\n",
      "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
      "   Locations:\n",
      "\n",
      "-----------------------\n",
      "Input split[2]:\n",
      "   Length = 352\n",
      "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
      "   Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2019-11-15 02:31:45,673 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2019-11-15 02:31:45,673 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2019-11-15 02:31:45,673 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2019-11-15 02:31:45,673 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2019-11-15 02:31:45,673 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2019-11-15 02:31:45,678 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2019-11-15 02:31:45,802 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - \n",
      "2019-11-15 02:31:45,802 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2019-11-15 02:31:45,802 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2019-11-15 02:31:45,802 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 3602; bufvoid = 104857600\n",
      "2019-11-15 02:31:45,802 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 26213388(104853552); length = 1009/6553600\n",
      "2019-11-15 02:31:45,858 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2019-11-15 02:31:45,872 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1059171614_0001_m_000000_0 is done. And is in the process of committing\n",
      "2019-11-15 02:31:45,900 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n",
      "2019-11-15 02:31:45,900 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1059171614_0001_m_000000_0' done.\n",
      "2019-11-15 02:31:45,904 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1059171614_0001_m_000000_0: Counters: 18\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5805191\n",
      "\t\tFILE: Number of bytes written=12096229\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=24\n",
      "\t\tMap output records=253\n",
      "\t\tMap output bytes=3602\n",
      "\t\tMap output materialized bytes=2687\n",
      "\t\tInput split bytes=483\n",
      "\t\tCombine input records=253\n",
      "\t\tCombine output records=155\n",
      "\t\tSpilled Records=155\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=400556032\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "2019-11-15 02:31:45,904 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1059171614_0001_m_000000_0\n",
      "2019-11-15 02:31:45,904 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
      "2019-11-15 02:31:45,908 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks\n",
      "2019-11-15 02:31:45,908 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1059171614_0001_r_000000_0\n",
      "2019-11-15 02:31:45,930 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2019-11-15 02:31:45,930 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2019-11-15 02:31:45,946 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2019-11-15 02:31:45,949 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@34b2efd6\n",
      "2019-11-15 02:31:45,962 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=652528832, maxSingleShuffleLimit=163132208, mergeThreshold=430669056, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2019-11-15 02:31:45,964 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local1059171614_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2019-11-15 02:31:45,998 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local1059171614_0001_m_000000_0 decomp: 2683 len: 2687 to MEMORY\n",
      "2019-11-15 02:31:46,005 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 2683 bytes from map-output for attempt_local1059171614_0001_m_000000_0\n",
      "2019-11-15 02:31:46,007 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 2683, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2683\n",
      "2019-11-15 02:31:46,008 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning\n",
      "2019-11-15 02:31:46,009 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2019-11-15 02:31:46,009 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2019-11-15 02:31:46,014 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2019-11-15 02:31:46,014 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 2677 bytes\n",
      "2019-11-15 02:31:46,018 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 1 segments, 2683 bytes to disk to satisfy reduce memory limit\n",
      "2019-11-15 02:31:46,019 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 2687 bytes from disk\n",
      "2019-11-15 02:31:46,020 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce\n",
      "2019-11-15 02:31:46,020 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2019-11-15 02:31:46,021 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 2677 bytes\n",
      "2019-11-15 02:31:46,022 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2019-11-15 02:31:46,025 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2019-11-15 02:31:46,026 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2019-11-15 02:31:46,027 [pool-4-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2019-11-15 02:31:46,038 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1059171614_0001_r_000000_0 is done. And is in the process of committing\n",
      "2019-11-15 02:31:46,041 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2019-11-15 02:31:46,041 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local1059171614_0001_r_000000_0 is allowed to commit now\n",
      "2019-11-15 02:31:46,044 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local1059171614_0001_r_000000_0' to file:/tmp/temp-1327976925/tmp-382248047/_temporary/0/task_local1059171614_0001_r_000000\n",
      "2019-11-15 02:31:46,045 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2019-11-15 02:31:46,045 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1059171614_0001_r_000000_0' done.\n",
      "2019-11-15 02:31:46,046 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1059171614_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5810597\n",
      "\t\tFILE: Number of bytes written=12099135\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=155\n",
      "\t\tReduce shuffle bytes=2687\n",
      "\t\tReduce input records=155\n",
      "\t\tReduce output records=15\n",
      "\t\tSpilled Records=155\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=12\n",
      "\t\tTotal committed heap usage (bytes)=400556032\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "2019-11-15 02:31:46,046 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1059171614_0001_r_000000_0\n",
      "2019-11-15 02:31:46,047 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.\n",
      "2019-11-15 02:31:50,467 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:50,479 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:50,479 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2019-11-15 02:31:50,481 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:50,548 [JobControl] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:50,553 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2019-11-15 02:31:50,632 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2019-11-15 02:31:50,634 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2019-11-15 02:31:50,648 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1598448235_0002\n",
      "2019-11-15 02:31:50,800 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1573785110672/pig-0.17.0-core-h2.jar <- /datalake/pig/pig-0.17.0-core-h2.jar\n",
      "2019-11-15 02:31:50,803 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1327976925/tmp197641247/pig-0.17.0-core-h2.jar as file:/tmp/hadoop-root/mapred/local/1573785110672/pig-0.17.0-core-h2.jar\n",
      "2019-11-15 02:31:50,803 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1573785110673/automaton-1.11-8.jar <- /datalake/pig/automaton-1.11-8.jar\n",
      "2019-11-15 02:31:50,807 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1327976925/tmp-1024201414/automaton-1.11-8.jar as file:/tmp/hadoop-root/mapred/local/1573785110673/automaton-1.11-8.jar\n",
      "2019-11-15 02:31:50,807 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1573785110674/antlr-runtime-3.4.jar <- /datalake/pig/antlr-runtime-3.4.jar\n",
      "2019-11-15 02:31:50,810 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1327976925/tmp1175028320/antlr-runtime-3.4.jar as file:/tmp/hadoop-root/mapred/local/1573785110674/antlr-runtime-3.4.jar\n",
      "2019-11-15 02:31:50,811 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1573785110675/joda-time-2.9.3.jar <- /datalake/pig/joda-time-2.9.3.jar\n",
      "2019-11-15 02:31:50,814 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1327976925/tmp663184705/joda-time-2.9.3.jar as file:/tmp/hadoop-root/mapred/local/1573785110675/joda-time-2.9.3.jar\n",
      "2019-11-15 02:31:50,859 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1573785110672/pig-0.17.0-core-h2.jar\n",
      "2019-11-15 02:31:50,859 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1573785110673/automaton-1.11-8.jar\n",
      "2019-11-15 02:31:50,859 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1573785110674/antlr-runtime-3.4.jar\n",
      "2019-11-15 02:31:50,859 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1573785110675/joda-time-2.9.3.jar\n",
      "2019-11-15 02:31:50,860 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
      "2019-11-15 02:31:50,861 [Thread-54] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
      "2019-11-15 02:31:50,871 [Thread-54] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2019-11-15 02:31:50,872 [Thread-54] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2019-11-15 02:31:50,872 [Thread-54] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2019-11-15 02:31:50,873 [Thread-54] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2019-11-15 02:31:50,873 [Thread-54] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2019-11-15 02:31:50,873 [Thread-54] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
      "2019-11-15 02:31:50,882 [Thread-54] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
      "2019-11-15 02:31:50,882 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1598448235_0002_m_000000_0\n",
      "2019-11-15 02:31:50,893 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2019-11-15 02:31:50,893 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2019-11-15 02:31:50,893 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2019-11-15 02:31:50,896 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 207\n",
      "Input split[0]:\n",
      "   Length = 207\n",
      "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
      "   Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2019-11-15 02:31:50,951 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2019-11-15 02:31:50,951 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2019-11-15 02:31:50,951 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2019-11-15 02:31:50,951 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2019-11-15 02:31:50,952 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2019-11-15 02:31:50,953 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2019-11-15 02:31:50,959 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - \n",
      "2019-11-15 02:31:50,959 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2019-11-15 02:31:50,959 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2019-11-15 02:31:50,959 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 237; bufvoid = 104857600\n",
      "2019-11-15 02:31:50,959 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 26214340(104857360); length = 57/6553600\n",
      "2019-11-15 02:31:50,961 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2019-11-15 02:31:50,962 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1598448235_0002_m_000000_0 is done. And is in the process of committing\n",
      "2019-11-15 02:31:50,965 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n",
      "2019-11-15 02:31:50,965 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1598448235_0002_m_000000_0' done.\n",
      "2019-11-15 02:31:50,965 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1598448235_0002_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11612080\n",
      "\t\tFILE: Number of bytes written=24179434\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=15\n",
      "\t\tMap output records=15\n",
      "\t\tMap output bytes=237\n",
      "\t\tMap output materialized bytes=273\n",
      "\t\tInput split bytes=379\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=15\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=505937920\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "2019-11-15 02:31:50,966 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1598448235_0002_m_000000_0\n",
      "2019-11-15 02:31:50,966 [Thread-54] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
      "2019-11-15 02:31:50,967 [Thread-54] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks\n",
      "2019-11-15 02:31:50,967 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1598448235_0002_r_000000_0\n",
      "2019-11-15 02:31:50,979 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2019-11-15 02:31:50,979 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2019-11-15 02:31:50,981 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2019-11-15 02:31:50,981 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@bb946c0\n",
      "2019-11-15 02:31:50,982 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=652528832, maxSingleShuffleLimit=163132208, mergeThreshold=430669056, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2019-11-15 02:31:50,983 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local1598448235_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2019-11-15 02:31:50,987 [localfetcher#2] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#2 about to shuffle output of map attempt_local1598448235_0002_m_000000_0 decomp: 269 len: 273 to MEMORY\n",
      "2019-11-15 02:31:50,988 [localfetcher#2] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 269 bytes from map-output for attempt_local1598448235_0002_m_000000_0\n",
      "2019-11-15 02:31:50,988 [localfetcher#2] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 269, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->269\n",
      "2019-11-15 02:31:50,990 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning\n",
      "2019-11-15 02:31:50,991 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2019-11-15 02:31:50,991 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2019-11-15 02:31:50,992 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2019-11-15 02:31:50,992 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 256 bytes\n",
      "2019-11-15 02:31:50,994 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 1 segments, 269 bytes to disk to satisfy reduce memory limit\n",
      "2019-11-15 02:31:50,994 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 273 bytes from disk\n",
      "2019-11-15 02:31:50,994 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce\n",
      "2019-11-15 02:31:50,994 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2019-11-15 02:31:50,995 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 256 bytes\n",
      "2019-11-15 02:31:50,995 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2019-11-15 02:31:51,009 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2019-11-15 02:31:51,009 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2019-11-15 02:31:51,037 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1598448235_0002_r_000000_0 is done. And is in the process of committing\n",
      "2019-11-15 02:31:51,044 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2019-11-15 02:31:51,044 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local1598448235_0002_r_000000_0 is allowed to commit now\n",
      "2019-11-15 02:31:51,051 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local1598448235_0002_r_000000_0' to file:/datalake/pig/tmp/output/_temporary/0/task_local1598448235_0002_r_000000\n",
      "2019-11-15 02:31:51,052 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2019-11-15 02:31:51,052 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1598448235_0002_r_000000_0' done.\n",
      "2019-11-15 02:31:51,053 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1598448235_0002_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11612658\n",
      "\t\tFILE: Number of bytes written=24179800\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=15\n",
      "\t\tReduce shuffle bytes=273\n",
      "\t\tReduce input records=15\n",
      "\t\tReduce output records=15\n",
      "\t\tSpilled Records=15\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=505937920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "2019-11-15 02:31:51,053 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1598448235_0002_r_000000_0\n",
      "2019-11-15 02:31:51,053 [Thread-54] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.\n",
      "2019-11-15 02:31:56,065 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:56,066 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:56,067 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:56,080 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:56,081 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:56,082 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:56,090 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:56,091 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2019-11-15 02:31:56,092 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n"
     ]
    }
   ],
   "source": [
    "!pig -execute 'run script.pig'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de los resultados en el HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root root          0 2019-11-15 02:31 tmp/output/_SUCCESS\n",
      "-rw-r--r--   1 root root         81 2019-11-15 02:31 tmp/output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls tmp/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\n",
      "DA\t1\n",
      "be\t1\n",
      "by\t2\n",
      "in\t5\n",
      "is\t3\n",
      "of\t8\n",
      "on\t1\n",
      "or\t5\n",
      "to\t12\n",
      "Big\t1\n",
      "The\t2\n",
      "aid\t1\n",
      "and\t15\n",
      "are\t1\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat tmp/output/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza del HDFS y de la máquina local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf input tmp output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
