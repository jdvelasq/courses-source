{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop en modo standalone (sin hdfs)\n",
    "===\n",
    "\n",
    "* *30 min* | Última modificación: Noviembre 14, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del problema\n",
    "\n",
    "Se desea contar la frecuencia de ocurrencia de palabras en un conjunto de documentos. Debido a los requerimientos de diseño (gran volúmen de datos y tiempos rápidos de respuesta) se desea implementar una arquitectura Big Data. \n",
    "\n",
    "\n",
    "Se desea **probar** la implementación en **Python** y ejecutar Hadoop en **modo standalone** para depuración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se usa la opción streaming de Hadoop. Se el tutorial anterior como punto de inicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicio de la máquina virtual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usa linux o macOS puede pasar directamente al siguiente paso. Inicie la VM con:\n",
    "\n",
    "```bash\n",
    "vagrant up\n",
    "```\n",
    "\n",
    "y luego vaya a la carpeta de trabajo:\n",
    "\n",
    "```\n",
    "cd /vagrant\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecución del contendor de Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si va a iniciar el contendor de Hadoop en la carpeta compartida con su máquina local use:\n",
    "\n",
    "```\n",
    "docker run --rm -it -v \"$PWD\":/datalake --name hadoop -p 8888:8888 jdvelasq/hadoop:2.8.5-standalone\n",
    "```\n",
    "\n",
    "Si desea iniciar la sesión en el `datalake` use:\n",
    "\n",
    "```\n",
    "docker run --rm -it -v datalake:/datalake --name hadoop -p 8888:8888 jdvelasq/hadoop:2.8.5-standalone\n",
    "```\n",
    "\n",
    "\n",
    "Si un contenedor ya se está ejecutando puede abrir un nuevo terminal con:\n",
    "\n",
    "```\n",
    "docker exec -it hadoop bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Archivos de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generarán tres archivos de prueba para probar el sistema. Puede usar directamente comandos del sistema operativo en el Terminal y el editor de texto `pico` para crear los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se crea el directorio de entrada en la carpeta actual simulando el HDFS\n",
    "!hadoop fs -mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de la implementación dentro de Hadoop\n",
    "\n",
    "En el modo standalone, el hdfs opera directamente sobre los archivos de la máquina local. En este caso en la máquina virtual creada Vagrant.\n",
    "\n",
    "La implementación es idéntica y solo cambia el manejo de archivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "##\n",
    "## Esta es la funcion que mapea la entrada a parejas (clave, valor)\n",
    "##\n",
    "import sys\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    ##\n",
    "    ## itera sobre cada linea de codigo recibida\n",
    "    ## a traves del flujo de entrada\n",
    "    ##\n",
    "    for line in sys.stdin:\n",
    "        \n",
    "        ##\n",
    "        ## genera las tuplas palabra \\tabulador 1\n",
    "        ## ya que es un conteo de palabras\n",
    "        ##\n",
    "        for word in line.split(): \n",
    "                   \n",
    "            ##\n",
    "            ## escribe al flujo estandar de salida\n",
    "            ##\n",
    "            sys.stdout.write(\"{}\\t1\\n\".format(word))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: '/tmp/input/text*.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "## la salida de la función anterior es:\n",
    "!cat /tmp/input/text*.txt | python3 mapper.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3\n",
    "\n",
    "Se implementa la función `reduce` y se salva en el archivo `reducer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "\n",
    "import sys\n",
    "\n",
    "##\n",
    "## Esta funcion reduce los elementos que \n",
    "## tienen la misma clave\n",
    "##\n",
    "\n",
    "if __name__ == '__main__': \n",
    "  \n",
    "    curkey = None\n",
    "    total = 0\n",
    "    \n",
    "    ##\n",
    "    ## cada linea de texto recibida es una \n",
    "    ## entrada clave \\tabulador valor\n",
    "    ##\n",
    "    for line in sys.stdin:\n",
    "        \n",
    "        key, val = line.split(\"\\t\") \n",
    "        val = int(val)\n",
    "        \n",
    "        if key == curkey: \n",
    "            ##\n",
    "            ## No se ha cambiado de clave. Aca se \n",
    "            ## acumulan los valores para la misma clave.\n",
    "            ##\n",
    "            total += val  \n",
    "        else:\n",
    "            ##\n",
    "            ## Se cambio de clave. Se reinicia el\n",
    "            ## acumulador.\n",
    "            ##\n",
    "            if curkey is not None:\n",
    "                ##\n",
    "                ## una vez se han reducido todos los elementos\n",
    "                ## con la misma clave se imprime el resultado en\n",
    "                ## el flujo de salida\n",
    "                ##\n",
    "                sys.stdout.write(\"{}\\t{}\\n\".format(curkey, total)) \n",
    "            \n",
    "            curkey = key\n",
    "            total = val\n",
    "            \n",
    "    sys.stdout.write(\"{}\\t{}\\n\".format(curkey, total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t3\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Specifically,\t1\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## La función sort hace que todos los elementos con \n",
    "## la misma clave queden en lineas consecutivas.\n",
    "## Hace el papel del módulo Shuffle & Sort\n",
    "##\n",
    "!cat ./input/text*.txt | python3 mapper.py | sort | python3 reducer.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución en Hadoop en modo standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1. Movimiento de los datos al HDFS\n",
    "\n",
    "Se copian los archivos de entrada del sistema local (o del datalake) al HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root root       1093 2019-11-15 02:17 tmp/input/text0.txt\n",
      "-rw-r--r--   1 root root        352 2019-11-15 02:17 tmp/input/text1.txt\n",
      "-rw-r--r--   1 root root        440 2019-11-15 02:17 tmp/input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir tmp\n",
    "!hadoop fs -mkdir tmp/input\n",
    "!hadoop fs -copyFromLocal input/* tmp/input\n",
    "!hadoop fs -ls tmp/input/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los comandos de la opción fs de Hadoop siguen siendo válidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Se ejecuta en Hadoop.\n",
    "##   -files: archivos a copiar al hdfs\n",
    "##   -input: archivo de entrada\n",
    "##   -output: directorio de salida\n",
    "##   -file: archivos a copiar de la maquina local al hdfs\n",
    "##   -maper: programa que ejecuta el map\n",
    "##   -reducer: programa que ejecuta la reducción\n",
    "##\n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar -input tmp/input  -output tmp/output -mapper 'python3 mapper.py' -reducer 'python3 reducer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root root          0 2019-11-15 02:17 tmp/output/_SUCCESS\n",
      "-rw-r--r--   1 root root       1649 2019-11-15 02:17 tmp/output/part-00000\n"
     ]
    }
   ],
   "source": [
    "## contenido del directorio con los \n",
    "## resultados de la corrida\n",
    "!hadoop fs -ls tmp/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t3\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Specifically,\t1\n"
     ]
    }
   ],
   "source": [
    "## se visualiza el archivo con los\n",
    "## resultados de la corrida\n",
    "!hadoop fs -cat tmp/output/part-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de la máquina local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm reducer.py mapper.py\n",
    "!rm -rf input tmp"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
