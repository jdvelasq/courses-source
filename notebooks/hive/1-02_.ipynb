{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de palabras en Apache Hive (script)\n",
    "===\n",
    "\n",
    "* Última modificación: Mayo 17, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datos\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/wordcount/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/wordcount/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/wordcount/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versión en productivo\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la segunda parte, se procede a llevar el aplicativo a productivo con los siguientes cambios:\n",
    "\n",
    "* Los datos son leidos del sistema HDFS de Hadoop.\n",
    "\n",
    "* Los resultdos son guardados en una carpeta del sistema Hadoop.\n",
    "\n",
    "* El script se almacena en un archivo en el disco duro, para su uso posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copia de los datos al sistema HDFS\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxrwx---   - root supergroup          0 2022-05-27 04:19 /tmp/hadoop-yarn\n",
      "drwxrwxrwx   - root supergroup          0 2022-05-27 04:19 /tmp/hive\n",
      "drwxrwxrwx   - root supergroup          0 2022-05-27 04:25 /tmp/output\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se usan un directorio temporal en el HDFS. La siguiente\n",
    "# instrucción muestra el contenido del dicho directorio\n",
    "#\n",
    "!hdfs dfs -ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Crea la carpeta wordcount en el hdfs\n",
    "#\n",
    "!hdfs dfs -mkdir /tmp/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxrwx---   - root supergroup          0 2022-05-27 04:19 /tmp/hadoop-yarn\n",
      "drwxrwxrwx   - root supergroup          0 2022-05-27 04:19 /tmp/hive\n",
      "drwxrwxrwx   - root supergroup          0 2022-05-27 04:25 /tmp/output\n",
      "drwxr-xr-x   - root supergroup          0 2022-05-27 04:29 /tmp/wordcount\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Verifica la creación de la carpeta\n",
    "#\n",
    "!hdfs dfs -ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copia los archvios del directorio local /tmp/wordcount/\n",
    "# al directorio /tmp/wordcount/ en el hdfs\n",
    "#\n",
    "!hdfs dfs -copyFromLocal /tmp/wordcount/*  /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 root supergroup       1093 2022-05-27 04:29 /tmp/wordcount/text0.txt\n",
      "-rw-r--r--   1 root supergroup        352 2022-05-27 04:29 /tmp/wordcount/text1.txt\n",
      "-rw-r--r--   1 root supergroup        440 2022-05-27 04:29 /tmp/wordcount/text2.txt\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Verifica que los archivos esten copiados\n",
    "# en el hdfs\n",
    "#\n",
    "!hdfs dfs -ls /tmp/wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generación del script y ajuste del código\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizan dos cambios. En primer lugar, se sustituye la línea \n",
    "\n",
    "    LOAD DATA LOCAL INPATH \"wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "    \n",
    "por:\n",
    "\n",
    "    LOAD DATA INPATH \"/tmp/wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "\n",
    "para que Hive lea los datos del directorio `/tmp/wordcount/` en el HDFS. En segundo lugar, se agrega\n",
    "\n",
    "    INSERT OVERWRITE DIRECTORY '/tmp/output' \n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "    SELECT * FROM word_counts;\n",
    "    \n",
    "para que los resultados sean almacenados en la carpeta `/tmp/output` como un archivo en formato CSV. El programa es guadado como `wordcount.hql` en el computador local. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/wordcount.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount.hql\n",
    "\n",
    "DROP TABLE IF EXISTS docs;\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "\n",
    "CREATE TABLE docs (line STRING);\n",
    "\n",
    "LOAD DATA INPATH \"/tmp/wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "\n",
    "CREATE TABLE word_counts \n",
    "AS\n",
    "    SELECT word, count(1) AS count \n",
    "    FROM\n",
    "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
    "GROUP BY \n",
    "    word\n",
    "ORDER BY \n",
    "    word;\n",
    "    \n",
    "INSERT OVERWRITE DIRECTORY '/tmp/output' \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "SELECT * FROM word_counts;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-2.3.9.jar!/hive-log4j2.properties Async: true\n",
      "OK\n",
      "Time taken: 6.613 seconds\n",
      "OK\n",
      "Time taken: 0.049 seconds\n",
      "OK\n",
      "Time taken: 0.421 seconds\n",
      "Loading data to table default.docs\n",
      "OK\n",
      "Time taken: 0.53 seconds\n",
      "Query ID = root_20220527042936_e173e339-063c-4111-ae65-3cc9d15c8d89\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1653625133568_0008, Tracking URL = http://10e0f132c3fa:8088/proxy/application_1653625133568_0008/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1653625133568_0008\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2022-05-27 04:29:43,190 Stage-1 map = 0%,  reduce = 0%\n",
      "2022-05-27 04:29:46,333 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.05 sec\n",
      "2022-05-27 04:29:51,482 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.35 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 350 msec\n",
      "Ended Job = job_1653625133568_0008\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1653625133568_0009, Tracking URL = http://10e0f132c3fa:8088/proxy/application_1653625133568_0009/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1653625133568_0009\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2022-05-27 04:30:02,186 Stage-2 map = 0%,  reduce = 0%\n",
      "2022-05-27 04:30:06,309 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.15 sec\n",
      "2022-05-27 04:30:11,448 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.71 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 710 msec\n",
      "Ended Job = job_1653625133568_0009\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/word_counts\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.35 sec   HDFS Read: 10194 HDFS Write: 4345 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.71 sec   HDFS Read: 9491 HDFS Write: 1731 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 6 seconds 60 msec\n",
      "OK\n",
      "Time taken: 36.562 seconds\n",
      "Query ID = root_20220527043012_82339861-3111-4ce0-9d68-48576cc592a7\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1653625133568_0010, Tracking URL = http://10e0f132c3fa:8088/proxy/application_1653625133568_0010/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1653625133568_0010\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2022-05-27 04:30:21,678 Stage-1 map = 0%,  reduce = 0%\n",
      "2022-05-27 04:30:25,783 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.5 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 500 msec\n",
      "Ended Job = job_1653625133568_0010\n",
      "Stage-3 is selected by condition resolver.\n",
      "Stage-2 is filtered out by condition resolver.\n",
      "Stage-4 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/tmp/output/.hive-staging_hive_2022-05-27_04-30-12_746_4329887753979883221-1/-ext-10000\n",
      "Moving data to directory /tmp/output\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 1.5 sec   HDFS Read: 5028 HDFS Write: 1653 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 1 seconds 500 msec\n",
      "OK\n",
      "Time taken: 14.14 seconds\n"
     ]
    }
   ],
   "source": [
    "!hive -f /tmp/wordcount.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive on Tez\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/wordcount.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount.hql\n",
    "\n",
    "set hive.execution.engine = tez;\n",
    "\n",
    "DROP TABLE IF EXISTS docs;\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "\n",
    "CREATE TABLE docs (line STRING);\n",
    "\n",
    "LOAD DATA INPATH \"/tmp/wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "\n",
    "CREATE TABLE word_counts \n",
    "AS\n",
    "    SELECT word, count(1) AS count \n",
    "    FROM\n",
    "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
    "GROUP BY \n",
    "    word\n",
    "ORDER BY \n",
    "    word;\n",
    "    \n",
    "INSERT OVERWRITE DIRECTORY '/tmp/output' \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "SELECT * FROM word_counts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-2.3.9.jar!/hive-log4j2.properties Async: true\n",
      "OK\n",
      "Time taken: 6.397 seconds\n",
      "OK\n",
      "Time taken: 0.049 seconds\n",
      "OK\n",
      "Time taken: 0.412 seconds\n",
      "Loading data to table default.docs\n",
      "OK\n",
      "Time taken: 0.497 seconds\n",
      "Exception in thread \"main\" java.lang.NoClassDefFoundError: org/apache/tez/runtime/api/Event\n",
      "\tat org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsDependentOptimizations(TezCompiler.java:336)\n",
      "\tat org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:104)\n",
      "\tat org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140)\n",
      "\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11273)\n",
      "\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:286)\n",
      "\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:512)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\n",
      "\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:474)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:490)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:244)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:158)\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.tez.runtime.api.Event\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\t... 26 more\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /tmp/wordcount\n",
    "!hdfs dfs -copyFromLocal /tmp/wordcount/*  /tmp/wordcount/\n",
    "!hive -f /tmp/wordcount.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxrwx---   - root supergroup          0 2022-05-27 04:19 /tmp/hadoop-yarn\n",
      "drwxrwxrwx   - root supergroup          0 2022-05-27 04:19 /tmp/hive\n",
      "drwxrwxrwx   - root supergroup          0 2022-05-27 04:30 /tmp/output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive on Spark\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/wordcount.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount.hql\n",
    "\n",
    "set hive.execution.engine = spark;\n",
    "\n",
    "DROP TABLE IF EXISTS docs;\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "\n",
    "CREATE TABLE docs (line STRING);\n",
    "\n",
    "LOAD DATA INPATH \"/tmp/wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "\n",
    "CREATE TABLE word_counts \n",
    "AS\n",
    "    SELECT word, count(1) AS count \n",
    "    FROM\n",
    "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
    "GROUP BY \n",
    "    word\n",
    "ORDER BY \n",
    "    word;\n",
    "    \n",
    "INSERT OVERWRITE DIRECTORY '/tmp/output' \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "SELECT * FROM word_counts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp/wordcount': File exists\n",
      "copyFromLocal: `/tmp/wordcount/text0.txt': File exists\n",
      "copyFromLocal: `/tmp/wordcount/text1.txt': File exists\n",
      "copyFromLocal: `/tmp/wordcount/text2.txt': File exists\n",
      "\n",
      "Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-2.3.9.jar!/hive-log4j2.properties Async: true\n",
      "OK\n",
      "Time taken: 6.46 seconds\n",
      "OK\n",
      "Time taken: 0.05 seconds\n",
      "OK\n",
      "Time taken: 0.42 seconds\n",
      "Loading data to table default.docs\n",
      "OK\n",
      "Time taken: 0.513 seconds\n",
      "Query ID = root_20220527044041_9f5e46a6-f837-42de-9047-ec329e89722a\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Spark Job = c2fec351-64c0-4fbc-85f5-ee9f8946b148\n",
      "Job failed with java.lang.NoSuchMethodError: org.apache.spark.api.java.JavaSparkContext.accumulator(Ljava/lang/Object;Ljava/lang/String;Lorg/apache/spark/AccumulatorParam;)Lorg/apache/spark/Accumulator;\n",
      "\tat org.apache.hive.spark.counter.SparkCounter.<init>(SparkCounter.java:60)\n",
      "\tat org.apache.hive.spark.counter.SparkCounterGroup.createCounter(SparkCounterGroup.java:52)\n",
      "\tat org.apache.hive.spark.counter.SparkCounters.createCounter(SparkCounters.java:71)\n",
      "\tat org.apache.hive.spark.counter.SparkCounters.createCounter(SparkCounters.java:67)\n",
      "\tat org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient$JobStatusJob.call(RemoteHiveSparkClient.java:337)\n",
      "\tat org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:358)\n",
      "\tat org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:323)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. java.lang.NoSuchMethodError: org.apache.spark.api.java.JavaSparkContext.accumulator(Ljava/lang/Object;Ljava/lang/String;Lorg/apache/spark/AccumulatorParam;)Lorg/apache/spark/Accumulator;\n",
      "\tat org.apache.hive.spark.counter.SparkCounter.<init>(SparkCounter.java:60)\n",
      "\tat org.apache.hive.spark.counter.SparkCounterGroup.createCounter(SparkCounterGroup.java:52)\n",
      "\tat org.apache.hive.spark.counter.SparkCounters.createCounter(SparkCounters.java:71)\n",
      "\tat org.apache.hive.spark.counter.SparkCounters.createCounter(SparkCounters.java:67)\n",
      "\tat org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient$JobStatusJob.call(RemoteHiveSparkClient.java:337)\n",
      "\tat org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:358)\n",
      "\tat org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:323)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /tmp/wordcount\n",
    "!hdfs dfs -copyFromLocal /tmp/wordcount/*  /tmp/wordcount/\n",
    "!hive -f /tmp/wordcount.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de resultados\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados quedan almacenados en la carpeta `/tmp/output` del sistema HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxrwxrwx   1 root supergroup       1653 2022-05-27 04:25 /tmp/output/000000_0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",20\n",
      "(DA),1\n",
      "(see,1\n",
      "Analytics,2\n",
      "Analytics,,1\n",
      "Big,1\n",
      "Data,3\n",
      "Especially,1\n",
      "Organizations,1\n",
      "Since,1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /tmp/output/000000_0 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copia de los resultados a la máquina local\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/output/000000_0\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal /tmp/output /tmp/output\n",
    "!ls /tmp/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",20\n",
      "(DA),1\n",
      "(see,1\n",
      "Analytics,2\n",
      "Analytics,,1\n",
      "Big,1\n",
      "Data,3\n",
      "Especially,1\n",
      "Organizations,1\n",
      "Since,1\n",
      "Specifically,,1\n",
      "The,2\n",
      "a,1\n",
      "about,1\n",
      "aid,1\n",
      "algorithms,1\n",
      "analysis,,1\n",
      "analysis.,1\n",
      "analytics,8\n",
      "analytics,,8\n",
      "analytics.,1\n",
      "analyze,1\n",
      "and,15\n",
      "application,1\n",
      "apply,1\n",
      "are,1\n",
      "areas,2\n",
      "assortment,1\n",
      "be,1\n",
      "big,1\n",
      "business,4\n",
      "by,2\n",
      "call,1\n",
      "can,2\n",
      "certain,1\n",
      "changes.,1\n",
      "cognitive,1\n",
      "commercial,1\n",
      "communication,1\n",
      "computation,1\n",
      "computer,2\n",
      "conclusions,1\n",
      "contain,,1\n",
      "credit,1\n",
      "current,1\n",
      "data,4\n",
      "data),,1\n",
      "data.,1\n",
      "decision,1\n",
      "decisions,2\n",
      "describe,,1\n",
      "descriptive,1\n",
      "discovery,,1\n",
      "disprove,1\n",
      "draw,1\n",
      "effects,1\n",
      "enable,1\n",
      "enterprise,1\n",
      "evaluate,1\n",
      "events,,1\n",
      "examining,1\n",
      "extensive,1\n",
      "field,1\n",
      "for,1\n",
      "force,1\n",
      "fraud,1\n",
      "gaining,1\n",
      "given,1\n",
      "goal,1\n",
      "harness,1\n",
      "historical,1\n",
      "hypotheses.,1\n",
      "improve,2\n",
      "improvements,1\n",
      "in,5\n",
      "include,1\n",
      "increasingly,1\n",
      "industries,1\n",
      "information,1\n",
      "information,,1\n",
      "interpretation,,1\n",
      "involves,1\n",
      "is,3\n",
      "knowledge,1\n",
      "make,2\n",
      "management,,1\n",
      "marketing,2\n",
      "mathematics.,1\n",
      "may,1\n",
      "meaningful,1\n",
      "methods,1\n",
      "mix,1\n",
      "modeling,,2\n",
      "models,,1\n",
      "more-informed,1\n",
      "most,1\n",
      "of,8\n",
      "often,1\n",
      "on,1\n",
      "operations,1\n",
      "optimization,1\n",
      "optimization,,2\n",
      "or,5\n",
      "order,1\n",
      "organizations,1\n",
      "past,1\n",
      "patterns,1\n",
      "performance,1\n",
      "performance.,2\n",
      "potential,1\n",
      "predict,,1\n",
      "predictive,2\n",
      "prescriptive,1\n",
      "price,1\n",
      "process,1\n",
      "programming,1\n",
      "promotion,1\n",
      "quantify,1\n",
      "recorded,1\n",
      "relies,1\n",
      "require,1\n",
      "research,2\n",
      "researchers,1\n",
      "retail,1\n",
      "rich,1\n",
      "risk,1\n",
      "sales,1\n",
      "scenario.,1\n",
      "science,,2\n",
      "scientific,1\n",
      "scientists,1\n",
      "sets,1\n",
      "simultaneous,1\n",
      "sizing,1\n",
      "software,1\n",
      "software.,1\n",
      "specialized,1\n",
      "speech,1\n",
      "statistics,,2\n",
      "stock-keeping,1\n",
      "store,1\n",
      "studying,1\n",
      "systems,1\n",
      "techniques,1\n",
      "technologies,1\n",
      "the,10\n",
      "theories,1\n",
      "they,1\n",
      "to,12\n",
      "tool,1\n",
      "trends,,1\n",
      "unit,1\n",
      "used,3\n",
      "valuable,1\n",
      "verify,1\n",
      "web,1\n",
      "which,1\n",
      "widely,1\n",
      "with,2\n",
      "within,1\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/output/000000_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción para extraer los resultados es usar\n",
    "\n",
    "      $ hive -S -e 'SELECT * FROM word_counts;' > result.csv\n",
    "     \n",
    "     \n",
    "en donde el archivo `result.txt` se almacena localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf *.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
