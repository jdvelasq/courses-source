{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de palabras en Apache Hive (script)\n",
    "===\n",
    "\n",
    "* Última modificación: Mayo 17, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datos\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/wordcount/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/wordcount/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/wordcount/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versión en productivo\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la segunda parte, se procede a llevar el aplicativo a productivo con los siguientes cambios:\n",
    "\n",
    "* Los datos son leidos del sistema HDFS de Hadoop.\n",
    "\n",
    "* Los resultdos son guardados en una carpeta del sistema Hadoop.\n",
    "\n",
    "* El script se almacena en un archivo en el disco duro, para su uso posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copia de los datos al sistema HDFS\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxrwx---   - root supergroup          0 2022-05-18 05:11 /tmp/hadoop-yarn\n",
      "drwxrwxrwx   - root supergroup          0 2022-05-18 05:12 /tmp/hive\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Se usan un directorio temporal en el HDFS. La siguiente\n",
    "# instrucción muestra el contenido del dicho directorio\n",
    "#\n",
    "!hdfs dfs -ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Crea la carpeta wordcount en el hdfs\n",
    "#\n",
    "!hdfs dfs -mkdir /tmp/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxrwx---   - root supergroup          0 2022-05-18 05:11 /tmp/hadoop-yarn\n",
      "drwxrwxrwx   - root supergroup          0 2022-05-18 05:12 /tmp/hive\n",
      "drwxr-xr-x   - root supergroup          0 2022-05-18 05:13 /tmp/wordcount\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Verifica la creación de la carpeta\n",
    "#\n",
    "!hdfs dfs -ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copia los archvios del directorio local /tmp/wordcount/\n",
    "# al directorio /tmp/wordcount/ en el hdfs\n",
    "#\n",
    "!hdfs dfs -copyFromLocal /tmp/wordcount/*  /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 root supergroup       1093 2022-05-18 05:14 /tmp/wordcount/text0.txt\n",
      "-rw-r--r--   1 root supergroup        352 2022-05-18 05:14 /tmp/wordcount/text1.txt\n",
      "-rw-r--r--   1 root supergroup        440 2022-05-18 05:14 /tmp/wordcount/text2.txt\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Verifica que los archivos esten copiados\n",
    "# en el hdfs\n",
    "#\n",
    "!hdfs dfs -ls /tmp/wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generación del script y ajuste del código\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizan dos cambios. En primer lugar, se sustituye la línea \n",
    "\n",
    "    LOAD DATA LOCAL INPATH \"wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "    \n",
    "por:\n",
    "\n",
    "    LOAD DATA INPATH \"/tmp/wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "\n",
    "para que Hive lea los datos del directorio `/tmp/wordcount/` en el HDFS. En segundo lugar, se agrega\n",
    "\n",
    "    INSERT OVERWRITE DIRECTORY '/tmp/output' \n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "    SELECT * FROM word_counts;\n",
    "    \n",
    "para que los resultados sean almacenados en la carpeta `/tmp/output` como un archivo en formato CSV. El programa es guadado como `wordcount.hql` en el computador local. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/wordcount.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/wordcount.hql\n",
    "\n",
    "DROP TABLE IF EXISTS docs;\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "\n",
    "CREATE TABLE docs (line STRING);\n",
    "\n",
    "LOAD DATA INPATH \"/tmp/wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "\n",
    "CREATE TABLE word_counts \n",
    "AS\n",
    "    SELECT word, count(1) AS count \n",
    "    FROM\n",
    "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
    "GROUP BY \n",
    "    word\n",
    "ORDER BY \n",
    "    word;\n",
    "    \n",
    "INSERT OVERWRITE DIRECTORY '/tmp/output' \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "SELECT * FROM word_counts;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `wordcount.hql': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcount.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "\n",
      "Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-2.3.9.jar!/hive-log4j2.properties Async: true\n",
      "OK\n",
      "Time taken: 6.625 seconds\n",
      "OK\n",
      "Time taken: 0.1 seconds\n",
      "OK\n",
      "Time taken: 0.473 seconds\n",
      "Loading data to table default.docs\n",
      "OK\n",
      "Time taken: 0.518 seconds\n",
      "Query ID = root_20220518051423_eca70e0a-6fb2-4bd3-aa04-df507d760320\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1652850657922_0003, Tracking URL = http://2088d6db398b:8088/proxy/application_1652850657922_0003/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1652850657922_0003\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2022-05-18 05:14:30,653 Stage-1 map = 0%,  reduce = 0%\n",
      "2022-05-18 05:14:34,973 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.9 sec\n",
      "2022-05-18 05:14:39,094 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.21 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 210 msec\n",
      "Ended Job = job_1652850657922_0003\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1652850657922_0004, Tracking URL = http://2088d6db398b:8088/proxy/application_1652850657922_0004/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1652850657922_0004\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2022-05-18 05:14:50,431 Stage-2 map = 0%,  reduce = 0%\n",
      "2022-05-18 05:14:53,583 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.15 sec\n",
      "2022-05-18 05:14:57,705 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.73 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 730 msec\n",
      "Ended Job = job_1652850657922_0004\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/word_counts\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.21 sec   HDFS Read: 10195 HDFS Write: 4345 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.73 sec   HDFS Read: 9498 HDFS Write: 1731 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 5 seconds 940 msec\n",
      "OK\n",
      "Time taken: 35.585 seconds\n",
      "Query ID = root_20220518051458_6fbf9540-f7e3-46c1-9665-cdbe735f2ffd\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1652850657922_0005, Tracking URL = http://2088d6db398b:8088/proxy/application_1652850657922_0005/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1652850657922_0005\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2022-05-18 05:15:08,968 Stage-1 map = 0%,  reduce = 0%\n",
      "2022-05-18 05:15:12,067 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.31 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 310 msec\n",
      "Ended Job = job_1652850657922_0005\n",
      "Stage-3 is selected by condition resolver.\n",
      "Stage-2 is filtered out by condition resolver.\n",
      "Stage-4 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/tmp/output/.hive-staging_hive_2022-05-18_05-14-58_999_6798449333896886518-1/-ext-10000\n",
      "Moving data to directory /tmp/output\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 1.31 sec   HDFS Read: 5028 HDFS Write: 1653 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 1 seconds 310 msec\n",
      "OK\n",
      "Time taken: 15.176 seconds\n"
     ]
    }
   ],
   "source": [
    "!hive -f /tmp/wordcount.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de resultados\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados quedan almacenados en la carpeta `/tmp/output` del sistema HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxrwxrwx   1 root supergroup       1653 2022-05-18 05:15 /tmp/output/000000_0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",20\n",
      "(DA),1\n",
      "(see,1\n",
      "Analytics,2\n",
      "Analytics,,1\n",
      "Big,1\n",
      "Data,3\n",
      "Especially,1\n",
      "Organizations,1\n",
      "Since,1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /tmp/output/000000_0 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copia de los resultados a la máquina local\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'output/*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal /tmp/output /tmp/output\n",
    "!ls output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",20\n",
      "(DA),1\n",
      "(see,1\n",
      "Analytics,2\n",
      "Analytics,,1\n",
      "Big,1\n",
      "Data,3\n",
      "Especially,1\n",
      "Organizations,1\n",
      "Since,1\n",
      "Specifically,,1\n",
      "The,2\n",
      "a,1\n",
      "about,1\n",
      "aid,1\n",
      "algorithms,1\n",
      "analysis,,1\n",
      "analysis.,1\n",
      "analytics,8\n",
      "analytics,,8\n",
      "analytics.,1\n",
      "analyze,1\n",
      "and,15\n",
      "application,1\n",
      "apply,1\n",
      "are,1\n",
      "areas,2\n",
      "assortment,1\n",
      "be,1\n",
      "big,1\n",
      "business,4\n",
      "by,2\n",
      "call,1\n",
      "can,2\n",
      "certain,1\n",
      "changes.,1\n",
      "cognitive,1\n",
      "commercial,1\n",
      "communication,1\n",
      "computation,1\n",
      "computer,2\n",
      "conclusions,1\n",
      "contain,,1\n",
      "credit,1\n",
      "current,1\n",
      "data,4\n",
      "data),,1\n",
      "data.,1\n",
      "decision,1\n",
      "decisions,2\n",
      "describe,,1\n",
      "descriptive,1\n",
      "discovery,,1\n",
      "disprove,1\n",
      "draw,1\n",
      "effects,1\n",
      "enable,1\n",
      "enterprise,1\n",
      "evaluate,1\n",
      "events,,1\n",
      "examining,1\n",
      "extensive,1\n",
      "field,1\n",
      "for,1\n",
      "force,1\n",
      "fraud,1\n",
      "gaining,1\n",
      "given,1\n",
      "goal,1\n",
      "harness,1\n",
      "historical,1\n",
      "hypotheses.,1\n",
      "improve,2\n",
      "improvements,1\n",
      "in,5\n",
      "include,1\n",
      "increasingly,1\n",
      "industries,1\n",
      "information,1\n",
      "information,,1\n",
      "interpretation,,1\n",
      "involves,1\n",
      "is,3\n",
      "knowledge,1\n",
      "make,2\n",
      "management,,1\n",
      "marketing,2\n",
      "mathematics.,1\n",
      "may,1\n",
      "meaningful,1\n",
      "methods,1\n",
      "mix,1\n",
      "modeling,,2\n",
      "models,,1\n",
      "more-informed,1\n",
      "most,1\n",
      "of,8\n",
      "often,1\n",
      "on,1\n",
      "operations,1\n",
      "optimization,1\n",
      "optimization,,2\n",
      "or,5\n",
      "order,1\n",
      "organizations,1\n",
      "past,1\n",
      "patterns,1\n",
      "performance,1\n",
      "performance.,2\n",
      "potential,1\n",
      "predict,,1\n",
      "predictive,2\n",
      "prescriptive,1\n",
      "price,1\n",
      "process,1\n",
      "programming,1\n",
      "promotion,1\n",
      "quantify,1\n",
      "recorded,1\n",
      "relies,1\n",
      "require,1\n",
      "research,2\n",
      "researchers,1\n",
      "retail,1\n",
      "rich,1\n",
      "risk,1\n",
      "sales,1\n",
      "scenario.,1\n",
      "science,,2\n",
      "scientific,1\n",
      "scientists,1\n",
      "sets,1\n",
      "simultaneous,1\n",
      "sizing,1\n",
      "software,1\n",
      "software.,1\n",
      "specialized,1\n",
      "speech,1\n",
      "statistics,,2\n",
      "stock-keeping,1\n",
      "store,1\n",
      "studying,1\n",
      "systems,1\n",
      "techniques,1\n",
      "technologies,1\n",
      "the,10\n",
      "theories,1\n",
      "they,1\n",
      "to,12\n",
      "tool,1\n",
      "trends,,1\n",
      "unit,1\n",
      "used,3\n",
      "valuable,1\n",
      "verify,1\n",
      "web,1\n",
      "which,1\n",
      "widely,1\n",
      "with,2\n",
      "within,1\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/output/000000_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción para extraer los resultados es usar\n",
    "\n",
    "      $ hive -S -e 'SELECT * FROM word_counts;' > result.csv\n",
    "     \n",
    "     \n",
    "en donde el archivo `result.txt` se almacena localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf *.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
