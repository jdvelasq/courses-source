{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronóstico de la popularidad de libros (MLlib: RDD-based)\n",
    "===\n",
    "\n",
    "* *30 min* | Última modificación: Noviembre 6, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición del problema\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La editorial O'Really desea construir una herramienta analítica que le permita a un editor estimar la popularidad relativa de un nuevo libro antes de su lanzamiento, con el fin de poder priorizar los títulos a publicar e inclusive rechazar posibles proyectos editoriales. Para resolver este problema se tiene una base de datos con los 100 libros más vendidos por O'Really durante el año 2011. La base contiene el título del libro, su descripción y su ranking en pupularidad. Para este caso se hipotetiza que la aparición de ciertas palabras en la descripción del libro permitirá determinar su popularidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación del archivo de datos\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-06 23:43:03--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/oreilly.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203329 (199K) [text/plain]\n",
      "Saving to: ‘oreilly.csv’\n",
      "\n",
      "oreilly.csv         100%[===================>] 198.56K   173KB/s    in 1.1s    \n",
      "\n",
      "2020-11-06 23:43:06 (173 KB/s) - ‘oreilly.csv’ saved [203329/203329]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/oreilly.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización de Spark\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Carga de las librerías de Spark\n",
    "#\n",
    "import findspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "APP_NAME = \"spark-app\"\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de datos\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Este archivo resulta particularmente difícil de\n",
    "# leer en Spark, por lo que se lee usando Pandas\n",
    "# para luego cargarlo en Spark.\n",
    "#\n",
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.read_csv(\n",
    "    \"oreilly.csv\", sep=\",\", thousands=None, decimal=\".\", encoding=\"latin-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(IP_Family='9780596000271.IP', BOOK_title='Programming Perl, 3E', BOOK_ISBN='9780596000271', Rank=1, Long_Desc='Perl is a powerful programming language that  has grown in popularity since it first appeared in 1988. The first edition of this book, <i>Programming Perl,</i> hit the shelves in 1990, and was quickly adopted as the  undisputed bible of the language. Since then, Perl  has grown with the times, and so has this book.\\r\\r\\r\\r<i>Programming Perl</i> is not just a book about Perl. It is  also a unique introduction to the language and its culture,  as one might expect only from its authors. Larry Wall is the inventor of Perl, and provides a unique perspective on  the evolution of Perl and its future direction. Tom Christiansen was one of the first champions of the language,  and lives and breathes the complexities of Perl internals  as few other mortals do. Jon Orwant is the editor of \\r\\r<i>The Perl Journal,</i> which has brought together the Perl  community as a common forum for new developments in Perl.\\r\\r\\r\\rAny Perl book can show the syntax of Perl\\'s functions, but only this one is a comprehensive guide to all the nooks and crannies of the language. Any Perl book can explain typeglobs, pseudohashes, and closures, but only this one shows  how they really work. Any Perl book can say that <i>my</i> is faster than <i>local,</i> but only this one explains why. Any Perl book  can have a title, but only this book is affectionately known by all Perl programmers as \"The Camel.\"  \\r\\r\\r\\rThis third edition of <i>Programming Perl</i> has been expanded to cover version 5.6 of this maturing language. New topics include threading, the compiler, Unicode, and other  new features that have been added since the previous edition.'),\n",
       " Row(IP_Family='9781565923928.IP', BOOK_title='Javascript: The Definitive Guide, 3E', BOOK_ISBN='9781565923928', Rank=2, Long_Desc=\"JavaScript is a powerful scripting language that can be embedded directly in HTML. It allows you to create dynamic, interactive Web-based applications that run completely within a Web browser; you don't have to do any server-side programming, like writing CGI scripts.  \\r\\r\\r\\rJavaScript is a simpler language than Java. It can be embedded directly in Web pages without compilation, so it is more flexible and easier to use for simple tasks like animation. However, although you can write reasonably robust and complete Web applications using JavaScript alone, JavaScript is not a substitute for Java. In fact, JavaScript is a good client-side complement to Java; using the two together allows you to create more complex applications than are possible with JavaScript alone.\\r\\r\\r\\r<i>JavaScript: The Definitive Guide</i> provides a thorough description of the core JavaScript language and its client-side framework, complete with sophisticated examples that show you how to handle common tasks, like validating form data and working with cookies. The book also contains a definitive, in-depth reference section that covers every core and client-side JavaScript function, object, method, property, constructor, and event handler. This book is an indispensable reference for all JavaScript programmers, regardless of experience level.\\r\\r\\r\\rThis third edition of <i>JavaScript: The Definitive Guide</i> describes the latest version of the language, JavaScript 1.2, as supported by Netscape Navigator 4 and Internet Explorer 4. The book also covers JavaScript 1.1, which is the first industry-standard version known as ECMAScript. The new features of JavaScript 1.2, which are likely to be embodied in a later ECMAScript standard release, are clearly indicated, so that you can use them as appropriate in your scripts.\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Se crea el esquema de la tabla en Spark \n",
    "#\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "mySchema = StructType([ \n",
    "    StructField(\"IP_Family\", StringType(), True),\\\n",
    "    StructField(\"BOOK_title\", StringType(), True),\\\n",
    "    StructField(\"BOOK_ISBN\", StringType(), True),\\\n",
    "    StructField(\"Rank\", IntegerType(), True),\\\n",
    "    StructField(\"Long_Desc\", StringType(), True)])\n",
    "\n",
    "#\n",
    "# Se crea el DataFrame de Spark a partir \n",
    "# del DataFrame de Spark\n",
    "#\n",
    "rdd = spark.createDataFrame(pandas_df, schema=mySchema).rdd\n",
    "\n",
    "#\n",
    "# Primeros dos registros\n",
    "#\n",
    "rdd.collect()[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación del texto\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Selecciona las columnas 3 (rank) y 4 (Long_Desc)\n",
    "#\n",
    "rdd_rank = rdd.map(lambda w: w[3])\n",
    "rdd_text = rdd.map(lambda w: w[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Descarga NLTK\n",
    "#\n",
    "!pip3 install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Crea un consecutivo de 1's y 0's para indicar\n",
    "# si el libro fue exitoso o no. Esta será la variable\n",
    "# de salida del modelo de regresión logística\n",
    "#\n",
    "rdd_rank = rdd_rank.map(lambda w: 1 if w >= 50 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Procesa la descripción del libro\n",
    "#\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#\n",
    "# Divide el texto por palabras\n",
    "#\n",
    "rdd_text = rdd_text.map(lambda w: word_tokenize(w))\n",
    "\n",
    "#\n",
    "# Selecciona las palabras que están conformadas\n",
    "# únicamente por letras\n",
    "#\n",
    "import re\n",
    "\n",
    "rdd_text = rdd_text.map(lambda w: [re.sub(r'[^A-Za-z]', '', word) for word in w])\n",
    "rdd_text = rdd_text.map(lambda w: [word for word in w if word != ''])\n",
    "\n",
    "#\n",
    "# Transforma el texto a minusculas\n",
    "#\n",
    "rdd_text = rdd_text.map(lambda w: [word.lower() for word in w])\n",
    "\n",
    "#\n",
    "# Elimina las stopwords\n",
    "#\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "rdd_text = rdd_text.map(lambda w: [word for word in w if word not in STOPWORDS])\n",
    "\n",
    "#\n",
    "# Reduce las palabras a su raíz\n",
    "#\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "rdd_text = rdd_text.map(lambda w: [ps.stem(word) for word in w])\n",
    "\n",
    "#\n",
    "# Construye la matriz de documento-termino\n",
    "#\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(rdd_text)\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF(minDocFreq=2).fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "tfidf.collect()[0]\n",
    "\n",
    "#\n",
    "# Construye el RDD para entrenamiento del modelo. \n",
    "# Une los dos RDD (rank, description)\n",
    "#\n",
    "rdd_LR = rdd_rank.zip(tfidf)\n",
    "\n",
    "#\n",
    "# Etiqueta los puntos para el modelo de regresión\n",
    "#\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "rdd_LR = rdd_LR.map(lambda w: LabeledPoint(w[0], w[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresión Logística\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.01\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Especificación y entrenamiento del modelo de regresión logística\n",
    "#\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(\n",
    "    data=rdd_LR,\n",
    "    regParam=0.0,\n",
    "    regType='l2',\n",
    "    intercept=False,\n",
    "    numClasses=2,\n",
    ")\n",
    "\n",
    "#\n",
    "# Evaluación del modelo.\n",
    "#   \n",
    "labelsAndPreds = rdd_LR.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(rdd_LR.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n",
    "\n",
    "# rdd_rank.collect()[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear support vector machines\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "model = SVMWithSGD.train(rdd_LR, iterations=100)\n",
    "\n",
    "labelsAndPreds = rdd_LR.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(rdd_LR.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "model = DecisionTree.trainClassifier(\n",
    "    rdd_LR,\n",
    "    numClasses=2,\n",
    "    categoricalFeaturesInfo={},\n",
    "    impurity=\"gini\",\n",
    "    maxDepth=3,\n",
    "    maxBins=5,\n",
    "#    minInstancesPerNode=1,\n",
    "#    minInfoGain=0.0,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsAndPreds = rdd_LR.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(\n",
    "    rdd_LR.count()\n",
    ")\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nteract": {
   "version": "0.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
