{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronóstico de la popularidad de libros\n",
    "===\n",
    "\n",
    "* Última modificación: Junio 22, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se presentan varios métodos de clasificación usando PySpark, aplicados a un problema de minería de texto. Al finalizar el estudio de este documento, el lector estará en capacidad de:\n",
    "\n",
    "* Aplicar técnicas de preparación de texto en Spark.\n",
    "\n",
    "* Usar modelos de regresión logística, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición del problema\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La editorial O'Really desea construir una herramienta analítica que le permita a un editor estimar la popularidad relativa de un nuevo libro antes de su lanzamiento, con el fin de poder priorizar los títulos a publicar e inclusive rechazar posibles proyectos editoriales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver este problema se tiene una base de datos con los 100 libros más vendidos por O'Really durante el año 2011. La base contiene el título del libro, su descripción y su ranking en pupularidad. Para este caso se hipotetiza que la aparición de ciertas palabras en la descripción del libro permitirá determinar su popularidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solución\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Carga de las librerías de Spark\n",
    "##\n",
    "import findspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "APP_NAME = \"spark-logreg-app\"\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME) \n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploración\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-01 03:09:34--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/oreilly.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203329 (199K) [text/plain]\n",
      "Saving to: 'oreilly.csv.1'\n",
      "\n",
      "oreilly.csv.1       100%[===================>] 198.56K   852KB/s    in 0.2s    \n",
      "\n",
      "2020-11-01 03:09:34 (852 KB/s) - 'oreilly.csv.1' saved [203329/203329]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/oreilly.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "##\n",
    "## Este archivo resulta particularmente difícil de \n",
    "## leer en Spark, por lo que se lee usando Pandas\n",
    "## para luego cargarlo en Spark.\n",
    "##\n",
    "pdDF = pd.read_csv(\n",
    "    \"oreilly.csv\",\n",
    "    sep = ',',           # separador de campos\n",
    "    thousands = None,    # separador de miles para números\n",
    "    decimal = '.',       # separador de los decimales para números\n",
    "    encoding='latin-1')  # idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Se crea el esquema de la tabla en Spark \n",
    "##\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "mySchema = StructType([ \n",
    "    StructField(\"IP_Family\", StringType(), True),\\\n",
    "    StructField(\"BOOK_title\", StringType(), True),\\\n",
    "    StructField(\"BOOK_ISBN\", StringType(), True),\\\n",
    "    StructField(\"Rank\", IntegerType(), True),\\\n",
    "    StructField(\"Long_Desc\", StringType(), True)])\n",
    "\n",
    "##\n",
    "## Se crea el DataFrame de Spark a partir \n",
    "## del DataFrame de Spark\n",
    "##\n",
    "df = spark.createDataFrame(pdDF, schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IP_Family: string (nullable = true)\n",
      " |-- BOOK_title: string (nullable = true)\n",
      " |-- BOOK_ISBN: string (nullable = true)\n",
      " |-- Rank: integer (nullable = true)\n",
      " |-- Long_Desc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se verifican los tipos de los \n",
    "## campos del DataFrame\n",
    "##\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construcción de los modelos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación del texto en variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           Long_Desc|               words|\n",
      "+--------------------+--------------------+\n",
      "|Perl is a powerfu...|[perl, is, a, pow...|\n",
      "|JavaScript is a p...|[javascript, is, ...|\n",
      "|You're not alone....|[you're, not, alo...|\n",
      "|Learning a comple...|[learning, a, com...|\n",
      "|With Leopard, App...|[with, leopard,, ...|\n",
      "|Tired of reading ...|[tired, of, readi...|\n",
      "|This bestselling ...|[this, bestsellin...|\n",
      "|You may have seen...|[you, may, have, ...|\n",
      "|You can set your ...|[you, can, set, y...|\n",
      "|Once a little-kno...|[once, a, little-...|\n",
      "|JavaScript is a p...|[javascript, is, ...|\n",
      "|<i>Web Design in ...|[<i>web, design, ...|\n",
      "|New York Times co...|[new, york, times...|\n",
      "|In this update of...|[in, this, update...|\n",
      "|The <i>Perl Cookb...|[the, <i>perl, co...|\n",
      "|For a company tha...|[for, a, company,...|\n",
      "|If you are a Web ...|[if, you, are, a,...|\n",
      "|Apple says that M...|[apple, says, tha...|\n",
      "|<p>This Fifth Edi...|[<p>this, fifth, ...|\n",
      "|If you ask Perl p...|[if, you, ask, pe...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "##\n",
    "## Se usa el tokenizer para separar las palabras. Cada \n",
    "## elemento de la columna words es una lista con las \n",
    "## palabras que conforman el texto\n",
    "##\n",
    "tokenizer = Tokenizer(inputCol=\"Long_Desc\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "df.select('Long_Desc', 'words').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               words|            filtered|\n",
      "+--------------------+--------------------+\n",
      "|[perl, is, a, pow...|[perl, powerful, ...|\n",
      "|[javascript, is, ...|[javascript, powe...|\n",
      "|[you're, not, alo...|[alone.<br, />, <...|\n",
      "|[learning, a, com...|[learning, comple...|\n",
      "|[with, leopard,, ...|[leopard,, apple,...|\n",
      "|[tired, of, readi...|[tired, reading, ...|\n",
      "|[this, bestsellin...|[bestselling, qui...|\n",
      "|[you, may, have, ...|[may, seen, unix,...|\n",
      "|[you, can, set, y...|[set, watch, it:,...|\n",
      "|[once, a, little-...|[little-known, pr...|\n",
      "|[javascript, is, ...|[javascript, powe...|\n",
      "|[<i>web, design, ...|[<i>web, design, ...|\n",
      "|[new, york, times...|[new, york, times...|\n",
      "|[in, this, update...|[update, bestsell...|\n",
      "|[the, <i>perl, co...|[<i>perl, cookboo...|\n",
      "|[for, a, company,...|[company, promise...|\n",
      "|[if, you, are, a,...|[web, content, de...|\n",
      "|[apple, says, tha...|[apple, says, mac...|\n",
      "|[<p>this, fifth, ...|[<p>this, fifth, ...|\n",
      "|[if, you, ask, pe...|[ask, perl, progr...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "##\n",
    "## Se procede a remover las stop words del texto\n",
    "##\n",
    "df = StopWordsRemover(inputCol=\"words\", \n",
    "                      outputCol=\"filtered\").transform(df)\n",
    "df.select('words', 'filtered').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            filtered|         rawFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[perl, powerful, ...|(50,[0,1,2,3,4,5,...|\n",
      "|[javascript, powe...|(50,[0,2,4,5,6,7,...|\n",
      "|[alone.<br, />, <...|(50,[0,1,2,3,5,6,...|\n",
      "|[learning, comple...|(50,[0,1,2,3,4,5,...|\n",
      "|[leopard,, apple,...|(50,[0,1,2,3,4,5,...|\n",
      "|[tired, reading, ...|(50,[0,1,2,3,4,5,...|\n",
      "|[bestselling, qui...|(50,[0,1,2,3,4,5,...|\n",
      "|[may, seen, unix,...|(50,[0,1,2,3,4,5,...|\n",
      "|[set, watch, it:,...|(50,[0,1,2,3,4,5,...|\n",
      "|[little-known, pr...|(50,[0,1,2,3,4,5,...|\n",
      "|[javascript, powe...|(50,[0,2,3,4,5,6,...|\n",
      "|[<i>web, design, ...|(50,[1,2,3,4,5,7,...|\n",
      "|[new, york, times...|(50,[0,1,2,3,4,5,...|\n",
      "|[update, bestsell...|(50,[0,1,2,3,5,6,...|\n",
      "|[<i>perl, cookboo...|(50,[0,1,2,3,4,5,...|\n",
      "|[company, promise...|(50,[0,1,2,3,4,5,...|\n",
      "|[web, content, de...|(50,[0,1,2,3,4,5,...|\n",
      "|[apple, says, mac...|(50,[0,1,2,3,4,5,...|\n",
      "|[<p>this, fifth, ...|(50,[0,2,3,4,5,6,...|\n",
      "|[ask, perl, progr...|(50,[0,2,3,5,6,7,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Una vez se han removido las stopwords, se \n",
    "## procede a transformar el texto en bag-of-words\n",
    "##\n",
    "hashingTF = HashingTF(\n",
    "    inputCol=\"filtered\", \n",
    "    outputCol=\"rawFeatures\", \n",
    "    numFeatures=50)\n",
    "\n",
    "df = hashingTF.transform(df)\n",
    "\n",
    "df.select(['filtered', 'rawFeatures']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            filtered|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[perl, powerful, ...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[javascript, powe...|(50,[0,2,4,5,6,7,...|(50,[0,2,4,5,6,7,...|\n",
      "|[alone.<br, />, <...|(50,[0,1,2,3,5,6,...|(50,[0,1,2,3,5,6,...|\n",
      "|[learning, comple...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[leopard,, apple,...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[tired, reading, ...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[bestselling, qui...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[may, seen, unix,...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[set, watch, it:,...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[little-known, pr...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[javascript, powe...|(50,[0,2,3,4,5,6,...|(50,[0,2,3,4,5,6,...|\n",
      "|[<i>web, design, ...|(50,[1,2,3,4,5,7,...|(50,[1,2,3,4,5,7,...|\n",
      "|[new, york, times...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[update, bestsell...|(50,[0,1,2,3,5,6,...|(50,[0,1,2,3,5,6,...|\n",
      "|[<i>perl, cookboo...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[company, promise...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[web, content, de...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[apple, says, mac...|(50,[0,1,2,3,4,5,...|(50,[0,1,2,3,4,5,...|\n",
      "|[<p>this, fifth, ...|(50,[0,2,3,4,5,6,...|(50,[0,2,3,4,5,6,...|\n",
      "|[ask, perl, progr...|(50,[0,2,3,5,6,7,...|(50,[0,2,3,5,6,7,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Luego se procede a reescalar los valores\n",
    "## usando la función IDF para mejorar el \n",
    "## desempeño del modelo\n",
    "##\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", \n",
    "          outputCol=\"features\")\n",
    "\n",
    "idfModel = idf.fit(df)\n",
    "\n",
    "df = idfModel.transform(df)\n",
    "\n",
    "df.select(\"filtered\", \"rawFeatures\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construcción de la variable de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se procede a construir una variable binaria de salida para indicar el ranking de los libros. Si la variable vale 1, el libro pertence a los primeros 50, y 0 en caso contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Rank|\n",
      "+----+\n",
      "|   1|\n",
      "|   2|\n",
      "|   3|\n",
      "|   4|\n",
      "|   5|\n",
      "|   6|\n",
      "|   7|\n",
      "|   8|\n",
      "|   9|\n",
      "|  10|\n",
      "|  11|\n",
      "|  12|\n",
      "|  13|\n",
      "|  14|\n",
      "|  15|\n",
      "|  16|\n",
      "|  17|\n",
      "|  18|\n",
      "|  19|\n",
      "|  20|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import when \n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "df = df.withColumn(\n",
    "    'label', \n",
    "    when(df['Rank'] >= 50, lit(0).cast(DoubleType())).otherwise(lit(1).cast(DoubleType())))\n",
    "\n",
    "df.select('Rank').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se construye un modelo de regresión logística que pronóstica si un libro pertence al grupo de los primeros 50 o no, con base en las palabras del abstract y que ya fueron representadas como un bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importa la librería\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "## Parametriza el modelo\n",
    "lr = LogisticRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    rawPredictionCol='rawLR',\n",
    "    probabilityCol='probLR',\n",
    "    predictionCol='LR',\n",
    "    maxIter=1000, \n",
    "    regParam=0.1, \n",
    "    elasticNetParam=0.8)\n",
    "\n",
    "## Entrena el modelo\n",
    "model = lr.fit(df)\n",
    "\n",
    "## Pronostico\n",
    "df = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arbol de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(\n",
    "    labelCol=\"label\", \n",
    "    featuresCol=\"features\",\n",
    "    predictionCol='DTC',\n",
    "    rawPredictionCol='rawDTC',\n",
    "    probabilityCol='probDTC')\n",
    "\n",
    "model = dt.fit(df)\n",
    "\n",
    "df = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\", \n",
    "    featuresCol=\"features\", \n",
    "    predictionCol='RFC',\n",
    "    rawPredictionCol='rawRFC',\n",
    "    probabilityCol='probRFC',\n",
    "    numTrees=10)\n",
    "\n",
    "model = rf.fit(df)\n",
    "\n",
    "df = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumen de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+---+\n",
      "|label| LR|DTC|RFC|\n",
      "+-----+---+---+---+\n",
      "|  1.0|0.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|0.0|1.0|1.0|\n",
      "|  1.0|0.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|0.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|0.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "|  1.0|1.0|1.0|1.0|\n",
      "+-----+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Pronósticos para los modelos\n",
    "##\n",
    "df.select(['label', 'LR', 'DTC', 'RFC']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "  areaUnderROC : 0.6749999999999999\n",
      "  areaUnderPR  : 0.5457142857142857\n",
      "\n",
      "DTC\n",
      "  areaUnderROC : 0.9102564102564101\n",
      "  areaUnderPR  : 0.8805442176870748\n",
      "\n",
      "RFC\n",
      "  areaUnderROC : 0.9803921568627452\n",
      "  areaUnderPR  : 0.9903921568627451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Métricas de desempeño\n",
    "##\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "for m in ['LR', 'DTC', 'RFC']:\n",
    "\n",
    "    data = df.select(['label', m]).rdd.map(lambda x: (x[0], x[1]))\n",
    "    metrics = BinaryClassificationMetrics(data)\n",
    "    print(m)\n",
    "    print('  areaUnderROC :', metrics.areaUnderROC)\n",
    "    print('  areaUnderPR  :', metrics.areaUnderPR)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nteract": {
   "version": "0.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
