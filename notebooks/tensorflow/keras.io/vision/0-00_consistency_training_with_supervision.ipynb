{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Consistency Training with Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install -q tf-models-official tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from official.vision.image_classification.augment import RandAugment\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "\n",
    "CROP_TO = 72\n",
    "RESIZE_TO = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "val_samples = 49500\n",
    "new_train_x, new_y_train = x_train[: val_samples + 1], y_train[: val_samples + 1]\n",
    "val_x, val_y = x_train[val_samples:], y_train[val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Initialize `RandAugment` object with 2 layers of\n",
    "# augmentation transforms and strength of 9.\n",
    "augmenter = RandAugment(num_layers=2, magnitude=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def preprocess_train(image, label, noisy=True):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    # We first resize the original image to a larger dimension\n",
    "    # and then we take random crops from it.\n",
    "    image = tf.image.resize(image, [RESIZE_TO, RESIZE_TO])\n",
    "    image = tf.image.random_crop(image, [CROP_TO, CROP_TO, 3])\n",
    "    if noisy:\n",
    "        image = augmenter.distort(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def preprocess_test(image, label):\n",
    "    image = tf.image.resize(image, [CROP_TO, CROP_TO])\n",
    "    return image, label\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((new_train_x, new_y_train))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# This dataset will be used to train the first model.\n",
    "train_clean_ds = (\n",
    "    train_ds.shuffle(BATCH_SIZE * 10, seed=42)\n",
    "    .map(lambda x, y: (preprocess_train(x, y, noisy=False)), num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# This prepares the `Dataset` object to use RandAugment.\n",
    "train_noisy_ds = (\n",
    "    train_ds.shuffle(BATCH_SIZE * 10, seed=42)\n",
    "    .map(preprocess_train, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "validation_ds = (\n",
    "    validation_ds.map(preprocess_test, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    test_ds.map(preprocess_test, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# This dataset will be used to train the second model.\n",
    "consistency_training_ds = tf.data.Dataset.zip((train_clean_ds, train_noisy_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "sample_images, sample_labels = next(iter(train_clean_ds))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image in enumerate(sample_images[:9]):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image.numpy().astype(\"int\"))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "sample_images, sample_labels = next(iter(train_noisy_ds))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, image in enumerate(sample_images[:9]):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image.numpy().astype(\"int\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def get_training_model(num_classes=10):\n",
    "    resnet50_v2 = tf.keras.applications.ResNet50V2(\n",
    "        weights=None,\n",
    "        include_top=False,\n",
    "        input_shape=(CROP_TO, CROP_TO, 3),\n",
    "    )\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.Input((CROP_TO, CROP_TO, 3)),\n",
    "            layers.experimental.preprocessing.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "            resnet50_v2,\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(num_classes),\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "initial_teacher_model = get_training_model()\n",
    "initial_teacher_model.save_weights(\"initial_teacher_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Define the callbacks.\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Initialize SWA from tf-hub.\n",
    "SWA = tfa.optimizers.SWA\n",
    "\n",
    "# Compile and train the teacher model.\n",
    "teacher_model = get_training_model()\n",
    "teacher_model.load_weights(\"initial_teacher_model.h5\")\n",
    "teacher_model.compile(\n",
    "    # Notice that we are wrapping our optimizer within SWA\n",
    "    optimizer=SWA(tf.keras.optimizers.Adam()),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "history = teacher_model.fit(\n",
    "    train_clean_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_ds,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    ")\n",
    "\n",
    "# Evaluate the teacher model on the test set.\n",
    "_, acc = teacher_model.evaluate(test_ds, verbose=0)\n",
    "print(f\"Test accuracy: {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Majority of the code is taken from:\n",
    "# https://keras.io/examples/vision/knowledge_distillation/\n",
    "class SelfTrainer(tf.keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(SelfTrainer, self).__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        super(SelfTrainer, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Since our dataset is a zip of two independent datasets,\n",
    "        # after initially parsing them, we segregate the\n",
    "        # respective images and labels next.\n",
    "        clean_ds, noisy_ds = data\n",
    "        clean_images, _ = clean_ds\n",
    "        noisy_images, y = noisy_ds\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(clean_images, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(noisy_images, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            total_loss = (student_loss + distillation_loss) / 2\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(total_loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`\n",
    "        self.compiled_metrics.update_state(\n",
    "            y, tf.nn.softmax(student_predictions, axis=1)\n",
    "        )\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"total_loss\": total_loss})\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # During inference, we only pass a dataset consisting images and labels.\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Update the metrics\n",
    "        self.compiled_metrics.update_state(y, tf.nn.softmax(y_prediction, axis=1))\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Define the callbacks.\n",
    "# We are using a larger decay factor to stabilize the training.\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    patience=3, factor=0.5, monitor=\"val_accuracy\"\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=10, restore_best_weights=True, monitor=\"val_accuracy\"\n",
    ")\n",
    "\n",
    "# Compile and train the student model.\n",
    "self_trainer = SelfTrainer(student=get_training_model(), teacher=teacher_model)\n",
    "self_trainer.compile(\n",
    "    # Notice we are *not* using SWA here.\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    student_loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=tf.keras.losses.KLDivergence(),\n",
    "    temperature=10,\n",
    ")\n",
    "history = self_trainer.fit(\n",
    "    consistency_training_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_ds,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    ")\n",
    "\n",
    "# Evaluate the student model.\n",
    "acc = self_trainer.evaluate(test_ds, verbose=0)\n",
    "print(f\"Test accuracy from student model: {acc*100}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "consistency_training",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
