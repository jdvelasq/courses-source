{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Self-supervised contrastive learning with SimSiam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "CROP_TO = 32\n",
    "SEED = 26\n",
    "\n",
    "PROJECT_DIM = 2048\n",
    "LATENT_DIM = 512\n",
    "WEIGHT_DECAY = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(f\"Total training examples: {len(x_train)}\")\n",
    "print(f\"Total test examples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def flip_random_crop(image):\n",
    "    # With random crops we also apply horizontal flipping.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_crop(image, (CROP_TO, CROP_TO, 3))\n",
    "    return image\n",
    "\n",
    "\n",
    "def color_jitter(x, strength=[0.4, 0.4, 0.4, 0.1]):\n",
    "    x = tf.image.random_brightness(x, max_delta=0.8 * strength[0])\n",
    "    x = tf.image.random_contrast(\n",
    "        x, lower=1 - 0.8 * strength[1], upper=1 + 0.8 * strength[1]\n",
    "    )\n",
    "    x = tf.image.random_saturation(\n",
    "        x, lower=1 - 0.8 * strength[2], upper=1 + 0.8 * strength[2]\n",
    "    )\n",
    "    x = tf.image.random_hue(x, max_delta=0.2 * strength[3])\n",
    "    # Affine transformations can disturb the natural range of\n",
    "    # RGB images, hence this is needed.\n",
    "    x = tf.clip_by_value(x, 0, 255)\n",
    "    return x\n",
    "\n",
    "\n",
    "def color_drop(x):\n",
    "    x = tf.image.rgb_to_grayscale(x)\n",
    "    x = tf.tile(x, [1, 1, 3])\n",
    "    return x\n",
    "\n",
    "\n",
    "def random_apply(func, x, p):\n",
    "    if tf.random.uniform([], minval=0, maxval=1) < p:\n",
    "        return func(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def custom_augment(image):\n",
    "    # As discussed in the SimCLR paper, the series of augmentation\n",
    "    # transformations (except for random crops) need to be applied\n",
    "    # randomly to impose translational invariance.\n",
    "    image = flip_random_crop(image)\n",
    "    image = random_apply(color_jitter, image, p=0.8)\n",
    "    image = random_apply(color_drop, image, p=0.2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "ssl_ds_one = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "ssl_ds_one = (\n",
    "    ssl_ds_one.shuffle(1024, seed=SEED)\n",
    "    .map(custom_augment, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "ssl_ds_two = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "ssl_ds_two = (\n",
    "    ssl_ds_two.shuffle(1024, seed=SEED)\n",
    "    .map(custom_augment, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "# We then zip both of these datasets.\n",
    "ssl_ds = tf.data.Dataset.zip((ssl_ds_one, ssl_ds_two))\n",
    "\n",
    "# Visualize a few augmented images.\n",
    "sample_images_one = next(iter(ssl_ds_one))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for n in range(25):\n",
    "    ax = plt.subplot(5, 5, n + 1)\n",
    "    plt.imshow(sample_images_one[n].numpy().astype(\"int\"))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Ensure that the different versions of the dataset actually contain\n",
    "# identical images.\n",
    "sample_images_two = next(iter(ssl_ds_two))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for n in range(25):\n",
    "    ax = plt.subplot(5, 5, n + 1)\n",
    "    plt.imshow(sample_images_two[n].numpy().astype(\"int\"))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!wget -q https://git.io/JYx2x -O resnet_cifar10_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import resnet_cifar10_v2\n",
    "\n",
    "N = 2\n",
    "DEPTH = N * 9 + 2\n",
    "NUM_BLOCKS = ((DEPTH - 2) // 9) - 1\n",
    "\n",
    "\n",
    "def get_encoder():\n",
    "    # Input and backbone.\n",
    "    inputs = layers.Input((CROP_TO, CROP_TO, 3))\n",
    "    x = layers.experimental.preprocessing.Rescaling(scale=1.0 / 127.5, offset=-1)(\n",
    "        inputs\n",
    "    )\n",
    "    x = resnet_cifar10_v2.stem(x)\n",
    "    x = resnet_cifar10_v2.learner(x, NUM_BLOCKS)\n",
    "    x = layers.GlobalAveragePooling2D(name=\"backbone_pool\")(x)\n",
    "\n",
    "    # Projection head.\n",
    "    x = layers.Dense(\n",
    "        PROJECT_DIM, use_bias=False, kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dense(\n",
    "        PROJECT_DIM, use_bias=False, kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n",
    "    )(x)\n",
    "    outputs = layers.BatchNormalization()(x)\n",
    "    return tf.keras.Model(inputs, outputs, name=\"encoder\")\n",
    "\n",
    "\n",
    "def get_predictor():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Note the AutoEncoder-like structure.\n",
    "            layers.Input((PROJECT_DIM,)),\n",
    "            layers.Dense(\n",
    "                LATENT_DIM,\n",
    "                use_bias=False,\n",
    "                kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
    "            ),\n",
    "            layers.ReLU(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(PROJECT_DIM),\n",
    "        ],\n",
    "        name=\"predictor\",\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def compute_loss(p, z):\n",
    "    # The authors of SimSiam emphasize the impact of\n",
    "    # the `stop_gradient` operator in the paper as it\n",
    "    # has an important role in the overall optimization.\n",
    "    z = tf.stop_gradient(z)\n",
    "    p = tf.math.l2_normalize(p, axis=1)\n",
    "    z = tf.math.l2_normalize(z, axis=1)\n",
    "    # Negative cosine similarity (minimizing this is\n",
    "    # equivalent to maximizing the similarity).\n",
    "    return -tf.reduce_mean(tf.reduce_sum((p * z), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class SimSiam(tf.keras.Model):\n",
    "    def __init__(self, encoder, predictor):\n",
    "        super(SimSiam, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.predictor = predictor\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        ds_one, ds_two = data\n",
    "\n",
    "        # Forward pass through the encoder and predictor.\n",
    "        with tf.GradientTape() as tape:\n",
    "            z1, z2 = self.encoder(ds_one), self.encoder(ds_two)\n",
    "            p1, p2 = self.predictor(z1), self.predictor(z2)\n",
    "            # Note that here we are enforcing the network to match\n",
    "            # the representations of two differently augmented batches\n",
    "            # of data.\n",
    "            loss = compute_loss(p1, z2) / 2 + compute_loss(p2, z1) / 2\n",
    "\n",
    "        # Compute gradients and update the parameters.\n",
    "        learnable_params = (\n",
    "            self.encoder.trainable_variables + self.predictor.trainable_variables\n",
    "        )\n",
    "        gradients = tape.gradient(loss, learnable_params)\n",
    "        self.optimizer.apply_gradients(zip(gradients, learnable_params))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create a cosine decay learning scheduler.\n",
    "num_training_samples = len(x_train)\n",
    "steps = EPOCHS * (num_training_samples // BATCH_SIZE)\n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "    initial_learning_rate=0.03, decay_steps=steps\n",
    ")\n",
    "\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Compile model and start training.\n",
    "simsiam = SimSiam(get_encoder(), get_predictor())\n",
    "simsiam.compile(optimizer=tf.keras.optimizers.SGD(lr_decayed_fn, momentum=0.6))\n",
    "history = simsiam.fit(ssl_ds, epochs=EPOCHS, callbacks=[early_stopping])\n",
    "\n",
    "# Visualize the training progress of the model.\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.grid()\n",
    "plt.title(\"Negative Cosine Similairty\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# We first create labeled `Dataset` objects.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# Then we shuffle, batch, and prefetch this dataset for performance. We\n",
    "# also apply random resized crops as an augmentation but only to the\n",
    "# training set.\n",
    "train_ds = (\n",
    "    train_ds.shuffle(1024)\n",
    "    .map(lambda x, y: (flip_random_crop(x), y), num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "# Extract the backbone ResNet20.\n",
    "backbone = tf.keras.Model(\n",
    "    simsiam.encoder.input, simsiam.encoder.get_layer(\"backbone_pool\").output\n",
    ")\n",
    "\n",
    "# We then create our linear classifier and train it.\n",
    "backbone.trainable = False\n",
    "inputs = layers.Input((CROP_TO, CROP_TO, 3))\n",
    "x = backbone(inputs, training=False)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "linear_model = tf.keras.Model(inputs, outputs, name=\"linear_model\")\n",
    "\n",
    "# Compile model and start training.\n",
    "linear_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    optimizer=tf.keras.optimizers.SGD(lr_decayed_fn, momentum=0.9),\n",
    ")\n",
    "history = linear_model.fit(\n",
    "    train_ds, validation_data=test_ds, epochs=EPOCHS, callbacks=[early_stopping]\n",
    ")\n",
    "_, test_acc = linear_model.evaluate(test_ds)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "simsiam",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
