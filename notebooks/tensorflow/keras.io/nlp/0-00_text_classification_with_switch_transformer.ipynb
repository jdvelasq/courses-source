{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Text classification with Switch Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "num_tokens_per_example = 200  # Only consider the first 200 words of each movie review\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train, maxlen=num_tokens_per_example\n",
    ")\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=num_tokens_per_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token.\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feedforward network.\n",
    "num_experts = 10  # Number of experts used in the Switch Transformer.\n",
    "batch_size = 50  # Batch size.\n",
    "learning_rate = 0.001  # Learning rate.\n",
    "dropout_rate = 0.25  # Dropout rate.\n",
    "num_epochs = 3  # Number of epochs.\n",
    "num_tokens_per_batch = (\n",
    "    batch_size * num_tokens_per_example\n",
    ")  # Total number of tokens per batch.\n",
    "print(f\"Number of tokens per batch: {num_tokens_per_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def create_feedforward_network(ff_dim, name=None):\n",
    "    return keras.Sequential(\n",
    "        [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(ff_dim)], name=name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def load_balanced_loss(router_probs, expert_mask):\n",
    "    # router_probs [tokens_per_batch, num_experts] is the probability assigned for\n",
    "    # each expert per token. expert_mask [tokens_per_batch, num_experts] contains\n",
    "    # the expert with the highest router probability in one−hot format.\n",
    "\n",
    "    num_experts = tf.shape(expert_mask)[-1]\n",
    "    # Get the fraction of tokens routed to each expert.\n",
    "    # density is a vector of length num experts that sums to 1.\n",
    "    density = tf.reduce_mean(expert_mask, axis=0)\n",
    "    # Get fraction of probability mass assigned to each expert from the router\n",
    "    # across all tokens. density_proxy is a vector of length num experts that sums to 1.\n",
    "    density_proxy = tf.reduce_mean(router_probs, axis=0)\n",
    "    # Want both vectors to have uniform allocation (1/num experts) across all\n",
    "    # num_expert elements. The two vectors will be pushed towards uniform allocation\n",
    "    # when the dot product is minimized.\n",
    "    loss = tf.reduce_mean(density_proxy * density) * tf.cast(\n",
    "        (num_experts ** 2), tf.dtypes.float32\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class Router(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_capacity):\n",
    "        self.num_experts = num_experts\n",
    "        self.route = layers.Dense(units=num_experts)\n",
    "        self.expert_capacity = expert_capacity\n",
    "        super(Router, self).__init__()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs shape: [tokens_per_batch, embed_dim]\n",
    "        # router_logits shape: [tokens_per_batch, num_experts]\n",
    "        router_logits = self.route(inputs)\n",
    "\n",
    "        if training:\n",
    "            # Add noise for exploration across experts.\n",
    "            router_logits += tf.random.uniform(\n",
    "                shape=router_logits.shape, minval=0.9, maxval=1.1\n",
    "            )\n",
    "        # Probabilities for each token of what expert it should be sent to.\n",
    "        router_probs = keras.activations.softmax(router_logits, axis=-1)\n",
    "        # Get the top−1 expert for each token. expert_gate is the top−1 probability\n",
    "        # from the router for each token. expert_index is what expert each token\n",
    "        # is going to be routed to.\n",
    "        expert_gate, expert_index = tf.math.top_k(router_probs, k=1)\n",
    "        # expert_mask shape: [tokens_per_batch, num_experts]\n",
    "        expert_mask = tf.one_hot(expert_index, depth=self.num_experts)\n",
    "        # Compute load balancing loss.\n",
    "        aux_loss = load_balanced_loss(router_probs, expert_mask)\n",
    "        self.add_loss(aux_loss)\n",
    "        # Experts have a fixed capacity, ensure we do not exceed it. Construct\n",
    "        # the batch indices, to each expert, with position in expert make sure that\n",
    "        # not more that expert capacity examples can be routed to each expert.\n",
    "        position_in_expert = tf.cast(\n",
    "            tf.math.cumsum(expert_mask, axis=0) * expert_mask, tf.dtypes.int32\n",
    "        )\n",
    "        # Keep only tokens that fit within expert capacity.\n",
    "        expert_mask *= tf.cast(\n",
    "            tf.math.less(\n",
    "                tf.cast(position_in_expert, tf.dtypes.int32), self.expert_capacity\n",
    "            ),\n",
    "            tf.dtypes.float32,\n",
    "        )\n",
    "        expert_mask_flat = tf.reduce_sum(expert_mask, axis=-1)\n",
    "        # Mask out the experts that have overflowed the expert capacity.\n",
    "        expert_gate *= expert_mask_flat\n",
    "        # Combine expert outputs and scaling with router probability.\n",
    "        # combine_tensor shape: [tokens_per_batch, num_experts, expert_capacity]\n",
    "        combined_tensor = (\n",
    "            tf.expand_dims(\n",
    "                expert_gate\n",
    "                * expert_mask_flat\n",
    "                * tf.squeeze(tf.one_hot(expert_index, depth=self.num_experts), 1),\n",
    "                -1,\n",
    "            )\n",
    "            * tf.squeeze(tf.one_hot(position_in_expert, depth=self.expert_capacity), 1)\n",
    "        )\n",
    "        # Create binary dispatch_tensor [tokens_per_batch, num_experts, expert_capacity]\n",
    "        # that is 1 if the token gets routed to the corresponding expert.\n",
    "        dispatch_tensor = tf.cast(combined_tensor, tf.dtypes.float32)\n",
    "\n",
    "        return dispatch_tensor, combined_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class Switch(layers.Layer):\n",
    "    def __init__(self, num_experts, embed_dim, num_tokens_per_batch, capacity_factor=1):\n",
    "        self.num_experts = num_experts\n",
    "        self.embed_dim = embed_dim\n",
    "        self.experts = [\n",
    "            create_feedforward_network(embed_dim) for _ in range(num_experts)\n",
    "        ]\n",
    "\n",
    "        self.expert_capacity = num_tokens_per_batch // self.num_experts\n",
    "        self.router = Router(self.num_experts, self.expert_capacity)\n",
    "        super(Switch, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        num_tokens_per_example = tf.shape(inputs)[1]\n",
    "\n",
    "        # inputs shape: [num_tokens_per_batch, embed_dim]\n",
    "        inputs = tf.reshape(inputs, [num_tokens_per_batch, self.embed_dim])\n",
    "        # dispatch_tensor shape: [expert_capacity, num_experts, tokens_per_batch]\n",
    "        # combine_tensor shape: [tokens_per_batch, num_experts, expert_capacity]\n",
    "        dispatch_tensor, combine_tensor = self.router(inputs)\n",
    "        # expert_inputs shape: [num_experts, expert_capacity, embed_dim]\n",
    "        expert_inputs = tf.einsum(\"ab,acd->cdb\", inputs, dispatch_tensor)\n",
    "        expert_inputs = tf.reshape(\n",
    "            expert_inputs, [self.num_experts, self.expert_capacity, self.embed_dim]\n",
    "        )\n",
    "        # Dispatch to experts\n",
    "        expert_input_list = tf.unstack(expert_inputs, axis=0)\n",
    "        expert_output_list = [\n",
    "            self.experts[idx](expert_input)\n",
    "            for idx, expert_input in enumerate(expert_input_list)\n",
    "        ]\n",
    "        # expert_outputs shape: [expert_capacity, num_experts, embed_dim]\n",
    "        expert_outputs = tf.stack(expert_output_list, axis=1)\n",
    "        # expert_outputs_combined shape: [tokens_per_batch, embed_dim]\n",
    "        expert_outputs_combined = tf.einsum(\n",
    "            \"abc,xba->xc\", expert_outputs, combine_tensor\n",
    "        )\n",
    "        # output shape: [batch_size, num_tokens_per_example, embed_dim]\n",
    "        outputs = tf.reshape(\n",
    "            expert_outputs_combined,\n",
    "            [batch_size, num_tokens_per_example, self.embed_dim],\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # The ffn can be either a standard feedforward network or a switch\n",
    "        # layer with a Mixture of Experts.\n",
    "        self.ffn = ffn\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def create_classifier():\n",
    "    switch = Switch(num_experts, embed_dim, num_tokens_per_batch)\n",
    "    transformer_block = TransformerBlock(ff_dim, num_heads, switch)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_tokens_per_example,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(\n",
    "        num_tokens_per_example, vocab_size, embed_dim\n",
    "    )\n",
    "    x = embedding_layer(inputs)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    classifier = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def run_experiment(classifier):\n",
    "    classifier.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    history = classifier.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "classifier = create_classifier()\n",
    "run_experiment(classifier)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_with_switch_transformer",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
