{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Keras debugging tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class MyAntirectifier(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        output_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(output_dim * 2, output_dim),\n",
    "            initializer=\"he_normal\",\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Take the positive part of the input\n",
    "        pos = tf.nn.relu(inputs)\n",
    "        # Take the negative part of the input\n",
    "        neg = tf.nn.relu(-inputs)\n",
    "        # Concatenate the positive and negative parts\n",
    "        concatenated = tf.concat([pos, neg], axis=0)\n",
    "        # Project the concatenation down to the same dimensionality as the input\n",
    "        return tf.matmul(concatenated, self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class MyAntirectifier(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        output_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(output_dim * 2, output_dim),\n",
    "            initializer=\"he_normal\",\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        pos = tf.nn.relu(inputs)\n",
    "        neg = tf.nn.relu(-inputs)\n",
    "        print(\"pos.shape:\", pos.shape)\n",
    "        print(\"neg.shape:\", neg.shape)\n",
    "        concatenated = tf.concat([pos, neg], axis=0)\n",
    "        print(\"concatenated.shape:\", concatenated.shape)\n",
    "        print(\"kernel.shape:\", self.kernel.shape)\n",
    "        return tf.matmul(concatenated, self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class MyAntirectifier(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        output_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(output_dim * 2, output_dim),\n",
    "            initializer=\"he_normal\",\n",
    "            name=\"kernel\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        pos = tf.nn.relu(inputs)\n",
    "        neg = tf.nn.relu(-inputs)\n",
    "        print(\"pos.shape:\", pos.shape)\n",
    "        print(\"neg.shape:\", neg.shape)\n",
    "        concatenated = tf.concat([pos, neg], axis=1)\n",
    "        print(\"concatenated.shape:\", concatenated.shape)\n",
    "        print(\"kernel.shape:\", self.kernel.shape)\n",
    "        return tf.matmul(concatenated, self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "x = tf.random.normal(shape=(2, 5))\n",
    "y = MyAntirectifier()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "num_tags = 12  # Number of unique issue tags\n",
    "num_words = 10000  # Size of vocabulary obtained when preprocessing text data\n",
    "num_departments = 4  # Number of departments for predictions\n",
    "\n",
    "title_input = keras.Input(\n",
    "    shape=(None,), name=\"title\"\n",
    ")  # Variable-length sequence of ints\n",
    "body_input = keras.Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\n",
    "tags_input = keras.Input(\n",
    "    shape=(num_tags,), name=\"tags\"\n",
    ")  # Binary vectors of size `num_tags`\n",
    "\n",
    "# Embed each word in the title into a 64-dimensional vector\n",
    "title_features = layers.Embedding(num_words, 64)(title_input)\n",
    "# Embed each word in the text into a 64-dimensional vector\n",
    "body_features = layers.Embedding(num_words, 64)(body_input)\n",
    "\n",
    "# Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
    "title_features = layers.LSTM(128)(title_features)\n",
    "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
    "body_features = layers.LSTM(32)(body_features)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "x = layers.concatenate([title_features, body_features, tags_input])\n",
    "\n",
    "# Stick a logistic regression for priority prediction on top of the features\n",
    "priority_pred = layers.Dense(1, name=\"priority\")(x)\n",
    "# Stick a department classifier on top of the features\n",
    "department_pred = layers.Dense(num_departments, name=\"department\")(x)\n",
    "\n",
    "# Instantiate an end-to-end model predicting both priority and department\n",
    "model = keras.Model(\n",
    "    inputs=[title_input, body_input, tags_input],\n",
    "    outputs=[priority_pred, department_pred],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        trainable_vars = self.trainable_variables\n",
    "        with tf.GradientTape() as tape2:\n",
    "            with tf.GradientTape() as tape1:\n",
    "                preds = self(inputs, training=True)  # Forward pass\n",
    "                # Compute the loss value\n",
    "                # (the loss function is configured in `compile()`)\n",
    "                loss = self.compiled_loss(targets, preds)\n",
    "            # Compute first-order gradients\n",
    "            dl_dw = tape1.gradient(loss, trainable_vars)\n",
    "        # Compute second-order gradients\n",
    "        d2l_dw2 = tape2.gradient(dl_dw, trainable_vars)\n",
    "\n",
    "        # Combine first-order and second-order gradients\n",
    "        grads = [0.5 * w1 + 0.5 * w2 for (w1, w2) in zip(d2l_dw2, dl_dw)]\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(targets, preds)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Construct an instance of MyModel\n",
    "def get_model():\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    intermediate = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(intermediate)\n",
    "    model = MyModel(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784)) / 255\n",
    "\n",
    "model = get_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=1024, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        print()\n",
    "        print(\"----Start of step: %d\" % (self.step_counter,))\n",
    "        self.step_counter += 1\n",
    "\n",
    "        inputs, targets = data\n",
    "        trainable_vars = self.trainable_variables\n",
    "        with tf.GradientTape() as tape2:\n",
    "            with tf.GradientTape() as tape1:\n",
    "                preds = self(inputs, training=True)  # Forward pass\n",
    "                # Compute the loss value\n",
    "                # (the loss function is configured in `compile()`)\n",
    "                loss = self.compiled_loss(targets, preds)\n",
    "            # Compute first-order gradients\n",
    "            dl_dw = tape1.gradient(loss, trainable_vars)\n",
    "        # Compute second-order gradients\n",
    "        d2l_dw2 = tape2.gradient(dl_dw, trainable_vars)\n",
    "\n",
    "        print(\"Max of dl_dw[0]: %.4f\" % tf.reduce_max(dl_dw[0]))\n",
    "        print(\"Min of dl_dw[0]: %.4f\" % tf.reduce_min(dl_dw[0]))\n",
    "        print(\"Mean of dl_dw[0]: %.4f\" % tf.reduce_mean(dl_dw[0]))\n",
    "        print(\"-\")\n",
    "        print(\"Max of d2l_dw2[0]: %.4f\" % tf.reduce_max(d2l_dw2[0]))\n",
    "        print(\"Min of d2l_dw2[0]: %.4f\" % tf.reduce_min(d2l_dw2[0]))\n",
    "        print(\"Mean of d2l_dw2[0]: %.4f\" % tf.reduce_mean(d2l_dw2[0]))\n",
    "\n",
    "        # Combine first-order and second-order gradients\n",
    "        grads = [0.5 * w1 + 0.5 * w2 for (w1, w2) in zip(d2l_dw2, dl_dw)]\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(targets, preds)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    run_eagerly=True,\n",
    ")\n",
    "model.step_counter = 0\n",
    "# We pass epochs=1 and steps_per_epoch=10 to only run 10 steps of training.\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=1024, verbose=0, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        trainable_vars = self.trainable_variables\n",
    "        with tf.GradientTape() as tape2:\n",
    "            with tf.GradientTape() as tape1:\n",
    "                preds = self(inputs, training=True)  # Forward pass\n",
    "                # Compute the loss value\n",
    "                # (the loss function is configured in `compile()`)\n",
    "                loss = self.compiled_loss(targets, preds)\n",
    "            # Compute first-order gradients\n",
    "            dl_dw = tape1.gradient(loss, trainable_vars)\n",
    "        # Compute second-order gradients\n",
    "        d2l_dw2 = tape2.gradient(dl_dw, trainable_vars)\n",
    "\n",
    "        dl_dw = [tf.math.l2_normalize(w) for w in dl_dw]\n",
    "        d2l_dw2 = [tf.math.l2_normalize(w) for w in d2l_dw2]\n",
    "\n",
    "        # Combine first-order and second-order gradients\n",
    "        grads = [0.5 * w1 + 0.5 * w2 for (w1, w2) in zip(d2l_dw2, dl_dw)]\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(targets, preds)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=1024, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        trainable_vars = self.trainable_variables\n",
    "        with tf.GradientTape() as tape2:\n",
    "            with tf.GradientTape() as tape1:\n",
    "                preds = self(inputs, training=True)  # Forward pass\n",
    "                # Compute the loss value\n",
    "                # (the loss function is configured in `compile()`)\n",
    "                loss = self.compiled_loss(targets, preds)\n",
    "            # Compute first-order gradients\n",
    "            dl_dw = tape1.gradient(loss, trainable_vars)\n",
    "        # Compute second-order gradients\n",
    "        d2l_dw2 = tape2.gradient(dl_dw, trainable_vars)\n",
    "\n",
    "        dl_dw = [tf.math.l2_normalize(w) for w in dl_dw]\n",
    "        d2l_dw2 = [tf.math.l2_normalize(w) for w in d2l_dw2]\n",
    "\n",
    "        # Combine first-order and second-order gradients\n",
    "        grads = [0.2 * w1 + 0.8 * w2 for (w1, w2) in zip(d2l_dw2, dl_dw)]\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(targets, preds)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "lr = learning_rate = keras.optimizers.schedules.InverseTimeDecay(\n",
    "    initial_learning_rate=0.1, decay_steps=25, decay_rate=0.1\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(lr),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=2048, validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "debugging_tips",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
