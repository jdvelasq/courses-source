{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificación de texto usando TextEncoders\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 30:00 min | Última modificación: Mayo 3, 2021 | [YouTube]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basado en: https://www.tensorflow.org/tutorials/keras/text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "4.2.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tfds.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y configuración del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Carga de un dataset precodificado de ~8k\n",
    "#\n",
    "(train_data, test_data), info = tfds.load(\n",
    "    \"imdb_reviews/subwords8k\",\n",
    "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 8185\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# El dataset incluye en la componente info un\n",
    "#  codificador del tipo tfds.features.text.SubwordTextEncoder\n",
    "#\n",
    "encoder = info.features[\"text\"].encoder\n",
    "\n",
    "print(\"Tamaño del vocabulario: {}\".format(encoder.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto codificado [4025, 222, 6307, 2327, 4043, 2120, 7975]\n",
      "Texto original: \"Hello TensorFlow.\"\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  Ejemplo del codificador\n",
    "#\n",
    "sample_string = \"Hello TensorFlow.\"\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print(\"Texto codificado {}\".format(encoded_string))\n",
    "\n",
    "#\n",
    "#  Ejemplo de decodificador\n",
    "#\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print('Texto original: \"{}\"'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025 ---> Hell\n",
      "222 ---> o \n",
      "6307 ---> Ten\n",
      "2327 ---> sor\n",
      "4043 ---> Fl\n",
      "2120 ---> ow\n",
      "7975 ---> .\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  El codificador transforma sílabas y letras cuando la palabra no está\n",
    "#  en el vocabulario predefinido. Mientras cada string sea más parecido\n",
    "#  al dataset, más corta es la representación.\n",
    "#\n",
    "for ts in encoded_string:\n",
    "    print(\"{} ---> {}\".format(ts, encoder.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto codificado: [  62   18   41  604  927   65    3  644 7968   21]\n",
      "Etiqueta: 0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  Codificación del primer ejemplo\n",
    "#\n",
    "for train_example, train_label in train_data.take(1):\n",
    "    print(\"Texto codificado:\", train_example[:10].numpy())\n",
    "    print(\"Etiqueta:\", train_label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "#  Decodificación del primer ejemplo\n",
    "#\n",
    "encoder.decode(train_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos para entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163142200117106421974188313179280394805241517125443655434534"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  Las secuencias de enteros que representan las críticas tienen longitudes diferentes\n",
    "#\n",
    "for train_example, _ in train_data.take(20):\n",
    "    print(len(train_example.numpy()), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  Se forman vectores de la misma longitud rellenando con ceros.\n",
    "#\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_batches = train_data.shuffle(BUFFER_SIZE).padded_batch(\n",
    "    32, padded_shapes=([None], [])\n",
    ")\n",
    "\n",
    "test_batches = test_data.padded_batch(32, padded_shapes=([None], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: (32, 1357)\t\tlabel shape: (32,)\n",
      "Batch shape: (32, 766)\t\tlabel shape: (32,)\n",
      "Batch shape: (32, 734)\t\tlabel shape: (32,)\n",
      "Batch shape: (32, 477)\t\tlabel shape: (32,)\n",
      "Batch shape: (32, 1432)\t\tlabel shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "for example_batch, label_batch in train_batches.take(5):\n",
    "    print(\"Batch shape:\", example_batch.shape, end=\"\")\n",
    "    print(\"\\t\\tlabel shape:\", label_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del modelo usando Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          130960    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 130,977\n",
      "Trainable params: 130,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Embedding(encoder.vocab_size, 16),\n",
    "        keras.layers.GlobalAveragePooling1D(),\n",
    "        keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.6891 - accuracy: 0.4999 - val_loss: 0.6653 - val_accuracy: 0.5052\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6426 - accuracy: 0.5267 - val_loss: 0.5956 - val_accuracy: 0.6052\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5616 - accuracy: 0.6391 - val_loss: 0.5324 - val_accuracy: 0.6990\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.4888 - accuracy: 0.7348 - val_loss: 0.4820 - val_accuracy: 0.7802\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.4343 - accuracy: 0.7933 - val_loss: 0.4445 - val_accuracy: 0.7969\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3895 - accuracy: 0.8263 - val_loss: 0.4187 - val_accuracy: 0.7917\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3569 - accuracy: 0.8462 - val_loss: 0.3974 - val_accuracy: 0.8448\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3312 - accuracy: 0.8622 - val_loss: 0.3828 - val_accuracy: 0.8635\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3071 - accuracy: 0.8772 - val_loss: 0.3707 - val_accuracy: 0.8448\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2909 - accuracy: 0.8830 - val_loss: 0.3625 - val_accuracy: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fead8da1710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_batches,\n",
    "    epochs=10,\n",
    "    validation_data=test_batches,\n",
    "    validation_steps=30,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3326 - accuracy: 0.8584\n",
      "loss: 0.333\n",
      "accuracy: 0.858\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_batches)\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(\"%s: %.3f\" % (name, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
