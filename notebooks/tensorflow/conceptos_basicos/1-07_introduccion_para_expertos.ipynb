{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c0b058-1387-477f-8264-67eb4ddcc2de",
   "metadata": {},
   "source": [
    "# Introducción para expertos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d9ceb5-41b6-444e-a2e0-c0d7e9240e3f",
   "metadata": {},
   "source": [
    "* 30:00 min | Última modificación: Mayo 5, 2021 | [YouTube]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c5266-2d3c-4f5c-b8bf-7cdefe2af11c",
   "metadata": {},
   "source": [
    "Adaptado de:\n",
    "\n",
    "* https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "\n",
    "* https://keras.io/getting_started/intro_to_keras_for_researchers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9ae3a6-38ae-4887-9595-141eaf910158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a1282-9353-44ce-bd94-087870245fdc",
   "metadata": {},
   "source": [
    "## Tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e385e166-9027-49c6-afbc-075bfe36da78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[5, 2],\n",
       "       [1, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[5, 2], [1, 3]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b240cef-a452-4c7b-a8d0-95d52791e225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2],\n",
       "       [1, 3]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b82c27-25e5-45cd-912c-ed84d3c98bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tf.int32, TensorShape([2, 2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebd9fa9e-3219-4d6f-a270-18a145fb95e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones(shape=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c6caa5a-512f-45bb-b722-e3df0fb4ed5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros(shape=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f1257-f15a-4100-af78-3dddc8187bfe",
   "metadata": {},
   "source": [
    "## Aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba0968-b564-4ff8-8d39-441375417144",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c484081c-d20f-44ce-a330-1428c814a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15415263-50c6-45d6-9f91-c0858f6c1a0b",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d1693-4530-49b4-9a9b-c1acf5c72e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_value = tf.random.normal(shape=(2, 2))\n",
    "a = tf.Variable(initial_value)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4dec67-82de-44a8-997a-7f9675807752",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign(new_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j]\n",
    "\n",
    "added_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign_add(added_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j] + added_value[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed3ba67-2502-4fa2-815d-3281051be635",
   "metadata": {},
   "source": [
    "## Matemáticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988eb69-8ad8-400d-94a3-b93d4b723897",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "c = a + b\n",
    "d = tf.square(c)\n",
    "e = tf.exp(d)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507be6da-e053-4048-86ab-6d0ac021092d",
   "metadata": {},
   "source": [
    "## Gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163daab5-06f7-414b-91b8-b3982591cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)  # Start recording the history of operations applied to `a`\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))  # Do some math using `a`\n",
    "    # What's the gradient of `c` with respect to `a`?\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f058c-8635-4645-8990-085a9755b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(a)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cd3eb-26fb-42ea-9c73-955923495bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "        dc_da = tape.gradient(c, a)\n",
    "    d2c_da2 = outer_tape.gradient(dc_da, a)\n",
    "    print(d2c_da2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d6afd-8f31-4a0c-873c-18ddd92c515d",
   "metadata": {},
   "source": [
    "## Capas de Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d797a4-9d4b-498e-81be-a9faa24d20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f27295-6445-4960-a4d1-91eb719cde93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our layer.\n",
    "linear_layer = Linear(units=4, input_dim=2)\n",
    "\n",
    "# The layer can be treated as a function.\n",
    "# Here we call it on some data.\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert y.shape == (2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1fc616-2d66-4da9-b089-4ad7c1340d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b5b46-88f3-4831-8c48-86d709b3b879",
   "metadata": {},
   "source": [
    "## Creación de pesos en las capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209d8b0-cdad-42f3-8f8a-1b877c00087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "# Instantiate our lazy layer.\n",
    "linear_layer = Linear(4)\n",
    "\n",
    "# This will also call `build(input_shape)` and create the weights.\n",
    "y = linear_layer(tf.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171fcda-a6c2-47f6-b857-b02c36aaddb7",
   "metadata": {},
   "source": [
    "## Gradiente de una capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53a89a-233f-4a25-ae63-faa10ba98c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Instantiate our linear layer (defined above) with 10 units.\n",
    "linear_layer = Linear(10)\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Iterate over the batches of the dataset.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "\n",
    "    # Open a GradientTape.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        logits = linear_layer(x)\n",
    "\n",
    "        # Loss value for this batch.\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "    # Get gradients of weights wrt the loss.\n",
    "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e41c2c-16c8-40dd-8579-8a50a467839e",
   "metadata": {},
   "source": [
    "## Pesos entrenables y no entrenables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79166ea1-47e9-468c-ae3a-0ac222384cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    \"\"\"Returns the sum of the inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        # Create a non-trainable weight.\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [2. 2.]\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [4. 4.]\n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10922327-5a23-47fe-b2c3-765db742d798",
   "metadata": {},
   "source": [
    "## Capas de capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754088ee-1f57-4ada-ab2d-3be4db628a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reuse the Linear class\n",
    "# with a `build` method that we defined above.\n",
    "\n",
    "\n",
    "class MLP(keras.layers.Layer):\n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "# The first call to the `mlp` object will create the weights.\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# Weights are recursively tracked.\n",
    "assert len(mlp.weights) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856012a-bc6a-4f08-b41f-1a22395af6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56729cfa-36a0-4368-86a5-1578fbfed5d3",
   "metadata": {},
   "source": [
    "## Seguimiento de pérdidas en capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d99a8-d39c-4d30-923b-ccdaa8115897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularization(keras.layers.Layer):\n",
    "    \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularization, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # We use `add_loss` to create a regularization loss\n",
    "        # that depends on the inputs.\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c5a2f-404e-41d0-8939-d68d4956123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the loss layer in a MLP block.\n",
    "\n",
    "\n",
    "class SparseMLP(keras.layers.Layer):\n",
    "    \"\"\"Stack of Linear layers with a sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SparseMLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.regularization = ActivityRegularization(1e-2)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.regularization(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = SparseMLP()\n",
    "y = mlp(tf.ones((10, 10)))\n",
    "\n",
    "print(mlp.losses)  # List containing one float32 scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c85dd-8dd7-42b0-b63f-e1af3a00c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses correspond to the *last* forward pass.\n",
    "mlp = SparseMLP()\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1  # No accumulation.\n",
    "\n",
    "# Let's demonstrate how to use these losses in a training loop.\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# A new MLP.\n",
    "mlp = SparseMLP()\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        logits = mlp(x)\n",
    "\n",
    "        # External loss value for this batch.\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "        # Add the losses created during the forward pass.\n",
    "        loss += sum(mlp.losses)\n",
    "\n",
    "        # Get gradients of weights wrt the loss.\n",
    "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd89c7-7816-4704-b499-7c3dfdc6b0e8",
   "metadata": {},
   "source": [
    "## Metricas de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7a14b-6682-409a-9795-70c2ef8da4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a metric object\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Prepare our layer, loss, and optimizer.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(2):\n",
    "    # Iterate over the batches of a dataset.\n",
    "    for step, (x, y) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            # Compute the loss value for this batch.\n",
    "            loss_value = loss_fn(y, logits)\n",
    "\n",
    "        # Update the state of the `accuracy` metric.\n",
    "        accuracy.update_state(y, logits)\n",
    "\n",
    "        # Update the weights of the model to minimize the loss value.\n",
    "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "        # Logging the current accuracy value so far.\n",
    "        if step % 200 == 0:\n",
    "            print(\"Epoch:\", epoch, \"Step:\", step)\n",
    "            print(\"Total running accuracy so far: %.3f\" % accuracy.result())\n",
    "\n",
    "    # Reset the metric's state at the end of an epoch\n",
    "    accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c5997-183f-4f1b-a4ac-0d2b6aedcc9e",
   "metadata": {},
   "source": [
    "## Funciones compiladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7839e1-fbd5-42a0-9463-793d10bcf9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revisar vs tsflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0967ed-a37c-47e4-9644-e2aaa7a1c3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3125ecd-663f-4a99-8120-65bd1766cfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (x_train, y_train),\n",
    "    (x_test, y_test),\n",
    ") = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b40ca1-cbc3-4cc8-960b-f5b7883dc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    ")\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87648c3-87ac-4c1d-9a6e-07e504a54cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = keras.layers.Conv2D(32, 3, activation=\"relu\")\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.d1 = keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.d2 = keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86daddb9-c5aa-41a0-91ac-a89a8be59267",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28b7aa0-856f-494d-965d-a38732138a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "856ed015-0f3f-4343-ab9a-4f72c1a25e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285a2731-2d2c-4cc3-b642-8513b498ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a6b5c51-7599-49ad-83a0-9e0a8778ef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.13709662854671478, Accuracy: 95.8933334350586, Test Loss: 0.056839171797037125, Test Accuracy: 98.05999755859375\n",
      "Epoch 2, Loss: 0.04269269481301308, Accuracy: 98.68000030517578, Test Loss: 0.050610341131687164, Test Accuracy: 98.36000061035156\n",
      "Epoch 3, Loss: 0.022624198347330093, Accuracy: 99.2733383178711, Test Loss: 0.052395038306713104, Test Accuracy: 98.33999633789062\n",
      "Epoch 4, Loss: 0.013498089276254177, Accuracy: 99.53500366210938, Test Loss: 0.06471875309944153, Test Accuracy: 98.22000122070312\n",
      "Epoch 5, Loss: 0.010173597373068333, Accuracy: 99.63500213623047, Test Loss: 0.057008448988199234, Test Accuracy: 98.29999542236328\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, \"\n",
    "        f\"Loss: {train_loss.result()}, \"\n",
    "        f\"Accuracy: {train_accuracy.result() * 100}, \"\n",
    "        f\"Test Loss: {test_loss.result()}, \"\n",
    "        f\"Test Accuracy: {test_accuracy.result() * 100}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159e2c7-de34-48d2-a9b2-7c13a0cf51d6",
   "metadata": {},
   "source": [
    "## Modo de enetrenamiento y modo de inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976a84f-63ba-4bcd-a71f-0bc300534ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(keras.layers.Layer):\n",
    "    def __init__(self, rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class MLPWithDropout(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLPWithDropout()\n",
    "y_train = mlp(tf.ones((2, 2)), training=True)\n",
    "y_test = mlp(tf.ones((2, 2)), training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e2c41-5ced-448f-af0e-ddf0d38e31b4",
   "metadata": {},
   "source": [
    "## API Funcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f05bab-0d2c-45d9-90e3-c4ffa7ff3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use an `Input` object to describe the shape and dtype of the inputs.\n",
    "# This is the deep learning equivalent of *declaring a type*.\n",
    "# The shape argument is per-sample; it does not include the batch size.\n",
    "# The functional API focused on defining per-sample transformations.\n",
    "# The model we create will automatically batch the per-sample transformations,\n",
    "# so that it can be called on batches of data.\n",
    "inputs = tf.keras.Input(shape=(16,), dtype=\"float32\")\n",
    "\n",
    "# We call layers on these \"type\" objects\n",
    "# and they return updated types (new shapes/dtypes).\n",
    "x = Linear(32)(inputs)  # We are reusing the Linear layer we defined earlier.\n",
    "x = Dropout(0.5)(x)  # We are reusing the Dropout layer we defined earlier.\n",
    "outputs = Linear(10)(x)\n",
    "\n",
    "# A functional `Model` can be defined by specifying inputs and outputs.\n",
    "# A model is itself a layer like any other.\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# A functional model already has weights, before being called on any data.\n",
    "# That's because we defined its input shape in advance (in `Input`).\n",
    "assert len(model.weights) == 4\n",
    "\n",
    "# Let's call our model on some data, for fun.\n",
    "y = model(tf.ones((2, 16)))\n",
    "assert y.shape == (2, 10)\n",
    "\n",
    "# You can pass a `training` argument in `__call__`\n",
    "# (it will get passed down to the Dropout layer).\n",
    "y = model(tf.ones((2, 16)), training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b232e7de-4e58-4c74-919f-743856b170a4",
   "metadata": {},
   "source": [
    "## Ejemplo 1: Autoencoder variacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b6face-97a7-4af0-9b15-982c36fdc07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a434ab-a30c-4fcf-b274-7dd6bde6bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
    "        self.dense_output = layers.Dense(original_dim, activation=tf.nn.sigmoid)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b46af-4bc9-4a95-823a-7f4c0dc84199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(layers.Layer):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(**kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a929934-ce5f-4e67-a462-ba8f5fa250a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model.\n",
    "vae = VariationalAutoEncoder(original_dim=784, intermediate_dim=64, latent_dim=32)\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstructed = vae(x)  # Compute input reconstruction.\n",
    "        # Compute loss.\n",
    "        loss = loss_fn(x, reconstructed)\n",
    "        loss += sum(vae.losses)  # Add KLD term.\n",
    "    # Update the weights of the VAE.\n",
    "    grads = tape.gradient(loss, vae.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "losses = []  # Keep track of the losses over time.\n",
    "for step, x in enumerate(dataset):\n",
    "    loss = training_step(x)\n",
    "    # Logging.\n",
    "    losses.append(float(loss))\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
    "\n",
    "    # Stop after 1000 steps.\n",
    "    # Training the model to convergence is left\n",
    "    # as an exercise to the reader.\n",
    "    if step >= 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d64b34-3e63-4041-8103-bd7d6cc66b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e587d1f-d660-4344-8ffa-1741c911a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.map(lambda x: (x, x))  # Use x_train as both inputs & targets\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "# Configure the model for training.\n",
    "vae.compile(optimizer, loss=loss_fn)\n",
    "\n",
    "# Actually training the model.\n",
    "vae.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59e79c-9460-4102-ac05-70b495c8a9b1",
   "metadata": {},
   "source": [
    "## Ejemplo 2: Hypernetworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599e39b-684c-4fb9-bad5-7778abac4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_dim = 784\n",
    "classes = 10\n",
    "\n",
    "# This is the model we'll actually use to predict labels (the hypernetwork).\n",
    "outer_model = keras.Sequential(\n",
    "    [keras.layers.Dense(64, activation=tf.nn.relu), keras.layers.Dense(classes),]\n",
    ")\n",
    "\n",
    "# It doesn't need to create its own weights, so let's mark its layers\n",
    "# as already built. That way, calling `outer_model` won't create new variables.\n",
    "for layer in outer_model.layers:\n",
    "    layer.built = True\n",
    "\n",
    "# This is the number of weight coefficients to generate. Each layer in the\n",
    "# hypernetwork requires output_dim * input_dim + output_dim coefficients.\n",
    "num_weights_to_generate = (classes * 64 + classes) + (64 * input_dim + 64)\n",
    "\n",
    "# This is the model that generates the weights of the `outer_model` above.\n",
    "inner_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(num_weights_to_generate, activation=tf.nn.sigmoid),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5624ee-1995-4bab-9951-24006fc913e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "\n",
    "# We'll use a batch size of 1 for this experiment.\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(1)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict weights for the outer model.\n",
    "        weights_pred = inner_model(x)\n",
    "\n",
    "        # Reshape them to the expected shapes for w and b for the outer model.\n",
    "        # Layer 0 kernel.\n",
    "        start_index = 0\n",
    "        w0_shape = (input_dim, 64)\n",
    "        w0_coeffs = weights_pred[:, start_index : start_index + np.prod(w0_shape)]\n",
    "        w0 = tf.reshape(w0_coeffs, w0_shape)\n",
    "        start_index += np.prod(w0_shape)\n",
    "        # Layer 0 bias.\n",
    "        b0_shape = (64,)\n",
    "        b0_coeffs = weights_pred[:, start_index : start_index + np.prod(b0_shape)]\n",
    "        b0 = tf.reshape(b0_coeffs, b0_shape)\n",
    "        start_index += np.prod(b0_shape)\n",
    "        # Layer 1 kernel.\n",
    "        w1_shape = (64, classes)\n",
    "        w1_coeffs = weights_pred[:, start_index : start_index + np.prod(w1_shape)]\n",
    "        w1 = tf.reshape(w1_coeffs, w1_shape)\n",
    "        start_index += np.prod(w1_shape)\n",
    "        # Layer 1 bias.\n",
    "        b1_shape = (classes,)\n",
    "        b1_coeffs = weights_pred[:, start_index : start_index + np.prod(b1_shape)]\n",
    "        b1 = tf.reshape(b1_coeffs, b1_shape)\n",
    "        start_index += np.prod(b1_shape)\n",
    "\n",
    "        # Set the weight predictions as the weight variables on the outer model.\n",
    "        outer_model.layers[0].kernel = w0\n",
    "        outer_model.layers[0].bias = b0\n",
    "        outer_model.layers[1].kernel = w1\n",
    "        outer_model.layers[1].bias = b1\n",
    "\n",
    "        # Inference on the outer model.\n",
    "        preds = outer_model(x)\n",
    "        loss = loss_fn(y, preds)\n",
    "\n",
    "    # Train only inner model.\n",
    "    grads = tape.gradient(loss, inner_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, inner_model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "losses = []  # Keep track of the losses over time.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_step(x, y)\n",
    "\n",
    "    # Logging.\n",
    "    losses.append(float(loss))\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
    "\n",
    "    # Stop after 1000 steps.\n",
    "    # Training the model to convergence is left\n",
    "    # as an exercise to the reader.\n",
    "    if step >= 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7fa64d-64bf-4894-900c-01dfcb477f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96cac19-c0b6-4563-96c2-999aa5814550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665b0bc-e52a-46d9-8843-0070564691bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
