{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de datos en distintos formatos\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *30 min* | Última modificación: Junio 22, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive permite la lectura de archivos en distintos formatos. Al finalizar este tutorial, el lector estará en capacidad de leer archivos en formatos de texto, CSV y JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de Hive en un contenedor de Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usando el directorio de trabajo de la máquina local:\n",
    "\n",
    "```\n",
    "docker run --rm -it -v \"$PWD\":/datalake  --name hive -p 50070:50070 -p 8088:8088 -p 8888:8888 -p 5000:5000 jdvelasq/hive:2.3.6-pseudo\n",
    "```\n",
    "\n",
    "* Usando un volumen de Docker (llamado `datalake`):\n",
    "\n",
    "```\n",
    "docker run --rm -it -v datalake:/datalake --name hive  -p 50070:50070 -p 8088:8088 -p 8888:8888 -p 5000:5000 jdvelasq/hive:2.3.6-pseudo\n",
    "```\n",
    "\n",
    "\n",
    "* Consola conectada a un contendor que ya está corriendo:\n",
    "\n",
    "```\n",
    "docker exec -it hive bash\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Se carga la librería para interactuar con Hive desde Jupyter.\n",
    "## \n",
    "%load_ext bigdata\n",
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/drivers\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Crea la carpeta drivers en el HDFS\n",
    "##\n",
    "!hdfs dfs -rm -r -f /tmp/drivers\n",
    "!hdfs dfs -mkdir    /tmp/drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://deb.nodesource.com/node_13.x bionic InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease              \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease                   \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease               \n",
      "Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease              \n",
      "Reading package lists... Done                       \n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-08 11:53:13--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/drivers.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2043 (2.0K) [text/plain]\n",
      "Saving to: 'drivers.csv'\n",
      "\n",
      "drivers.csv         100%[===================>]   2.00K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2019-11-08 11:53:13 (1.48 MB/s) - 'drivers.csv' saved [2043/2043]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/drivers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-08 11:53:14--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/drivers.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4308 (4.2K) [text/plain]\n",
      "Saving to: 'drivers.json'\n",
      "\n",
      "drivers.json        100%[===================>]   4.21K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2019-11-08 11:53:14 (3.95 MB/s) - 'drivers.json' saved [4308/4308]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/drivers.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de formato JSON desde un archivo cargado como texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta estrategía de carga de datos, el archivo original con formato JSON es cargado como texto, donde cada registro de la tabla corresponde a una línea del archivo original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Copia el archivo al HDFS para su importación posterior a Hive\n",
    "##\n",
    "!hdfs dfs -copyFromLocal drivers.json  /tmp/drivers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup       4308 2019-11-08 11:53 /tmp/drivers/drivers.json\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/drivers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda, se crea la tabla `drivers_raw_json`, la cual tiene una única columna llamada `textcol`. Luego, el archivo `drivers.json` es cargado en dicha tabla. Finalmente, se visualizan los primeros cinco registros para verificar que la lectura fue correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS drivers_raw_json;\n",
      "OK\n",
      "Time taken: 7.144 seconds\n",
      "CREATE TABLE drivers_raw_json (\n",
      "    textcol STRING\n",
      ") \n",
      "STORED AS TEXTFILE;\n",
      "OK\n",
      "Time taken: 0.966 seconds\n",
      "LOAD DATA INPATH \n",
      "    '/tmp/drivers/drivers.json' \n",
      "OVERWRITE INTO TABLE drivers_raw_json;\n",
      "Loading data to table default.drivers_raw_json\n",
      "OK\n",
      "Time taken: 0.893 seconds\n",
      "SELECT * FROM drivers_raw_json LIMIT 5;\n",
      "OK\n",
      "{\"driverId\":10,\"name\":\"George Vetticaden\",\"ssn\":621011971,\"location\":\"244-4532 Nulla Rd.\",\"certified\":\"N\",\"wage-plan\":\"miles\"}\n",
      "{\"driverId\":11,\"name\":\"Jamie Engesser\",\"ssn\":262112338,\"location\":\"366-4125 Ac Street\",\"certified\":\"N\",\"wage-plan\":\"miles\"}\n",
      "{\"driverId\":12,\"name\":\"Paul Coddin\",\"ssn\":198041975,\"location\":\"Ap #622-957 Risus. Street\",\"certified\":\"Y\",\"wage-plan\":\"hours\"}\n",
      "{\"driverId\":13,\"name\":\"Joe Niemiec\",\"ssn\":139907145,\"location\":\"2071 Hendrerit. Ave\",\"certified\":\"Y\",\"wage-plan\":\"hours\"}\n",
      "{\"driverId\":14,\"name\":\"Adis Cesir\",\"ssn\":820812209,\"location\":\"Ap #810-1228 In St.\",\"certified\":\"Y\",\"wage-plan\":\"hours\"}\n",
      "Time taken: 2.372 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS drivers_raw_json;\n",
    "\n",
    "CREATE TABLE drivers_raw_json (\n",
    "    textcol STRING\n",
    ") \n",
    "STORED AS TEXTFILE;\n",
    "\n",
    "LOAD DATA INPATH \n",
    "    '/tmp/drivers/drivers.json' \n",
    "OVERWRITE INTO TABLE drivers_raw_json;\n",
    "\n",
    "SELECT * FROM drivers_raw_json LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que en la salida anterior, cada fila corresponde a un registro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura usando get_json_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de los campos pueden ser extraídos usando la función `get_json_object`, cuyos parámetros son el nombre del campo en la tabla y el nombre del campo en la estructura JSON. En el siguiente fragmento de código, se utiliza una consulta para extraer los campos `driverId`, `name`  y `ssn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "    GET_JSON_OBJECT(textcol,'$.driverId'),\n",
      "    GET_JSON_OBJECT(textcol,'$.name'),\n",
      "    GET_JSON_OBJECT(textcol,'$.ssn')\n",
      "FROM \n",
      "    drivers_raw_json \n",
      "LIMIT \n",
      "    10;\n",
      "OK\n",
      "10\tGeorge Vetticaden\t621011971\n",
      "11\tJamie Engesser\t262112338\n",
      "12\tPaul Coddin\t198041975\n",
      "13\tJoe Niemiec\t139907145\n",
      "14\tAdis Cesir\t820812209\n",
      "15\tRohit Bakshi\t239005227\n",
      "16\tTom McCuch\t363303105\n",
      "17\tEric Mizell\t123808238\n",
      "18\tGrant Liu\t171010151\n",
      "19\tAjay Singh\t160005158\n",
      "Time taken: 0.657 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT\n",
    "    GET_JSON_OBJECT(textcol,'$.driverId'),\n",
    "    GET_JSON_OBJECT(textcol,'$.name'),\n",
    "    GET_JSON_OBJECT(textcol,'$.ssn')\n",
    "FROM \n",
    "    drivers_raw_json \n",
    "LIMIT \n",
    "    10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de json_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función cumple el mismo objetivo de la anterior, pero es mucho más eficiente ya que el registro es procesado únicamente una vez para realizar la extracción de la información requerida. Ya que `json_tuple` es una UDF, debe usarse `LATERAL VIEW` para realizar la consulta, tal como se ejemplifica a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "    t1.driverId,\n",
      "    t1.name,\n",
      "    t1.ssn\n",
      "FROM\n",
      "    drivers_raw_json  t0\n",
      "LATERAL VIEW\n",
      "    JSON_TUPLE(t0.textcol, 'driverId', 'name', 'ssn') t1\n",
      "    AS driverId, name, ssn\n",
      "LIMIT 5;\n",
      "OK\n",
      "10\tGeorge Vetticaden\t621011971\n",
      "11\tJamie Engesser\t262112338\n",
      "12\tPaul Coddin\t198041975\n",
      "13\tJoe Niemiec\t139907145\n",
      "14\tAdis Cesir\t820812209\n",
      "Time taken: 0.106 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT\n",
    "    t1.driverId,\n",
    "    t1.name,\n",
    "    t1.ssn\n",
    "FROM\n",
    "    drivers_raw_json  t0\n",
    "LATERAL VIEW\n",
    "    JSON_TUPLE(t0.textcol, 'driverId', 'name', 'ssn') t1\n",
    "    AS driverId, name, ssn\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de archivos en formato JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive también permite la importación directa de archivos en formato JSON usando el serde `JsonSerDe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Se copia el archivo al sistema HDFS\n",
    "##\n",
    "!hdfs dfs -copyFromLocal drivers.*  /tmp/drivers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda, se crea la tabla `drivers_json` donde el formato de cada registro es especificado como JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS drivers_json;\n",
      "OK\n",
      "Time taken: 0.121 seconds\n",
      "CREATE TABLE drivers_json (\n",
      "    driverId  INT, \n",
      "    name      STRING, \n",
      "    ssn       BIGINT,\n",
      "    location  STRING, \n",
      "    certified STRING, \n",
      "    wageplan  STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'\n",
      "STORED AS TEXTFILE\n",
      "LOCATION '/tmp/drivers-json';\n",
      "OK\n",
      "Time taken: 0.099 seconds\n",
      "json;DATA INPATH '/tmp/drivers/drivers.json' OVERWRITE INTO TABLE drivers_ \n",
      "Loading data to table default.drivers_json\n",
      "OK\n",
      "Time taken: 0.436 seconds\n",
      "SELECT * FROM drivers_json LIMIT 5;\n",
      "OK\n",
      "10\tGeorge Vetticaden\t621011971\t244-4532 Nulla Rd.\tN\tNULL\n",
      "11\tJamie Engesser\t262112338\t366-4125 Ac Street\tN\tNULL\n",
      "12\tPaul Coddin\t198041975\tAp #622-957 Risus. Street\tY\tNULL\n",
      "13\tJoe Niemiec\t139907145\t2071 Hendrerit. Ave\tY\tNULL\n",
      "14\tAdis Cesir\t820812209\tAp #810-1228 In St.\tY\tNULL\n",
      "Time taken: 0.154 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS drivers_json;\n",
    "\n",
    "CREATE TABLE drivers_json (\n",
    "    driverId  INT, \n",
    "    name      STRING, \n",
    "    ssn       BIGINT,\n",
    "    location  STRING, \n",
    "    certified STRING, \n",
    "    wageplan  STRING)\n",
    "ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/tmp/drivers-json';\n",
    "\n",
    "LOAD DATA INPATH '/tmp/drivers/drivers.json' OVERWRITE INTO TABLE drivers_json;\n",
    "\n",
    "SELECT * FROM drivers_json LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de archivos en formato CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo se usa el serde `OpenCSVSerde` para leer archivo en formato CSV. Note que se usa la cláusula `with serdeproperties` para indicar las características del formato CSV utilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS drivers_csv;\n",
      "OK\n",
      "Time taken: 0.096 seconds\n",
      "CREATE TABLE drivers_csv (driverId  INT, \n",
      "                         name      STRING, \n",
      "                         ssn       BIGINT,\n",
      "                         location  STRING, \n",
      "                         certified STRING, \n",
      "                         wageplan  STRING)\n",
      "ROW FORMAT SERDE \n",
      "    'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
      "WITH SERDEPROPERTIES (\n",
      "   'separatorChar' = \",\",\n",
      "   'quoteChar'     = '\\'',\n",
      "   'escapeChar'    = \"\\\\\");\n",
      "OK\n",
      "Time taken: 0.074 seconds\n",
      "sv;D DATA INPATH '/tmp/drivers/drivers.csv' OVERWRITE INTO TABLE drivers_c \n",
      "Loading data to table default.drivers_csv\n",
      "OK\n",
      "Time taken: 0.425 seconds\n",
      "SELECT * FROM drivers_csv LIMIT 5;\n",
      "OK\n",
      "driverId\tname\tssn\tlocation\tcertified\twage-plan\n",
      "10\tGeorge Vetticaden\t621011971\t244-4532 Nulla Rd.\tN\tmiles\n",
      "11\tJamie Engesser\t262112338\t366-4125 Ac Street\tN\tmiles\n",
      "12\tPaul Coddin\t198041975\tAp #622-957 Risus. Street\tY\thours\n",
      "13\tJoe Niemiec\t139907145\t2071 Hendrerit. Ave\tY\thours\n",
      "Time taken: 0.158 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS drivers_csv;\n",
    "\n",
    "CREATE TABLE drivers_csv (driverId  INT, \n",
    "                         name      STRING, \n",
    "                         ssn       BIGINT,\n",
    "                         location  STRING, \n",
    "                         certified STRING, \n",
    "                         wageplan  STRING)\n",
    "ROW FORMAT SERDE \n",
    "    'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "WITH SERDEPROPERTIES (\n",
    "   'separatorChar' = \",\",\n",
    "   'quoteChar'     = '\\'',\n",
    "   'escapeChar'    = \"\\\\\");\n",
    "\n",
    "LOAD DATA INPATH '/tmp/drivers/drivers.csv' OVERWRITE INTO TABLE drivers_csv;\n",
    "\n",
    "SELECT * FROM drivers_csv LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga usando expresiones regulaes y RegexSerDe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se realiza la carga de datos especificando los campos mediante el uso de expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Copia los archivos al HDFS\n",
    "##\n",
    "!hdfs dfs -copyFromLocal drivers.csv  /tmp/drivers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS drivers_regex;\n",
      "OK\n",
      "Time taken: 0.079 seconds\n",
      "CREATE TABLE drivers_regex(\n",
      "    driverId  INT, \n",
      "    name      STRING, \n",
      "    ssn       BIGINT,\n",
      "    location  STRING, \n",
      "    certified STRING, \n",
      "    wageplan  STRING)\n",
      "ROW FORMAT SERDE \n",
      "    'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "   'input.regex' = '(\\\\d+),([^,]*),(\\\\d+),([^,]*),([^,]*),([^,]*)')\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.074 seconds\n",
      "egex;DATA INPATH '/tmp/drivers/drivers.csv' OVERWRITE INTO TABLE drivers_r \n",
      "Loading data to table default.drivers_regex\n",
      "OK\n",
      "Time taken: 0.433 seconds\n",
      "SELECT * FROM drivers_regex LIMIT 5;\n",
      "OK\n",
      "10\tGeorge Vetticaden\t621011971\t244-4532 Nulla Rd.\tN\tmiles\n",
      "11\tJamie Engesser\t262112338\t366-4125 Ac Street\tN\tmiles\n",
      "12\tPaul Coddin\t198041975\tAp #622-957 Risus. Street\tY\thours\n",
      "13\tJoe Niemiec\t139907145\t2071 Hendrerit. Ave\tY\thours\n",
      "14\tAdis Cesir\t820812209\tAp #810-1228 In St.\tY\thours\n",
      "Time taken: 0.132 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS drivers_regex;\n",
    "\n",
    "CREATE TABLE drivers_regex(\n",
    "    driverId  INT, \n",
    "    name      STRING, \n",
    "    ssn       BIGINT,\n",
    "    location  STRING, \n",
    "    certified STRING, \n",
    "    wageplan  STRING)\n",
    "ROW FORMAT SERDE \n",
    "    'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "WITH SERDEPROPERTIES (\n",
    "   'input.regex' = '(\\\\d+),([^,]*),(\\\\d+),([^,]*),([^,]*),([^,]*)')\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "LOAD DATA INPATH '/tmp/drivers/drivers.csv' OVERWRITE INTO TABLE drivers_regex;\n",
    "\n",
    "SELECT * FROM drivers_regex LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La expresión regular usada es la siguiente: \n",
    "\n",
    "    (\\\\d+),([^,]*),(\\\\d+),([^,]*),([^,]*),([^,]*)`\n",
    "\n",
    "donde:\n",
    "\n",
    "* Los paréntesis indica los campos, esto es, `(\\\\d+)` es el primer campo, `([^,]*)` es el segundo y así sucesivamente.\n",
    "\n",
    "* Se indica que la coma es el separador entre campos.\n",
    "\n",
    "* `(\\\\d+)` representa una cadena de uno o más dígitos.\n",
    "\n",
    "* `[...]` representan uno o más posibles caracteres, de tal forma que `[^,]` indica cualquier caracter excepto una coma. Finalmente, el * indica cero, una o más ocurrencias. Es así como `[^,]*` representa cualquier cadena de caracteres que no contenga una coma.\n",
    "\n",
    "* La expresión regular usada indica que los campos 1 y 3 son numéricos, y los restantes son texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm drivers.* *.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
