{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de palabras en Apache Hive\n",
    "===\n",
    "\n",
    "* *30 min* | Última modificación: Junio 22, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se aborda el proceso de construcción de aplicaciones en Apache Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de la librería de Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta librería permite enviar comandos a Hive de forma interactiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata\n",
    "%timeout 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificación y pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta primera parte se aborda el proceso de desarrollo y depuración de una aplicación de Hive en la máquina local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de datos de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos que usa la aplicación se encuentran localizados como una carpeta en el directorio actual de trabajo. Estos datos serán consumidos por Hive directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Se crea el directorio wordcount en la carpeta actual de trabajo\n",
    "## y se escriben tres archivos en ella.\n",
    "##\n",
    "!mkdir -p wordcount/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generan tres archivos de prueba que se almacenan en la carpeta `wordcount/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta aplicación se usarán dos tablas:\n",
    "\n",
    "* `docs`: para cargar el contenido de los archivos de texto, donde cada línea equivale a un registro.\n",
    "\n",
    "* `word_counts`: En donde aparece cada palabra y su respectivo conteo.\n",
    "\n",
    "A continuación se elimnan dichas tablas si existen en el sistema, y luego se crea la tabla `docs` con un solo campo del tipo `STRING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS docs;\n",
      "OK\n",
      "Time taken: 9.982 seconds\n",
      "DROP TABLE IF EXISTS word_counts;\n",
      "OK\n",
      "Time taken: 0.117 seconds\n",
      "CREATE TABLE docs (line STRING);\n",
      "OK\n",
      "Time taken: 0.669 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS docs;\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "CREATE TABLE docs (line STRING);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código, se hace la carga directa de todos los archivos que se encuentran en el directorio `wordcount` en la tabla `docs`. Luego, se imprimen los primeros cinco registros de la tabla para verificar que la lectura fue correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD DATA LOCAL INPATH \"wordcount/\" OVERWRITE INTO TABLE docs;\n",
      "Loading data to table default.docs\n",
      "OK\n",
      "Time taken: 1.232 seconds\n",
      "SELECT * FROM docs LIMIT 5;\n",
      "OK\n",
      "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
      "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
      "on the simultaneous application of statistics, computer programming and operations research \n",
      "to quantify performance.\n",
      "\n",
      "Time taken: 1.401 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "LOAD DATA LOCAL INPATH \"wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "SELECT * FROM docs LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargados los archivos, se procede a partir las líneas por palabras, usando la función `split(line, '\\\\s')`;  la expresión `\\\\s` indica que se realice la partición por los espacios en blanco; de esta forma, `split()` genera una lista de palabras. La función `explode(.)` de Hive en conjunto con `SELECT`, genera un nuevo registro por cada palabra en `line`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT explode(split(line, '\\\\s')) AS word FROM docs LIMIT 5;\n",
      "OK\n",
      "Analytics\n",
      "is\n",
      "the\n",
      "discovery,\n",
      "interpretation,\n",
      "Time taken: 0.469 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT explode(split(line, '\\\\s')) AS word FROM docs LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conteo de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar el conteo, la expresión `SELECT word, count(1) AS count ... GROUP BY word` genera una tabla con dos columnas, donde la primera columna (`word`) correspodne a cada palabra en el texto, y la segunda columna representa la cantidad de veces que aparece en los registros generados por la expresión `SELECT explode(split(line, '\\\\s')) AS word FROM docs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE word_counts \n",
      "AS\n",
      "    SELECT word, count(1) AS count \n",
      "    FROM\n",
      "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
      "GROUP BY \n",
      "    word\n",
      "ORDER BY \n",
      "    word;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = vagrant_20190524163121_b50c4a94-3298-41ff-83fe-2a699ac6f832\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1558627508041_0060, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558627508041_0060/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558627508041_0060\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-05-24 16:31:28,294 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-05-24 16:31:32,992 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.62 sec\n",
      "2019-05-24 16:31:38,709 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.84 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 840 msec\n",
      "Ended Job = job_1558627508041_0060\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1558627508041_0061, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558627508041_0061/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558627508041_0061\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2019-05-24 16:31:51,000 Stage-2 map = 0%,  reduce = 0%\n",
      "2019-05-24 16:31:55,602 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.98 sec\n",
      "2019-05-24 16:32:01,273 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 2.17 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 170 msec\n",
      "Ended Job = job_1558627508041_0061\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/word_counts\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.84 sec   HDFS Read: 10201 HDFS Write: 4345 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 2.17 sec   HDFS Read: 9507 HDFS Write: 1731 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 5 seconds 10 msec\n",
      "OK\n",
      "Time taken: 42.768 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "CREATE TABLE word_counts \n",
    "AS\n",
    "    SELECT word, count(1) AS count \n",
    "    FROM\n",
    "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
    "GROUP BY \n",
    "    word\n",
    "ORDER BY \n",
    "    word;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar los resultados obtenidos, se realiza un `SELECT` sobre la tabla `word_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM word_counts LIMIT 10;\n",
      "OK\n",
      "\t20\n",
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t3\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Time taken: 0.167 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM word_counts LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cierre de Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, y una vez se ha terminado de depurar el código, se cierra el interprete de Hive que se abrió en el background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%hive_quit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versión en productivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la segunda parte, se procede a llevar el aplicativo a productivo con los siguientes cambios:\n",
    "\n",
    "* Los datos son leidos del sistema HDFS de Hadoop.\n",
    "\n",
    "* Los resultdos son guardados en una carpeta del sistema Hadoop.\n",
    "\n",
    "* El script se almacena en un archivo en el disco duro, para su uso posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copia de los datos al sistema HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwx------   - vagrant supergroup          0 2019-05-23 17:24 /tmp/hadoop-yarn\n",
      "drwxrwxrwx   - vagrant supergroup          0 2019-05-23 16:09 /tmp/hive\n",
      "drwxrwxr-x   - vagrant supergroup          0 2019-05-24 16:14 /tmp/output\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se usan un directorio temporal en el HDFS. La siguiente\n",
    "## instrucción muestra el contenido del dicho directorio\n",
    "##\n",
    "!hdfs dfs -ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Crea la carpeta wordcount en el hdfs\n",
    "##\n",
    "!hdfs dfs -mkdir /tmp/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwx------   - vagrant supergroup          0 2019-05-23 17:24 /tmp/hadoop-yarn\n",
      "drwxrwxrwx   - vagrant supergroup          0 2019-05-23 16:09 /tmp/hive\n",
      "drwxrwxr-x   - vagrant supergroup          0 2019-05-24 16:14 /tmp/output\n",
      "drwxr-xr-x   - vagrant supergroup          0 2019-05-24 16:32 /tmp/wordcount\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Verifica la creación de la carpeta\n",
    "##\n",
    "!hdfs dfs -ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Copia los archvios del directorio local wordcount/\n",
    "## al directorio /tmp/wordcount/ en el hdfs\n",
    "##\n",
    "!hdfs dfs -copyFromLocal wordcount/*  /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 vagrant supergroup       1093 2019-05-24 16:32 /tmp/wordcount/text0.txt\n",
      "-rw-r--r--   1 vagrant supergroup        352 2019-05-24 16:32 /tmp/wordcount/text1.txt\n",
      "-rw-r--r--   1 vagrant supergroup        440 2019-05-24 16:32 /tmp/wordcount/text2.txt\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Verifica que los archivos esten copiados\n",
    "## en el hdfs\n",
    "##\n",
    "!hdfs dfs -ls /tmp/wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación del script y ajuste del código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizan dos cambios. En primer lugar, se sustituye la línea \n",
    "\n",
    "    LOAD DATA LOCAL INPATH \"wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "    \n",
    "por:\n",
    "\n",
    "    LOAD DATA INPATH \"/tmp/wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "\n",
    "para que Hive lea los datos del directorio `/tmp/wordcount/` en el HDFS. En segundo lugar, se agrega\n",
    "\n",
    "    INSERT OVERWRITE DIRECTORY '/tmp/output' \n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "    SELECT * FROM word_counts;\n",
    "    \n",
    "para que los resultados sean almacenados en la carpeta `/tmp/output` como un archivo en formato CSV. El programa es guadado como `wordcount.hql` en el computador local. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.hql\n",
    "\n",
    "DROP TABLE IF EXISTS docs;\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "\n",
    "CREATE TABLE docs (line STRING);\n",
    "\n",
    "LOAD DATA INPATH \"/tmp/wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "\n",
    "CREATE TABLE word_counts \n",
    "AS\n",
    "    SELECT word, count(1) AS count \n",
    "    FROM\n",
    "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
    "GROUP BY \n",
    "    word\n",
    "ORDER BY \n",
    "    word;\n",
    "    \n",
    "INSERT OVERWRITE DIRECTORY '/tmp/output' \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "SELECT * FROM word_counts;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se ha almacenado el programa, se puede ejecutar con:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive-2.3.4/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.8.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    }
   ],
   "source": [
    "!hive -S -e 'source wordcount.hql'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde `-S` indica que Hive se ejecute en modo silencioso; `-e` que se ejecute la expresión `source wordcount.hql`. Lo anterior es equivalente a abrir Hive y luego ejecutar:\n",
    "\n",
    "     hive> source 'wordcount.hql'\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados quedan almacenados en la carpeta `/tmp/output` del sistema HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxrwxr-x   1 vagrant supergroup       1653 2019-05-24 16:33 /tmp/output/000000_0\n"
     ]
    }
   ],
   "source": [
    "## Se lista el contenido del archivo.\n",
    "!hdfs dfs -ls /tmp/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",20\n",
      "(DA),1\n",
      "(see,1\n",
      "Analytics,2\n",
      "Analytics,,1\n",
      "Big,1\n",
      "Data,3\n",
      "Especially,1\n",
      "Organizations,1\n",
      "Since,1\n"
     ]
    }
   ],
   "source": [
    "## se visualiza la cabecera del archivo.\n",
    "!hdfs dfs -cat /tmp/output/000000_0 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción para extraer los resultados es usar\n",
    "\n",
    "      $ hive -S -e 'SELECT * FROM word_counts;' > result.csv\n",
    "     \n",
    "     \n",
    "en donde el archivo `result.txt` se almacena localmente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
