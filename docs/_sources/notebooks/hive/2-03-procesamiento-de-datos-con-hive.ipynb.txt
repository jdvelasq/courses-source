{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de datos con Hive\n",
    "\n",
    "* *30 min* | Última modificación: Junio 22, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tutorial esta basado en https://es.hortonworks.com/tutorial/how-to-process-data-with-apache-hive/\n",
    "\n",
    "El objetivo de este tutorial es implemetar consultas en Hive para analizar, procesar y filtrar los datos existentes en una bodega de datos, usando lenguaje SQL estándar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se usa el magic `bigdata` para usar interactivamente Hive desde un notebook de Jupyter. El parámetro `timeout` es el tiempo máximo de espera de procesamiento antes de que se reporte un error por procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata\n",
    "%timeout 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos se encuentran almacenados en la carpeta `drivers` del directorio actual. A continución se procede a crear la carpeta `/tmp/drivers` en el sistema de archivos de Hadoop (HDFS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp/drivers': File exists\n",
      "copyFromLocal: `/tmp/drivers/drivers.csv': File exists\n",
      "copyFromLocal: `/tmp/drivers/timesheet.csv': File exists\n",
      "-rw-r--r--   1 vagrant supergroup       2043 2019-05-26 13:52 /tmp/drivers/drivers.csv\n",
      "Found 1 items\n",
      "-rwxr-xr-x   1 vagrant supergroup       1800 2019-05-26 13:54 /tmp/drivers/specific-columns/000000_0\n",
      "-rw-r--r--   1 vagrant supergroup      26205 2019-05-26 13:52 /tmp/drivers/timesheet.csv\n",
      "-rw-r--r--   1 vagrant supergroup    2272077 2019-05-26 13:59 /tmp/drivers/truck_event_text_partition.csv\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Crea la carpeta drivers en el HDFS\n",
    "##\n",
    "!hdfs dfs -mkdir /tmp/drivers\n",
    "\n",
    "##\n",
    "## Copia los archivos al HDFS\n",
    "##\n",
    "!hdfs dfs -copyFromLocal drivers/*  /tmp/drivers/\n",
    "\n",
    "##\n",
    "## Lista los archivos al HDFS para verificar\n",
    "## que los archivos fueron copiados correctamente.\n",
    "##\n",
    "!hdfs dfs -ls /tmp/drivers/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El contenido de un archivo puede ser visualizado parcialmente usando el comando `tail`. Se usa para realizar una inspección rápida del contenido de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box 213- 8948 Nec Ave,Y,hours\n",
      "27,Mark Lochbihler,392603159,8355 Ipsum St.,Y,hours\n",
      "28,Olivier Renault,959908181,P.O. Box 243- 6509 Erat. Avenue,Y,hours\n",
      "29,Teddy Choi,185502192,P.O. Box 106- 7003 Amet Rd.,Y,hours\n",
      "30,Dan Rice,282307061,Ap #881-9267 Mollis Avenue,Y,hours\n",
      "31,Rommel Garcia,858912101,P.O. Box 945- 6015 Sociis St.,Y,hours\n",
      "32,Ryan Templeton,290304287,765-6599 Egestas. Av.,Y,hours\n",
      "33,Sridhara Sabbella,967409015,Ap #477-2507 Sagittis Avenue,Y,hours\n",
      "34,Frank Romano,391407216,Ap #753-6814 Quis Ave,Y,hours\n",
      "35,Emil Siemes,971401151,321-2976 Felis Rd.,Y,hours\n",
      "36,Andrew Grande,245303216,Ap #685-9598 Egestas Rd.,Y,hours\n",
      "37,Wes Floyd,190504074,P.O. Box 269- 9611 Nulla Street,Y,hours\n",
      "38,Scott Shaw,386411175,276 Lobortis Road,Y,hours\n",
      "39,David Kaiser,967706052,9185 At Street,Y,hours\n",
      "40,Nicolas Maillard,208510217,1027 Quis Rd.,Y,hours\n",
      "41,Greg Phillips,308103116,P.O. Box 847- 5961 Arcu. Road,Y,hours\n",
      "42,Randy Gelhausen,853302254,145-4200 In- Avenue,Y,hours\n",
      "43,Dave Patton,977706052,3028 A- St.,Y,hours"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se imprime el final del archivo drivers\n",
    "##\n",
    "!hdfs dfs -tail /tmp/drivers/drivers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42,36,56,2612\n",
      "42,37,48,2550\n",
      "42,38,55,2527\n",
      "42,39,57,2723\n",
      "42,40,55,2728\n",
      "42,41,50,2557\n",
      "42,42,53,2773\n",
      "42,43,55,2786\n",
      "42,44,54,2638\n",
      "42,45,57,2542\n",
      "42,46,48,2526\n",
      "42,47,50,2795\n",
      "42,48,53,2609\n",
      "42,49,58,2584\n",
      "42,50,48,2692\n",
      "42,51,50,2566\n",
      "42,52,48,2735\n",
      "43,1,46,2622\n",
      "43,2,47,2688\n",
      "43,3,50,2544\n",
      "43,4,56,2573\n",
      "43,5,54,2691\n",
      "43,6,52,2796\n",
      "43,7,53,2564\n",
      "43,8,58,2624\n",
      "43,9,50,2528\n",
      "43,10,57,2721\n",
      "43,11,51,2722\n",
      "43,12,59,2681\n",
      "43,13,52,2683\n",
      "43,14,46,2663\n",
      "43,15,53,2579\n",
      "43,16,56,2519\n",
      "43,17,54,2584\n",
      "43,18,47,2665\n",
      "43,19,55,2511\n",
      "43,20,60,2677\n",
      "43,21,52,2585\n",
      "43,22,60,2719\n",
      "43,23,48,2655\n",
      "43,24,48,2641\n",
      "43,25,53,2512\n",
      "43,26,48,2612\n",
      "43,27,58,2614\n",
      "43,28,60,2551\n",
      "43,29,55,2682\n",
      "43,30,49,2504\n",
      "43,31,51,2701\n",
      "43,32,57,2554\n",
      "43,33,52,2730\n",
      "43,34,54,2783\n",
      "43,35,51,2681\n",
      "43,36,51,2655\n",
      "43,37,46,2629\n",
      "43,38,58,2739\n",
      "43,39,47,2535\n",
      "43,40,50,2512\n",
      "43,41,51,2701\n",
      "43,42,55,2538\n",
      "43,43,58,2775\n",
      "43,44,56,2545\n",
      "43,45,46,2671\n",
      "43,46,57,2680\n",
      "43,47,50,2572\n",
      "43,48,52,2517\n",
      "43,49,56,2743\n",
      "43,50,59,2665\n",
      "43,51,58,2593\n",
      "43,52,48,2764"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /tmp/drivers/timesheet.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la tabla `temp_drivers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crea la tabla `temp_drivers`, que es almacenada en el disco como un archivo de texto, para almacenar la información de los conductores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS temp_drivers;\n",
      "OK\n",
      "Time taken: 9.983 seconds\n",
      "CREATE TABLE temp_drivers (col_value STRING) STORED AS TEXTFILE\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.7 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS temp_drivers;\n",
    "CREATE TABLE temp_drivers (col_value STRING) STORED AS TEXTFILE\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente, se visualizan las tablas en la base de datos actual que empiezan por t para verificar que la tabla fue creada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW TABLES LIKE 't*';\n",
      "OK\n",
      "temp_drivers\n",
      "temp_timesheet\n",
      "timesheet\n",
      "truck_events\n",
      "truck_events_subset\n",
      "Time taken: 0.115 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SHOW TABLES LIKE 't*';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos para la tabla `temp_drivers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente consulta realiza la carga de los datos del archivo `drivers.csv` en la tabla `temp_drivers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ers;\n",
      "Loading data to table default.temp_drivers\n",
      "OK\n",
      "Time taken: 0.892 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "LOAD DATA INPATH '/tmp/drivers/drivers.csv' OVERWRITE INTO TABLE temp_drivers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive consume los datos, es decir, mueve los datos a la bodega de datos, de tal forma que el archivo `drivers.csv` es eliminado de la carpeta `/tmp/drivers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - vagrant supergroup          0 2019-05-26 13:54 /tmp/drivers/specific-columns\n",
      "-rw-r--r--   1 vagrant supergroup      26205 2019-05-26 13:52 /tmp/drivers/timesheet.csv\n",
      "-rw-r--r--   1 vagrant supergroup    2272077 2019-05-26 13:59 /tmp/drivers/truck_event_text_partition.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene los primeros 10 registros de la tabla para realizar una inspección rápida de los datos y verificar que los datos fueron cargados correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM temp_drivers LIMIT 10;\n",
      "OK\n",
      "10,George Vetticaden,621011971,244-4532 Nulla Rd.,N,miles\n",
      "11,Jamie Engesser,262112338,366-4125 Ac Street,N,miles\n",
      "12,Paul Coddin,198041975,Ap #622-957 Risus. Street,Y,hours\n",
      "13,Joe Niemiec,139907145,2071 Hendrerit. Ave,Y,hours\n",
      "14,Adis Cesir,820812209,Ap #810-1228 In St.,Y,hours\n",
      "15,Rohit Bakshi,239005227,648-5681 Dui- Rd.,Y,hours\n",
      "16,Tom McCuch,363303105,P.O. Box 313- 962 Parturient Rd.,Y,hours\n",
      "17,Eric Mizell,123808238,P.O. Box 579- 2191 Gravida. Street,Y,hours\n",
      "18,Grant Liu,171010151,Ap #928-3159 Vestibulum Av.,Y,hours\n",
      "19,Ajay Singh,160005158,592-9430 Nonummy Avenue,Y,hours\n",
      "Time taken: 1.219 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM temp_drivers LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la tabla `drivers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crea la tabla `drivers` en donde se colocará la información extraída de la tabla `temp_drivers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS drivers;\n",
      "OK\n",
      "Time taken: 0.104 seconds\n",
      "CREATE TABLE drivers (driverId  INT, \n",
      "                      name      STRING, \n",
      "                      ssn       BIGINT,\n",
      "                      location  STRING, \n",
      "                      certified STRING, \n",
      "                      wageplan  STRING)\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.046 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS drivers;\n",
    "\n",
    "CREATE TABLE drivers (driverId  INT, \n",
    "                      name      STRING, \n",
    "                      ssn       BIGINT,\n",
    "                      location  STRING, \n",
    "                      certified STRING, \n",
    "                      wageplan  STRING)\n",
    "\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que cada registro de la tabla `temp_drivers` es una línea de texto, se aplica una expresión regular (`regexp_extract`) para realizar la división del texto por las comas. La parte `{1}` representa la primera cadena de caracteres después de realizar la partición, `{2}` la segunda y así sucesivamente. Después de la llamada a la función `regexp_extract` se indica el nombre de la columna en la tabla `drivers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT OVERWRITE TABLE drivers\n",
      "SELECT\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) name,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) ssn,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) location,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){5}', 1) certified,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){6}', 1) wageplan\n",
      "FROM \n",
      "    temp_drivers;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = vagrant_20190526135929_aa7a8e2b-cf1b-447c-b45d-5cc173930c9a\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1558824450495_0028, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558824450495_0028/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558824450495_0028\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2019-05-26 13:59:36,998 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-05-26 13:59:41,513 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.79 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 790 msec\n",
      "Ended Job = job_1558824450495_0028\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/drivers/.hive-staging_hive_2019-05-26_13-59-29_324_9173285359057532304-1/-ext-10000\n",
      "Loading data to table default.drivers\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 1.79 sec   HDFS Read: 6812 HDFS Write: 2036 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 1 seconds 790 msec\n",
      "OK\n",
      "Time taken: 13.89 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "INSERT OVERWRITE TABLE drivers\n",
    "SELECT\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) name,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) ssn,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) location,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){5}', 1) certified,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){6}', 1) wageplan\n",
    "FROM \n",
    "    temp_drivers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica la instrucción `SELECT` para revisar el resultado de la carga de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM drivers LIMIT 10;\n",
      "OK\n",
      "11\tJamie Engesser\t262112338\t366-4125 Ac Street\tN\tmiles\n",
      "12\tPaul Coddin\t198041975\tAp #622-957 Risus. Street\tY\thours\n",
      "13\tJoe Niemiec\t139907145\t2071 Hendrerit. Ave\tY\thours\n",
      "14\tAdis Cesir\t820812209\tAp #810-1228 In St.\tY\thours\n",
      "15\tRohit Bakshi\t239005227\t648-5681 Dui- Rd.\tY\thours\n",
      "16\tTom McCuch\t363303105\tP.O. Box 313- 962 Parturient Rd.\tY\thours\n",
      "17\tEric Mizell\t123808238\tP.O. Box 579- 2191 Gravida. Street\tY\thours\n",
      "18\tGrant Liu\t171010151\tAp #928-3159 Vestibulum Av.\tY\thours\n",
      "19\tAjay Singh\t160005158\t592-9430 Nonummy Avenue\tY\thours\n",
      "20\tChris Harris\t921812303\t883-2691 Proin Avenue\tY\thours\n",
      "Time taken: 0.125 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM drivers LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la tabla `temp_timesheet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a crear la tabla y cargar los datos para el archivo `time_sheet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS temp_timesheet;\n",
      "OK\n",
      "Time taken: 0.082 seconds\n",
      "CREATE TABLE temp_timesheet (col_value string) \n",
      "STORED AS TEXTFILE\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.039 seconds\n",
      "mesheet;\n",
      "Loading data to table default.temp_timesheet\n",
      "OK\n",
      "Time taken: 0.492 seconds\n",
      "SELECT * FROM temp_timesheet LIMIT 10;\n",
      "OK\n",
      "10,1,70,3300\n",
      "10,2,70,3300\n",
      "10,3,60,2800\n",
      "10,4,70,3100\n",
      "10,5,70,3200\n",
      "10,6,70,3300\n",
      "10,7,70,3000\n",
      "10,8,70,3300\n",
      "10,9,70,3200\n",
      "10,10,50,2500\n",
      "Time taken: 0.109 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS temp_timesheet;\n",
    "\n",
    "CREATE TABLE temp_timesheet (col_value string) \n",
    "STORED AS TEXTFILE\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "LOAD DATA INPATH '/tmp/drivers/timesheet.csv' OVERWRITE INTO TABLE temp_timesheet;\n",
    "\n",
    "SELECT * FROM temp_timesheet LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la tabla `timesheet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede igual que en las tablas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS timesheet;\n",
      "OK\n",
      "Time taken: 0.061 seconds\n",
      "ogged INT)\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.038 seconds\n",
      "INSERT OVERWRITE TABLE timesheet\n",
      "SELECT\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) week,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) hours_logged,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) miles_logged\n",
      "FROM \n",
      "    temp_timesheet;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = vagrant_20190526135945_aec017dd-051f-4936-aa3b-361f9948f08d\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1558824450495_0029, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558824450495_0029/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558824450495_0029\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2019-05-26 13:59:52,540 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-05-26 13:59:57,979 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.2 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 200 msec\n",
      "Ended Job = job_1558824450495_0029\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/timesheet/.hive-staging_hive_2019-05-26_13-59-45_243_3849914088891625676-1/-ext-10000\n",
      "Loading data to table default.timesheet\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.2 sec   HDFS Read: 30729 HDFS Write: 24476 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 200 msec\n",
      "OK\n",
      "Time taken: 14.3 seconds\n",
      "SELECT * FROM timesheet LIMIT 10;\n",
      "OK\n",
      "10\t2\t70\t3300\n",
      "10\t3\t60\t2800\n",
      "10\t4\t70\t3100\n",
      "10\t5\t70\t3200\n",
      "10\t6\t70\t3300\n",
      "10\t7\t70\t3000\n",
      "10\t8\t70\t3300\n",
      "10\t9\t70\t3200\n",
      "10\t10\t50\t2500\n",
      "10\t11\t70\t2900\n",
      "Time taken: 0.1 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS timesheet;\n",
    "\n",
    "CREATE TABLE timesheet (driverId INT, week INT, hours_logged INT , miles_logged INT)\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "INSERT OVERWRITE TABLE timesheet\n",
    "SELECT\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) week,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) hours_logged,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) miles_logged\n",
    "FROM \n",
    "    temp_timesheet;\n",
    "\n",
    "SELECT * FROM timesheet LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cantidad de horas y millas de cada conductor por año.\n",
    "\n",
    "En la siguiente consulta se desea obtener para cada conductor la cantidad de horas y millas por año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \n",
      "    driverId, \n",
      "    sum(hours_logged), \n",
      "    sum(miles_logged) \n",
      "FROM \n",
      "    timesheet \n",
      "GROUP BY \n",
      "    driverId;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = vagrant_20190526140000_883d5933-efd0-4f05-bf42-515cb864b49d\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1558824450495_0030, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558824450495_0030/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558824450495_0030\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-05-26 14:00:08,822 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-05-26 14:00:13,157 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.5 sec\n",
      "2019-05-26 14:00:18,572 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.83 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 830 msec\n",
      "Ended Job = job_1558824450495_0030\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.83 sec   HDFS Read: 33424 HDFS Write: 1005 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 830 msec\n",
      "OK\n",
      "10\t3162\t143850\n",
      "11\t3642\t179300\n",
      "12\t2639\t135962\n",
      "13\t2727\t134126\n",
      "14\t2781\t136624\n",
      "15\t2734\t138750\n",
      "16\t2746\t137205\n",
      "17\t2701\t135992\n",
      "18\t2654\t137834\n",
      "19\t2738\t137968\n",
      "20\t2644\t134564\n",
      "21\t2751\t138719\n",
      "22\t2733\t137550\n",
      "23\t2750\t137980\n",
      "24\t2647\t134461\n",
      "25\t2723\t139180\n",
      "26\t2730\t137530\n",
      "27\t2771\t137922\n",
      "28\t2723\t137469\n",
      "29\t2760\t138255\n",
      "30\t2773\t137473\n",
      "31\t2704\t137057\n",
      "32\t2736\t137422\n",
      "33\t2759\t139285\n",
      "34\t2811\t137728\n",
      "35\t2728\t138727\n",
      "36\t2795\t138025\n",
      "37\t2694\t137223\n",
      "38\t2760\t137464\n",
      "39\t2745\t138788\n",
      "40\t2700\t136931\n",
      "41\t2723\t138407\n",
      "42\t2697\t136673\n",
      "43\t2750\t136993\n",
      "Time taken: 19.496 seconds, Fetched: 34 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive \n",
    "SELECT \n",
    "    driverId, \n",
    "    sum(hours_logged), \n",
    "    sum(miles_logged) \n",
    "FROM \n",
    "    timesheet \n",
    "GROUP BY \n",
    "    driverId;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta para unir las tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso final consiste en crear una consulta que agregue el nombre del conductor de la tabla `drivers` con la cantidad de horas y millas por año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \n",
      "    d.driverId, \n",
      "    d.name, \n",
      "    t.total_hours, \n",
      "    t.total_miles \n",
      "FROM \n",
      "    drivers d\n",
      "JOIN (\n",
      "    SELECT \n",
      "        driverId, \n",
      "        sum(hours_logged)total_hours, \n",
      "        sum(miles_logged)total_miles \n",
      "    FROM \n",
      "        timesheet \n",
      "    GROUP BY \n",
      "        driverId \n",
      "    ) t\n",
      "ON \n",
      "    (d.driverId = t.driverId);\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = vagrant_20190526140020_29ccbd1b-c07c-4b6b-9c03-e591c24b92dd\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1558824450495_0031, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558824450495_0031/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558824450495_0031\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2019-05-26 14:00:29,173 Stage-2 map = 0%,  reduce = 0%\n",
      "2019-05-26 14:00:34,523 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.15 sec\n",
      "2019-05-26 14:00:38,865 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.31 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 310 msec\n",
      "Ended Job = job_1558824450495_0031\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive-2.3.4/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.8.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "2019-05-26 14:00:45\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2019-05-26 14:00:47\tDump the side-table for tag: 0 with group count: 33 into file: file:/tmp/vagrant/1a0d2a35-e99a-4d93-bff5-37a7462a341d/hive_2019-05-26_14-00-20_821_8120342489066290102-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable\n",
      "2019-05-26 14:00:47\tUploaded 1 File to: file:/tmp/vagrant/1a0d2a35-e99a-4d93-bff5-37a7462a341d/hive_2019-05-26_14-00-20_821_8120342489066290102-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable (1325 bytes)\n",
      "2019-05-26 14:00:47\tEnd of local task; Time Taken: 1.418 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1558824450495_0032, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558824450495_0032/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558824450495_0032\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0\n",
      "2019-05-26 14:00:53,327 Stage-3 map = 0%,  reduce = 0%\n",
      "2019-05-26 14:00:57,606 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.32 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 320 msec\n",
      "Ended Job = job_1558824450495_0032\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.31 sec   HDFS Read: 32524 HDFS Write: 946 SUCCESS\n",
      "Stage-Stage-3: Map: 1   Cumulative CPU: 1.32 sec   HDFS Read: 6808 HDFS Write: 1411 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 4 seconds 630 msec\n",
      "OK\n",
      "11\tJamie Engesser\t3642\t179300\n",
      "12\tPaul Coddin\t2639\t135962\n",
      "13\tJoe Niemiec\t2727\t134126\n",
      "14\tAdis Cesir\t2781\t136624\n",
      "15\tRohit Bakshi\t2734\t138750\n",
      "16\tTom McCuch\t2746\t137205\n",
      "17\tEric Mizell\t2701\t135992\n",
      "18\tGrant Liu\t2654\t137834\n",
      "19\tAjay Singh\t2738\t137968\n",
      "20\tChris Harris\t2644\t134564\n",
      "21\tJeff Markham\t2751\t138719\n",
      "22\tNadeem Asghar\t2733\t137550\n",
      "23\tAdam Diaz\t2750\t137980\n",
      "24\tDon Hilborn\t2647\t134461\n",
      "25\tJean-Philippe Playe\t2723\t139180\n",
      "26\tMichael Aube\t2730\t137530\n",
      "27\tMark Lochbihler\t2771\t137922\n",
      "28\tOlivier Renault\t2723\t137469\n",
      "29\tTeddy Choi\t2760\t138255\n",
      "30\tDan Rice\t2773\t137473\n",
      "31\tRommel Garcia\t2704\t137057\n",
      "32\tRyan Templeton\t2736\t137422\n",
      "33\tSridhara Sabbella\t2759\t139285\n",
      "34\tFrank Romano\t2811\t137728\n",
      "35\tEmil Siemes\t2728\t138727\n",
      "36\tAndrew Grande\t2795\t138025\n",
      "37\tWes Floyd\t2694\t137223\n",
      "38\tScott Shaw\t2760\t137464\n",
      "39\tDavid Kaiser\t2745\t138788\n",
      "40\tNicolas Maillard\t2700\t136931\n",
      "41\tGreg Phillips\t2723\t138407\n",
      "42\tRandy Gelhausen\t2697\t136673\n",
      "43\tDave Patton\t2750\t136993\n",
      "Time taken: 37.865 seconds, Fetched: 33 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT \n",
    "    d.driverId, \n",
    "    d.name, \n",
    "    t.total_hours, \n",
    "    t.total_miles \n",
    "FROM \n",
    "    drivers d\n",
    "JOIN (\n",
    "    SELECT \n",
    "        driverId, \n",
    "        sum(hours_logged)total_hours, \n",
    "        sum(miles_logged)total_miles \n",
    "    FROM \n",
    "        timesheet \n",
    "    GROUP BY \n",
    "        driverId \n",
    "    ) t\n",
    "ON \n",
    "    (d.driverId = t.driverId);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almacenamiento de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se agrega una porción de codigo adicional a la consulta anterior para almacenar la tabla final obtenida en la carpeta `/tmp/drivers/summary` del HDFS para que otras aplicaciones puedan usar estos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT OVERWRITE DIRECTORY '/tmp/drivers/summary' \n",
      "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
      "SELECT \n",
      "    d.driverId, \n",
      "    d.name, \n",
      "    t.total_hours, \n",
      "    t.total_miles \n",
      "FROM \n",
      "    drivers d\n",
      "JOIN (\n",
      "    SELECT \n",
      "        driverId, \n",
      "        sum(hours_logged)total_hours, \n",
      "        sum(miles_logged)total_miles \n",
      "    FROM \n",
      "        timesheet \n",
      "    GROUP BY \n",
      "        driverId \n",
      "    ) t\n",
      "ON \n",
      "    (d.driverId = t.driverId);\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = vagrant_20190526140100_79bddcf8-a683-4fdd-9950-afa1b6fc8b61\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1558824450495_0033, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558824450495_0033/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558824450495_0033\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2019-05-26 14:01:09,384 Stage-2 map = 0%,  reduce = 0%\n",
      "2019-05-26 14:01:13,796 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.96 sec\n",
      "2019-05-26 14:01:18,088 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.37 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 370 msec\n",
      "Ended Job = job_1558824450495_0033\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive-2.3.4/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.8.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "2019-05-26 14:01:24\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2019-05-26 14:01:26\tDump the side-table for tag: 0 with group count: 33 into file: file:/tmp/vagrant/1a0d2a35-e99a-4d93-bff5-37a7462a341d/hive_2019-05-26_14-01-00_064_2059878834031344160-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile10--.hashtable\n",
      "2019-05-26 14:01:26\tUploaded 1 File to: file:/tmp/vagrant/1a0d2a35-e99a-4d93-bff5-37a7462a341d/hive_2019-05-26_14-01-00_064_2059878834031344160-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile10--.hashtable (1325 bytes)\n",
      "2019-05-26 14:01:26\tEnd of local task; Time Taken: 1.607 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1558824450495_0034, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558824450495_0034/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558824450495_0034\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0\n",
      "2019-05-26 14:01:33,590 Stage-3 map = 0%,  reduce = 0%\n",
      "2019-05-26 14:01:37,908 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.32 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 320 msec\n",
      "Ended Job = job_1558824450495_0034\n",
      "Moving data to directory /tmp/drivers/summary\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.37 sec   HDFS Read: 32515 HDFS Write: 946 SUCCESS\n",
      "Stage-Stage-3: Map: 1   Cumulative CPU: 1.32 sec   HDFS Read: 6357 HDFS Write: 928 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 4 seconds 690 msec\n",
      "OK\n",
      "Time taken: 38.942 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "INSERT OVERWRITE DIRECTORY '/tmp/drivers/summary' \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "SELECT \n",
    "    d.driverId, \n",
    "    d.name, \n",
    "    t.total_hours, \n",
    "    t.total_miles \n",
    "FROM \n",
    "    drivers d\n",
    "JOIN (\n",
    "    SELECT \n",
    "        driverId, \n",
    "        sum(hours_logged)total_hours, \n",
    "        sum(miles_logged)total_miles \n",
    "    FROM \n",
    "        timesheet \n",
    "    GROUP BY \n",
    "        driverId \n",
    "    ) t\n",
    "ON \n",
    "    (d.driverId = t.driverId);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxr-xr-x   1 vagrant supergroup        928 2019-05-26 14:01 /tmp/drivers/summary/000000_0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/drivers/summary/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,Jamie Engesser,3642,179300\n",
      "12,Paul Coddin,2639,135962\n",
      "13,Joe Niemiec,2727,134126\n",
      "14,Adis Cesir,2781,136624\n",
      "15,Rohit Bakshi,2734,138750\n",
      "16,Tom McCuch,2746,137205\n",
      "17,Eric Mizell,2701,135992\n",
      "18,Grant Liu,2654,137834\n",
      "19,Ajay Singh,2738,137968\n",
      "20,Chris Harris,2644,134564\n",
      "21,Jeff Markham,2751,138719\n",
      "22,Nadeem Asghar,2733,137550\n",
      "23,Adam Diaz,2750,137980\n",
      "24,Don Hilborn,2647,134461\n",
      "25,Jean-Philippe Playe,2723,139180\n",
      "26,Michael Aube,2730,137530\n",
      "27,Mark Lochbihler,2771,137922\n",
      "28,Olivier Renault,2723,137469\n",
      "29,Teddy Choi,2760,138255\n",
      "30,Dan Rice,2773,137473\n",
      "31,Rommel Garcia,2704,137057\n",
      "32,Ryan Templeton,2736,137422\n",
      "33,Sridhara Sabbella,2759,139285\n",
      "34,Frank Romano,2811,137728\n",
      "35,Emil Siemes,2728,138727\n",
      "36,Andrew Grande,2795,138025\n",
      "37,Wes Floyd,2694,137223\n",
      "38,Scott Shaw,2760,137464\n",
      "39,David Kaiser,2745,138788\n",
      "40,Nicolas Maillard,2700,136931\n",
      "41,Greg Phillips,2723,138407\n",
      "42,Randy Gelhausen,2697,136673\n",
      "43,Dave Patton,2750,136993\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /tmp/drivers/summary/000000_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
