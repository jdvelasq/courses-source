{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de datos con Hive\n",
    "\n",
    "* *30 min* | Última modificación: Junio 22, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tutorial esta basado en https://es.hortonworks.com/tutorial/how-to-process-data-with-apache-hive/\n",
    "\n",
    "El objetivo de este tutorial es implemetar consultas en Hive para analizar, procesar y filtrar los datos existentes en una bodega de datos, usando lenguaje SQL estándar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se usa el magic `bigdata` para usar interactivamente Hive desde un notebook de Jupyter. El parámetro `timeout` es el tiempo máximo de espera de procesamiento antes de que se reporte un error por procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata\n",
    "%timeout 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos se encuentran almacenados en la carpeta `drivers` del directorio actual. A continución se procede a crear la carpeta `/tmp/drivers` en el sistema de archivos de Hadoop (HDFS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://deb.nodesource.com/node_13.x bionic InRelease [4584 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]               \n",
      "Get:4 https://deb.nodesource.com/node_13.x bionic/main amd64 Packages [764 B]  \n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [12.6 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [782 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]      \n",
      "Get:8 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [700 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]    \n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [5944 B]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]   \n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [996 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [25.9 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1309 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [9022 B]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4234 B]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [2496 B]\n",
      "Fetched 17.2 MB in 20s (865 kB/s)                                              \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  wget\n",
      "0 upgraded, 1 newly installed, 0 to remove and 3 not upgraded.\n",
      "Need to get 316 kB of archives.\n",
      "After this operation, 954 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 wget amd64 1.19.4-1ubuntu2.2 [316 kB]\n",
      "Fetched 316 kB in 1s (269 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package wget.\n",
      "(Reading database ... 25970 files and directories currently installed.)\n",
      "Preparing to unpack .../wget_1.19.4-1ubuntu2.2_amd64.deb ...\n",
      "Unpacking wget (1.19.4-1ubuntu2.2) ...\n",
      "Setting up wget (1.19.4-1ubuntu2.2) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-08 10:12:22--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/drivers.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2043 (2.0K) [text/plain]\n",
      "Saving to: 'drivers.csv'\n",
      "\n",
      "drivers.csv         100%[===================>]   2.00K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2019-11-08 10:12:22 (1.32 MB/s) - 'drivers.csv' saved [2043/2043]\n",
      "\n",
      "--2019-11-08 10:12:23--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/timesheet.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26205 (26K) [text/plain]\n",
      "Saving to: 'timesheet.csv'\n",
      "\n",
      "timesheet.csv       100%[===================>]  25.59K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2019-11-08 10:12:24 (934 KB/s) - 'timesheet.csv' saved [26205/26205]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/drivers.csv\n",
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/timesheet.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp/drivers': File exists\n",
      "-rw-r--r--   1 root supergroup       2043 2019-11-08 10:13 /tmp/drivers/drivers.csv\n",
      "-rw-r--r--   1 root supergroup      26205 2019-11-08 10:13 /tmp/drivers/timesheet.csv\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Crea la carpeta drivers en el HDFS\n",
    "##\n",
    "!hdfs dfs -mkdir /tmp/drivers\n",
    "\n",
    "##\n",
    "## Copia los archivos al HDFS\n",
    "##\n",
    "!hdfs dfs -copyFromLocal drivers.csv  /tmp/drivers/\n",
    "!hdfs dfs -copyFromLocal timesheet.csv  /tmp/drivers/\n",
    "\n",
    "##\n",
    "## Lista los archivos al HDFS para verificar\n",
    "## que los archivos fueron copiados correctamente.\n",
    "##\n",
    "!hdfs dfs -ls /tmp/drivers/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El contenido de un archivo puede ser visualizado parcialmente usando el comando `tail`. Se usa para realizar una inspección rápida del contenido de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box 213- 8948 Nec Ave,Y,hours\n",
      "27,Mark Lochbihler,392603159,8355 Ipsum St.,Y,hours\n",
      "28,Olivier Renault,959908181,P.O. Box 243- 6509 Erat. Avenue,Y,hours\n",
      "29,Teddy Choi,185502192,P.O. Box 106- 7003 Amet Rd.,Y,hours\n",
      "30,Dan Rice,282307061,Ap #881-9267 Mollis Avenue,Y,hours\n",
      "31,Rommel Garcia,858912101,P.O. Box 945- 6015 Sociis St.,Y,hours\n",
      "32,Ryan Templeton,290304287,765-6599 Egestas. Av.,Y,hours\n",
      "33,Sridhara Sabbella,967409015,Ap #477-2507 Sagittis Avenue,Y,hours\n",
      "34,Frank Romano,391407216,Ap #753-6814 Quis Ave,Y,hours\n",
      "35,Emil Siemes,971401151,321-2976 Felis Rd.,Y,hours\n",
      "36,Andrew Grande,245303216,Ap #685-9598 Egestas Rd.,Y,hours\n",
      "37,Wes Floyd,190504074,P.O. Box 269- 9611 Nulla Street,Y,hours\n",
      "38,Scott Shaw,386411175,276 Lobortis Road,Y,hours\n",
      "39,David Kaiser,967706052,9185 At Street,Y,hours\n",
      "40,Nicolas Maillard,208510217,1027 Quis Rd.,Y,hours\n",
      "41,Greg Phillips,308103116,P.O. Box 847- 5961 Arcu. Road,Y,hours\n",
      "42,Randy Gelhausen,853302254,145-4200 In- Avenue,Y,hours\n",
      "43,Dave Patton,977706052,3028 A- St.,Y,hours"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se imprime el final del archivo drivers\n",
    "##\n",
    "!hdfs dfs -tail /tmp/drivers/drivers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42,36,56,2612\n",
      "42,37,48,2550\n",
      "42,38,55,2527\n",
      "42,39,57,2723\n",
      "42,40,55,2728\n",
      "42,41,50,2557\n",
      "42,42,53,2773\n",
      "42,43,55,2786\n",
      "42,44,54,2638\n",
      "42,45,57,2542\n",
      "42,46,48,2526\n",
      "42,47,50,2795\n",
      "42,48,53,2609\n",
      "42,49,58,2584\n",
      "42,50,48,2692\n",
      "42,51,50,2566\n",
      "42,52,48,2735\n",
      "43,1,46,2622\n",
      "43,2,47,2688\n",
      "43,3,50,2544\n",
      "43,4,56,2573\n",
      "43,5,54,2691\n",
      "43,6,52,2796\n",
      "43,7,53,2564\n",
      "43,8,58,2624\n",
      "43,9,50,2528\n",
      "43,10,57,2721\n",
      "43,11,51,2722\n",
      "43,12,59,2681\n",
      "43,13,52,2683\n",
      "43,14,46,2663\n",
      "43,15,53,2579\n",
      "43,16,56,2519\n",
      "43,17,54,2584\n",
      "43,18,47,2665\n",
      "43,19,55,2511\n",
      "43,20,60,2677\n",
      "43,21,52,2585\n",
      "43,22,60,2719\n",
      "43,23,48,2655\n",
      "43,24,48,2641\n",
      "43,25,53,2512\n",
      "43,26,48,2612\n",
      "43,27,58,2614\n",
      "43,28,60,2551\n",
      "43,29,55,2682\n",
      "43,30,49,2504\n",
      "43,31,51,2701\n",
      "43,32,57,2554\n",
      "43,33,52,2730\n",
      "43,34,54,2783\n",
      "43,35,51,2681\n",
      "43,36,51,2655\n",
      "43,37,46,2629\n",
      "43,38,58,2739\n",
      "43,39,47,2535\n",
      "43,40,50,2512\n",
      "43,41,51,2701\n",
      "43,42,55,2538\n",
      "43,43,58,2775\n",
      "43,44,56,2545\n",
      "43,45,46,2671\n",
      "43,46,57,2680\n",
      "43,47,50,2572\n",
      "43,48,52,2517\n",
      "43,49,56,2743\n",
      "43,50,59,2665\n",
      "43,51,58,2593\n",
      "43,52,48,2764"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /tmp/drivers/timesheet.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la tabla `temp_drivers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crea la tabla `temp_drivers`, que es almacenada en el disco como un archivo de texto, para almacenar la información de los conductores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS temp_drivers;\n",
      "OK\n",
      "Time taken: 6.917 seconds\n",
      "CREATE TABLE temp_drivers (col_value STRING) STORED AS TEXTFILE\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 1.094 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS temp_drivers;\n",
    "CREATE TABLE temp_drivers (col_value STRING) STORED AS TEXTFILE\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente, se visualizan las tablas en la base de datos actual que empiezan por t para verificar que la tabla fue creada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW TABLES LIKE 't*';\n",
      "OK\n",
      "temp_drivers\n",
      "Time taken: 0.155 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SHOW TABLES LIKE 't*';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos para la tabla `temp_drivers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente consulta realiza la carga de los datos del archivo `drivers.csv` en la tabla `temp_drivers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ers; DATA INPATH '/tmp/drivers/drivers.csv' OVERWRITE INTO TABLE temp_driv \n",
      "Loading data to table default.temp_drivers\n",
      "OK\n",
      "Time taken: 1.093 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "LOAD DATA INPATH '/tmp/drivers/drivers.csv' OVERWRITE INTO TABLE temp_drivers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive consume los datos, es decir, mueve los datos a la bodega de datos, de tal forma que el archivo `drivers.csv` es eliminado de la carpeta `/tmp/drivers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup      26205 2019-11-08 10:13 /tmp/drivers/timesheet.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene los primeros 10 registros de la tabla para realizar una inspección rápida de los datos y verificar que los datos fueron cargados correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM temp_drivers LIMIT 10;\n",
      "OK\n",
      "10,George Vetticaden,621011971,244-4532 Nulla Rd.,N,miles\n",
      "11,Jamie Engesser,262112338,366-4125 Ac Street,N,miles\n",
      "12,Paul Coddin,198041975,Ap #622-957 Risus. Street,Y,hours\n",
      "13,Joe Niemiec,139907145,2071 Hendrerit. Ave,Y,hours\n",
      "14,Adis Cesir,820812209,Ap #810-1228 In St.,Y,hours\n",
      "15,Rohit Bakshi,239005227,648-5681 Dui- Rd.,Y,hours\n",
      "16,Tom McCuch,363303105,P.O. Box 313- 962 Parturient Rd.,Y,hours\n",
      "17,Eric Mizell,123808238,P.O. Box 579- 2191 Gravida. Street,Y,hours\n",
      "18,Grant Liu,171010151,Ap #928-3159 Vestibulum Av.,Y,hours\n",
      "19,Ajay Singh,160005158,592-9430 Nonummy Avenue,Y,hours\n",
      "Time taken: 2.294 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM temp_drivers LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la tabla `drivers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crea la tabla `drivers` en donde se colocará la información extraída de la tabla `temp_drivers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS drivers;\n",
      "OK\n",
      "Time taken: 0.008 seconds\n",
      "CREATE TABLE drivers (driverId  INT, \n",
      "                      name      STRING, \n",
      "                      ssn       BIGINT,\n",
      "                      location  STRING, \n",
      "                      certified STRING, \n",
      "                      wageplan  STRING)\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.074 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS drivers;\n",
    "\n",
    "CREATE TABLE drivers (driverId  INT, \n",
    "                      name      STRING, \n",
    "                      ssn       BIGINT,\n",
    "                      location  STRING, \n",
    "                      certified STRING, \n",
    "                      wageplan  STRING)\n",
    "\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que cada registro de la tabla `temp_drivers` es una línea de texto, se aplica una expresión regular (`regexp_extract`) para realizar la división del texto por las comas. La parte `{1}` representa la primera cadena de caracteres después de realizar la partición, `{2}` la segunda y así sucesivamente. Después de la llamada a la función `regexp_extract` se indica el nombre de la columna en la tabla `drivers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT OVERWRITE TABLE drivers\n",
      "SELECT\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) name,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) ssn,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) location,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){5}', 1) certified,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){6}', 1) wageplan\n",
      "FROM \n",
      "    temp_drivers;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191108101459_e2544eb9-0944-4166-b56a-1a6d34c91757\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1573207742881_0001, Tracking URL = http://dd8f0caea87b:8088/proxy/application_1573207742881_0001/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1573207742881_0001\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2019-11-08 10:15:15,481 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-11-08 10:15:23,934 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.44 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 440 msec\n",
      "Ended Job = job_1573207742881_0001\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/drivers/.hive-staging_hive_2019-11-08_10-14-59_974_3520773915705597245-1/-ext-10000\n",
      "Loading data to table default.drivers\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.44 sec   HDFS Read: 6812 HDFS Write: 2036 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 440 msec\n",
      "OK\n",
      "Time taken: 26.606 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "INSERT OVERWRITE TABLE drivers\n",
    "SELECT\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) name,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) ssn,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) location,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){5}', 1) certified,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){6}', 1) wageplan\n",
    "FROM \n",
    "    temp_drivers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica la instrucción `SELECT` para revisar el resultado de la carga de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM drivers LIMIT 10;\n",
      "OK\n",
      "11\tJamie Engesser\t262112338\t366-4125 Ac Street\tN\tmiles\n",
      "12\tPaul Coddin\t198041975\tAp #622-957 Risus. Street\tY\thours\n",
      "13\tJoe Niemiec\t139907145\t2071 Hendrerit. Ave\tY\thours\n",
      "14\tAdis Cesir\t820812209\tAp #810-1228 In St.\tY\thours\n",
      "15\tRohit Bakshi\t239005227\t648-5681 Dui- Rd.\tY\thours\n",
      "16\tTom McCuch\t363303105\tP.O. Box 313- 962 Parturient Rd.\tY\thours\n",
      "17\tEric Mizell\t123808238\tP.O. Box 579- 2191 Gravida. Street\tY\thours\n",
      "18\tGrant Liu\t171010151\tAp #928-3159 Vestibulum Av.\tY\thours\n",
      "19\tAjay Singh\t160005158\t592-9430 Nonummy Avenue\tY\thours\n",
      "20\tChris Harris\t921812303\t883-2691 Proin Avenue\tY\thours\n",
      "Time taken: 0.144 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM drivers LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la tabla `temp_timesheet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a crear la tabla y cargar los datos para el archivo `time_sheet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS temp_timesheet;\n",
      "OK\n",
      "Time taken: 0.009 seconds\n",
      "CREATE TABLE temp_timesheet (col_value string) \n",
      "STORED AS TEXTFILE\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.06 seconds\n",
      "mesheet;A INPATH '/tmp/drivers/timesheet.csv' OVERWRITE INTO TABLE temp_ti \n",
      "Loading data to table default.temp_timesheet\n",
      "OK\n",
      "Time taken: 0.414 seconds\n",
      "SELECT * FROM temp_timesheet LIMIT 10;\n",
      "OK\n",
      "10,1,70,3300\n",
      "10,2,70,3300\n",
      "10,3,60,2800\n",
      "10,4,70,3100\n",
      "10,5,70,3200\n",
      "10,6,70,3300\n",
      "10,7,70,3000\n",
      "10,8,70,3300\n",
      "10,9,70,3200\n",
      "10,10,50,2500\n",
      "Time taken: 0.131 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS temp_timesheet;\n",
    "\n",
    "CREATE TABLE temp_timesheet (col_value string) \n",
    "STORED AS TEXTFILE\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "LOAD DATA INPATH '/tmp/drivers/timesheet.csv' OVERWRITE INTO TABLE temp_timesheet;\n",
    "\n",
    "SELECT * FROM temp_timesheet LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la tabla `timesheet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede igual que en las tablas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS timesheet;\n",
      "OK\n",
      "Time taken: 0.008 seconds\n",
      "ogged INT)LE timesheet (driverId INT, week INT, hours_logged INT , miles_l \n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.065 seconds\n",
      "INSERT OVERWRITE TABLE timesheet\n",
      "SELECT\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) week,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) hours_logged,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) miles_logged\n",
      "FROM \n",
      "    temp_timesheet;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191108101549_2497d5ff-edf4-4076-84de-aa5034d5ebf2\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1573207742881_0002, Tracking URL = http://dd8f0caea87b:8088/proxy/application_1573207742881_0002/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1573207742881_0002\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2019-11-08 10:16:00,076 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-11-08 10:16:07,488 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.4 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 400 msec\n",
      "Ended Job = job_1573207742881_0002\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/timesheet/.hive-staging_hive_2019-11-08_10-15-49_287_5049985891851790760-1/-ext-10000\n",
      "Loading data to table default.timesheet\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 3.4 sec   HDFS Read: 30739 HDFS Write: 24476 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 400 msec\n",
      "OK\n",
      "Time taken: 20.737 seconds\n",
      "SELECT * FROM timesheet LIMIT 10;\n",
      "OK\n",
      "10\t2\t70\t3300\n",
      "10\t3\t60\t2800\n",
      "10\t4\t70\t3100\n",
      "10\t5\t70\t3200\n",
      "10\t6\t70\t3300\n",
      "10\t7\t70\t3000\n",
      "10\t8\t70\t3300\n",
      "10\t9\t70\t3200\n",
      "10\t10\t50\t2500\n",
      "10\t11\t70\t2900\n",
      "Time taken: 0.146 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS timesheet;\n",
    "\n",
    "CREATE TABLE timesheet (driverId INT, week INT, hours_logged INT , miles_logged INT)\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "INSERT OVERWRITE TABLE timesheet\n",
    "SELECT\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) week,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) hours_logged,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) miles_logged\n",
    "FROM \n",
    "    temp_timesheet;\n",
    "\n",
    "SELECT * FROM timesheet LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cantidad de horas y millas de cada conductor por año.\n",
    "\n",
    "En la siguiente consulta se desea obtener para cada conductor la cantidad de horas y millas por año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \n",
      "    driverId, \n",
      "    sum(hours_logged), \n",
      "    sum(miles_logged) \n",
      "FROM \n",
      "    timesheet \n",
      "GROUP BY \n",
      "    driverId;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191108101620_a7136990-cc28-4594-b59e-bdce97b50c0f\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1573207742881_0003, Tracking URL = http://dd8f0caea87b:8088/proxy/application_1573207742881_0003/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1573207742881_0003\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-11-08 10:16:31,957 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-11-08 10:16:39,327 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.97 sec\n",
      "2019-11-08 10:16:46,749 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.91 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 910 msec\n",
      "Ended Job = job_1573207742881_0003\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.91 sec   HDFS Read: 33422 HDFS Write: 1005 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 910 msec\n",
      "OK\n",
      "10\t3162\t143850\n",
      "11\t3642\t179300\n",
      "12\t2639\t135962\n",
      "13\t2727\t134126\n",
      "14\t2781\t136624\n",
      "15\t2734\t138750\n",
      "16\t2746\t137205\n",
      "17\t2701\t135992\n",
      "18\t2654\t137834\n",
      "19\t2738\t137968\n",
      "20\t2644\t134564\n",
      "21\t2751\t138719\n",
      "22\t2733\t137550\n",
      "23\t2750\t137980\n",
      "24\t2647\t134461\n",
      "25\t2723\t139180\n",
      "26\t2730\t137530\n",
      "27\t2771\t137922\n",
      "28\t2723\t137469\n",
      "29\t2760\t138255\n",
      "30\t2773\t137473\n",
      "31\t2704\t137057\n",
      "32\t2736\t137422\n",
      "33\t2759\t139285\n",
      "34\t2811\t137728\n",
      "35\t2728\t138727\n",
      "36\t2795\t138025\n",
      "37\t2694\t137223\n",
      "38\t2760\t137464\n",
      "39\t2745\t138788\n",
      "40\t2700\t136931\n",
      "41\t2723\t138407\n",
      "42\t2697\t136673\n",
      "43\t2750\t136993\n",
      "Time taken: 28.094 seconds, Fetched: 34 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive \n",
    "SELECT \n",
    "    driverId, \n",
    "    sum(hours_logged), \n",
    "    sum(miles_logged) \n",
    "FROM \n",
    "    timesheet \n",
    "GROUP BY \n",
    "    driverId;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta para unir las tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso final consiste en crear una consulta que agregue el nombre del conductor de la tabla `drivers` con la cantidad de horas y millas por año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \n",
      "    d.driverId, \n",
      "    d.name, \n",
      "    t.total_hours, \n",
      "    t.total_miles \n",
      "FROM \n",
      "    drivers d\n",
      "JOIN (\n",
      "    SELECT \n",
      "        driverId, \n",
      "        sum(hours_logged)total_hours, \n",
      "        sum(miles_logged)total_miles \n",
      "    FROM \n",
      "        timesheet \n",
      "    GROUP BY \n",
      "        driverId \n",
      "    ) t\n",
      "ON \n",
      "    (d.driverId = t.driverId);\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191108101653_b0d3d8c5-7207-4371-bffe-3056c25e3a12\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1573207742881_0004, Tracking URL = http://dd8f0caea87b:8088/proxy/application_1573207742881_0004/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1573207742881_0004\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2019-11-08 10:17:03,162 Stage-2 map = 0%,  reduce = 0%\n",
      "2019-11-08 10:17:11,514 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.51 sec\n",
      "2019-11-08 10:17:18,833 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.93 sec\n",
      "MapReduce Total cumulative CPU time: 4 seconds 930 msec\n",
      "Ended Job = job_1573207742881_0004\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "2019-11-08 10:17:28\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2019-11-08 10:17:30\tDump the side-table for tag: 0 with group count: 33 into file: file:/tmp/root/ec3a1d9b-0cbc-40cf-8ebe-40cc9fedd110/hive_2019-11-08_10-16-53_998_9182128341134448677-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable\n",
      "2019-11-08 10:17:30\tUploaded 1 File to: file:/tmp/root/ec3a1d9b-0cbc-40cf-8ebe-40cc9fedd110/hive_2019-11-08_10-16-53_998_9182128341134448677-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable (1325 bytes)\n",
      "2019-11-08 10:17:30\tEnd of local task; Time Taken: 2.087 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1573207742881_0005, Tracking URL = http://dd8f0caea87b:8088/proxy/application_1573207742881_0005/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1573207742881_0005\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0\n",
      "2019-11-08 10:17:41,007 Stage-3 map = 0%,  reduce = 0%\n",
      "2019-11-08 10:17:48,341 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.97 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 970 msec\n",
      "Ended Job = job_1573207742881_0005\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.93 sec   HDFS Read: 32512 HDFS Write: 946 SUCCESS\n",
      "Stage-Stage-3: Map: 1   Cumulative CPU: 1.97 sec   HDFS Read: 6784 HDFS Write: 1411 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 6 seconds 900 msec\n",
      "OK\n",
      "11\tJamie Engesser\t3642\t179300\n",
      "12\tPaul Coddin\t2639\t135962\n",
      "13\tJoe Niemiec\t2727\t134126\n",
      "14\tAdis Cesir\t2781\t136624\n",
      "15\tRohit Bakshi\t2734\t138750\n",
      "16\tTom McCuch\t2746\t137205\n",
      "17\tEric Mizell\t2701\t135992\n",
      "18\tGrant Liu\t2654\t137834\n",
      "19\tAjay Singh\t2738\t137968\n",
      "20\tChris Harris\t2644\t134564\n",
      "21\tJeff Markham\t2751\t138719\n",
      "22\tNadeem Asghar\t2733\t137550\n",
      "23\tAdam Diaz\t2750\t137980\n",
      "24\tDon Hilborn\t2647\t134461\n",
      "25\tJean-Philippe Playe\t2723\t139180\n",
      "26\tMichael Aube\t2730\t137530\n",
      "27\tMark Lochbihler\t2771\t137922\n",
      "28\tOlivier Renault\t2723\t137469\n",
      "29\tTeddy Choi\t2760\t138255\n",
      "30\tDan Rice\t2773\t137473\n",
      "31\tRommel Garcia\t2704\t137057\n",
      "32\tRyan Templeton\t2736\t137422\n",
      "33\tSridhara Sabbella\t2759\t139285\n",
      "34\tFrank Romano\t2811\t137728\n",
      "35\tEmil Siemes\t2728\t138727\n",
      "36\tAndrew Grande\t2795\t138025\n",
      "37\tWes Floyd\t2694\t137223\n",
      "38\tScott Shaw\t2760\t137464\n",
      "39\tDavid Kaiser\t2745\t138788\n",
      "40\tNicolas Maillard\t2700\t136931\n",
      "41\tGreg Phillips\t2723\t138407\n",
      "42\tRandy Gelhausen\t2697\t136673\n",
      "43\tDave Patton\t2750\t136993\n",
      "Time taken: 55.425 seconds, Fetched: 33 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT \n",
    "    d.driverId, \n",
    "    d.name, \n",
    "    t.total_hours, \n",
    "    t.total_miles \n",
    "FROM \n",
    "    drivers d\n",
    "JOIN (\n",
    "    SELECT \n",
    "        driverId, \n",
    "        sum(hours_logged)total_hours, \n",
    "        sum(miles_logged)total_miles \n",
    "    FROM \n",
    "        timesheet \n",
    "    GROUP BY \n",
    "        driverId \n",
    "    ) t\n",
    "ON \n",
    "    (d.driverId = t.driverId);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almacenamiento de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se agrega una porción de codigo adicional a la consulta anterior para almacenar la tabla final obtenida en la carpeta `/tmp/drivers/summary` del HDFS para que otras aplicaciones puedan usar estos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT OVERWRITE DIRECTORY '/tmp/drivers/summary' \n",
      "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
      "SELECT \n",
      "    d.driverId, \n",
      "    d.name, \n",
      "    t.total_hours, \n",
      "    t.total_miles \n",
      "FROM \n",
      "    drivers d\n",
      "JOIN (\n",
      "    SELECT \n",
      "        driverId, \n",
      "        sum(hours_logged)total_hours, \n",
      "        sum(miles_logged)total_miles \n",
      "    FROM \n",
      "        timesheet \n",
      "    GROUP BY \n",
      "        driverId \n",
      "    ) t\n",
      "ON \n",
      "    (d.driverId = t.driverId);\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191108101759_fcf02464-be59-4e0b-bea8-c46ec3eb05f0\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1573207742881_0006, Tracking URL = http://dd8f0caea87b:8088/proxy/application_1573207742881_0006/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1573207742881_0006\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2019-11-08 10:18:10,199 Stage-2 map = 0%,  reduce = 0%\n",
      "2019-11-08 10:18:17,608 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.61 sec\n",
      "2019-11-08 10:18:24,901 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.5 sec\n",
      "MapReduce Total cumulative CPU time: 4 seconds 500 msec\n",
      "Ended Job = job_1573207742881_0006\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "2019-11-08 10:18:35\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2019-11-08 10:18:37\tDump the side-table for tag: 0 with group count: 33 into file: file:/tmp/root/ec3a1d9b-0cbc-40cf-8ebe-40cc9fedd110/hive_2019-11-08_10-17-59_214_4330731367707089611-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile10--.hashtable\n",
      "2019-11-08 10:18:37\tUploaded 1 File to: file:/tmp/root/ec3a1d9b-0cbc-40cf-8ebe-40cc9fedd110/hive_2019-11-08_10-17-59_214_4330731367707089611-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile10--.hashtable (1325 bytes)\n",
      "2019-11-08 10:18:37\tEnd of local task; Time Taken: 2.068 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1573207742881_0007, Tracking URL = http://dd8f0caea87b:8088/proxy/application_1573207742881_0007/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1573207742881_0007\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0\n",
      "2019-11-08 10:18:48,260 Stage-3 map = 0%,  reduce = 0%\n",
      "2019-11-08 10:18:55,570 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.97 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 970 msec\n",
      "Ended Job = job_1573207742881_0007\n",
      "Moving data to directory /tmp/drivers/summary\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.5 sec   HDFS Read: 32512 HDFS Write: 946 SUCCESS\n",
      "Stage-Stage-3: Map: 1   Cumulative CPU: 1.97 sec   HDFS Read: 6345 HDFS Write: 928 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 6 seconds 470 msec\n",
      "OK\n",
      "Time taken: 58.506 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "INSERT OVERWRITE DIRECTORY '/tmp/drivers/summary' \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "SELECT \n",
    "    d.driverId, \n",
    "    d.name, \n",
    "    t.total_hours, \n",
    "    t.total_miles \n",
    "FROM \n",
    "    drivers d\n",
    "JOIN (\n",
    "    SELECT \n",
    "        driverId, \n",
    "        sum(hours_logged)total_hours, \n",
    "        sum(miles_logged)total_miles \n",
    "    FROM \n",
    "        timesheet \n",
    "    GROUP BY \n",
    "        driverId \n",
    "    ) t\n",
    "ON \n",
    "    (d.driverId = t.driverId);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxr-xr-x   1 root supergroup        928 2019-11-08 10:18 /tmp/drivers/summary/000000_0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/drivers/summary/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,Jamie Engesser,3642,179300\n",
      "12,Paul Coddin,2639,135962\n",
      "13,Joe Niemiec,2727,134126\n",
      "14,Adis Cesir,2781,136624\n",
      "15,Rohit Bakshi,2734,138750\n",
      "16,Tom McCuch,2746,137205\n",
      "17,Eric Mizell,2701,135992\n",
      "18,Grant Liu,2654,137834\n",
      "19,Ajay Singh,2738,137968\n",
      "20,Chris Harris,2644,134564\n",
      "21,Jeff Markham,2751,138719\n",
      "22,Nadeem Asghar,2733,137550\n",
      "23,Adam Diaz,2750,137980\n",
      "24,Don Hilborn,2647,134461\n",
      "25,Jean-Philippe Playe,2723,139180\n",
      "26,Michael Aube,2730,137530\n",
      "27,Mark Lochbihler,2771,137922\n",
      "28,Olivier Renault,2723,137469\n",
      "29,Teddy Choi,2760,138255\n",
      "30,Dan Rice,2773,137473\n",
      "31,Rommel Garcia,2704,137057\n",
      "32,Ryan Templeton,2736,137422\n",
      "33,Sridhara Sabbella,2759,139285\n",
      "34,Frank Romano,2811,137728\n",
      "35,Emil Siemes,2728,138727\n",
      "36,Andrew Grande,2795,138025\n",
      "37,Wes Floyd,2694,137223\n",
      "38,Scott Shaw,2760,137464\n",
      "39,David Kaiser,2745,138788\n",
      "40,Nicolas Maillard,2700,136931\n",
      "41,Greg Phillips,2723,138407\n",
      "42,Randy Gelhausen,2697,136673\n",
      "43,Dave Patton,2750,136993\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /tmp/drivers/summary/000000_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.csv *.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
