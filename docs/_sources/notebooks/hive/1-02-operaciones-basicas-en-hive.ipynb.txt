{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones básicas en Hive\n",
    "\n",
    "* *30 min* | Última modificación: Junio 22, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tutorial esta basado en https://es.hortonworks.com/tutorial/beginners-guide-to-apache-pig/\n",
    "\n",
    "En este tutorial se ejemplifica: \n",
    "\n",
    "* La carga de datos. \n",
    "\n",
    "* El uso básico de consultas.\n",
    "\n",
    "* La exportación de resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de Hive en un contenedor de Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usando el directorio de trabajo de la máquina local:\n",
    "\n",
    "```\n",
    "docker run --rm -it -v \"$PWD\":/datalake  --name hive -p 50070:50070 -p 8088:8088 -p 8888:8888 -p 5000:5000 jdvelasq/hive:2.3.6-pseudo\n",
    "```\n",
    "\n",
    "* Usando un volumen de Docker (llamado `datalake`):\n",
    "\n",
    "```\n",
    "docker run --rm -it -v datalake:/datalake --name hive  -p 50070:50070 -p 8088:8088 -p 8888:8888 -p 5000:5000 jdvelasq/hive:2.3.6-pseudo\n",
    "```\n",
    "\n",
    "\n",
    "* Consola conectada a un contendor que ya está corriendo:\n",
    "\n",
    "```\n",
    "docker exec -it hive bash\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se usa el magic `bigdata` para usar interactivamente Hive desde un notebook de Jupyter. El parámetro `timeout` es el tiempo máximo de espera de procesamiento antes de que se reporte un error por procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos se encuentran almacenados en la carpeta `drivers` del directorio actual. A continución se procede a crear la carpeta `/tmp/drivers` en el sistema de archivos de Hadoop (HDFS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-14 23:42:05--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/truck_event_text_partition.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2272077 (2.2M) [text/plain]\n",
      "Saving to: 'truck_event_text_partition.csv'\n",
      "\n",
      "truck_event_text_pa 100%[===================>]   2.17M  2.59MB/s    in 0.8s    \n",
      "\n",
      "2019-11-14 23:42:07 (2.59 MB/s) - 'truck_event_text_partition.csv' saved [2272077/2272077]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/truck_event_text_partition.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup    2272077 2019-11-14 23:42 /tmp/drivers/truck_event_text_partition.csv\n"
     ]
    }
   ],
   "source": [
    "## Borra la carpeta si existe\n",
    "!hdfs dfs -rm -r -f /tmp/drivers\n",
    "\n",
    "##\n",
    "## Crea la carpeta drivers en el HDFS\n",
    "##\n",
    "!hdfs dfs -mkdir /tmp/drivers\n",
    "\n",
    "##\n",
    "## Copia los archivos al HDFS\n",
    "##\n",
    "!hdfs dfs -copyFromLocal truck_event_text_partition.csv  /tmp/drivers/truck_event_text_partition.csv\n",
    "\n",
    "##\n",
    "## Lista los archivos al HDFS para verificar\n",
    "## que los archivos fueron copiados correctamente.\n",
    "##\n",
    "!hdfs dfs -ls /tmp/drivers/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los datos de los eventos de los conductores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código se crea crea la tabla de eventos de los conductores en el sistema; la primera instrucción borra la tabla si ya existe. Note que se debe especificar que los campos en las filas están delimitados por comas para que Hive los importe correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS truck_events;\n",
      "OK\n",
      "Time taken: 3.98 seconds\n",
      "CREATE TABLE truck_events (driverId       INT, \n",
      "                           truckId        INT,\n",
      "                           eventTime      STRING,\n",
      "                           eventType      STRING, \n",
      "                           longitude      DOUBLE, \n",
      "                           latitude       DOUBLE,\n",
      "                           eventKey       STRING, \n",
      "                           correlationId  STRING, \n",
      "                           driverName     STRING,\n",
      "                           routeId        BIGINT,\n",
      "                           routeName      STRING,\n",
      "                           eventDate      STRING)\n",
      "ROW FORMAT DELIMITED \n",
      "FIELDS TERMINATED BY ','\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.777 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS truck_events;\n",
    "\n",
    "CREATE TABLE truck_events (driverId       INT, \n",
    "                           truckId        INT,\n",
    "                           eventTime      STRING,\n",
    "                           eventType      STRING, \n",
    "                           longitude      DOUBLE, \n",
    "                           latitude       DOUBLE,\n",
    "                           eventKey       STRING, \n",
    "                           correlationId  STRING, \n",
    "                           driverName     STRING,\n",
    "                           routeId        BIGINT,\n",
    "                           routeName      STRING,\n",
    "                           eventDate      STRING)\n",
    "\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ','\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifican las tablas existentes en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW TABLES;\n",
      "OK\n",
      "docs\n",
      "truck_events\n",
      "word_counts\n",
      "Time taken: 0.093 seconds, Fetched: 3 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra la información detallada de creación de la tabla `truck_events`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW CREATE TABLE truck_events;\n",
      "OK\n",
      "CREATE TABLE `truck_events`(\n",
      "  `driverid` int, \n",
      "  `truckid` int, \n",
      "  `eventtime` string, \n",
      "  `eventtype` string, \n",
      "  `longitude` double, \n",
      "  `latitude` double, \n",
      "  `eventkey` string, \n",
      "  `correlationid` string, \n",
      "  `drivername` string, \n",
      "  `routeid` bigint, \n",
      "  `routename` string, \n",
      "  `eventdate` string)\n",
      "ROW FORMAT SERDE \n",
      "  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' \n",
      "WITH SERDEPROPERTIES ( \n",
      "  'field.delim'=',', \n",
      "  'serialization.format'=',') \n",
      "STORED AS INPUTFORMAT \n",
      "  'org.apache.hadoop.mapred.TextInputFormat' \n",
      "OUTPUTFORMAT \n",
      "  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION\n",
      "  'hdfs://0.0.0.0:9000/user/hive/warehouse/truck_events'\n",
      "TBLPROPERTIES (\n",
      "  'skip.header.line.count'='1', \n",
      "  'transient_lastDdlTime'='1573774948')\n",
      "Time taken: 0.273 seconds, Fetched: 27 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SHOW CREATE TABLE truck_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible visualizar los campos y su contenido con el comando `DESCRIBE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIBE truck_events;\n",
      "OK\n",
      "driverid            \tint                 \t                    \n",
      "truckid             \tint                 \t                    \n",
      "eventtime           \tstring              \t                    \n",
      "eventtype           \tstring              \t                    \n",
      "longitude           \tdouble              \t                    \n",
      "latitude            \tdouble              \t                    \n",
      "eventkey            \tstring              \t                    \n",
      "correlationid       \tstring              \t                    \n",
      "drivername          \tstring              \t                    \n",
      "routeid             \tbigint              \t                    \n",
      "routename           \tstring              \t                    \n",
      "eventdate           \tstring              \t                    \n",
      "Time taken: 0.045 seconds, Fetched: 12 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DESCRIBE truck_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La carga de datos se realiza con la siguiente consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD DATA INPATH '/tmp/drivers/truck_event_text_partition.csv' OVERWRITE \n",
      "INTO TABLE truck_events;\n",
      "Loading data to table default.truck_events\n",
      "OK\n",
      "Time taken: 0.744 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "LOAD DATA INPATH '/tmp/drivers/truck_event_text_partition.csv' OVERWRITE \n",
    "INTO TABLE truck_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifican las propieades de la tabla después de la carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW TBLPROPERTIES truck_events;\n",
      "OK\n",
      "numFiles\t1\n",
      "numRows\t0\n",
      "rawDataSize\t0\n",
      "skip.header.line.count\t1\n",
      "totalSize\t2272077\n",
      "transient_lastDdlTime\t1573774949\n",
      "Time taken: 0.03 seconds, Fetched: 6 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SHOW TBLPROPERTIES truck_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La visualización se realiza mediante consultas con\n",
    "`SELECT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM truck_events LIMIT 10;\n",
      "OK\n",
      "14\t25\t59:21.4\tNormal\t-94.58\t37.03\t14|25|9223370572464814373\t3.66E+18\tAdis Cesir\t160405074\tJoplin to Kansas City Route 2\t2016-05-27-22\n",
      "18\t16\t59:21.7\tNormal\t-89.66\t39.78\t18|16|9223370572464814089\t3.66E+18\tGrant Liu\t1565885487\tSpringfield to KC Via Hanibal\t2016-05-27-22\n",
      "27\t105\t59:21.7\tNormal\t-90.21\t38.65\t27|105|9223370572464814070\t3.66E+18\tMark Lochbihler\t1325562373\tSpringfield to KC Via Columbia Route 2\t2016-05-27-22\n",
      "11\t74\t59:21.7\tNormal\t-90.2\t38.65\t11|74|9223370572464814123\t3.66E+18\tJamie Engesser\t1567254452\tSaint Louis to Memphis Route2\t2016-05-27-22\n",
      "22\t87\t59:21.7\tNormal\t-90.04\t35.19\t22|87|9223370572464814101\t3.66E+18\tNadeem Asghar\t1198242881\t Saint Louis to Chicago Route2\t2016-05-27-22\n",
      "22\t87\t59:22.3\tNormal\t-90.37\t35.21\t22|87|9223370572464813486\t3.66E+18\tNadeem Asghar\t1198242881\t Saint Louis to Chicago Route2\t2016-05-27-22\n",
      "23\t68\t59:22.4\tNormal\t-89.91\t40.86\t23|68|9223370572464813450\t3.66E+18\tAdam Diaz\t160405074\tJoplin to Kansas City Route 2\t2016-05-27-22\n",
      "11\t74\t59:22.5\tNormal\t-89.74\t39.1\t11|74|9223370572464813355\t3.66E+18\tJamie Engesser\t1567254452\tSaint Louis to Memphis Route2\t2016-05-27-22\n",
      "20\t41\t59:22.5\tNormal\t-93.36\t41.69\t20|41|9223370572464813344\t3.66E+18\tChris Harris\t160779139\tDes Moines to Chicago Route 2\t2016-05-27-22\n",
      "32\t42\t59:22.5\tNormal\t-90.37\t35.21\t32|42|9223370572464813296\t3.66E+18\tRyan Templeton\t1090292248\tPeoria to Ceder Rapids Route 2\t2016-05-27-22\n",
      "Time taken: 1.113 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM truck_events LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención de un subconjunto de datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En hive es posible un subconjunto de datos y almacenarlo en una nueva tabla a partir de una consulta que permita obtener los datos deseados. En el siguiente código, se crea la tabla `truck_events_subset` con los primeros 100 registros de la tabla `truck_events`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS truck_events_subset;\n",
      "OK\n",
      "Time taken: 0.008 seconds\n",
      "CREATE TABLE truck_events_subset \n",
      "AS\n",
      "    SELECT *\n",
      "    FROM truck_events\n",
      "    LIMIT 100;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191114234231_aa3b3b9a-d3db-493d-85f3-4d873df2fc4a\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1573774297554_0006, Tracking URL = http://0982451e3758:8088/proxy/application_1573774297554_0006/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1573774297554_0006\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-11-14 23:42:38,953 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-11-14 23:42:43,223 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.26 sec\n",
      "2019-11-14 23:42:48,485 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.97 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 970 msec\n",
      "Ended Job = job_1573774297554_0006\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/truck_events_subset\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.97 sec   HDFS Read: 28725 HDFS Write: 13676 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 970 msec\n",
      "OK\n",
      "Time taken: 18.59 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS truck_events_subset;\n",
    "\n",
    "CREATE TABLE truck_events_subset \n",
    "AS\n",
    "    SELECT *\n",
    "    FROM truck_events\n",
    "    LIMIT 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior es equivalente al siguiente, donde se usa `LIKE` en `CREATE TABLE` para indicar que la nueva tabla `truck_events_subset` tiene la misma estructura de la tabla existente `truck_events`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS truck_events_subset;\n",
      "OK\n",
      "Time taken: 0.269 seconds\n",
      "CREATE TABLE truck_events_subset LIKE truck_events;\n",
      "OK\n",
      "Time taken: 0.061 seconds\n",
      "INSERT OVERWRITE TABLE truck_events_subset\n",
      "SELECT\n",
      "    *\n",
      "FROM\n",
      "    truck_events\n",
      "LIMIT\n",
      "    100;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191114234250_52b61a88-2397-4c40-8975-5f93c8eca79e\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1573774297554_0007, Tracking URL = http://0982451e3758:8088/proxy/application_1573774297554_0007/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1573774297554_0007\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-11-14 23:43:00,379 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-11-14 23:43:04,580 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.2 sec\n",
      "2019-11-14 23:43:09,820 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.8 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 800 msec\n",
      "Ended Job = job_1573774297554_0007\n",
      "Loading data to table default.truck_events_subset\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.8 sec   HDFS Read: 29426 HDFS Write: 13676 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 800 msec\n",
      "OK\n",
      "Time taken: 20.401 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS truck_events_subset;\n",
    "\n",
    "CREATE TABLE truck_events_subset LIKE truck_events;\n",
    "\n",
    "INSERT OVERWRITE TABLE truck_events_subset\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    truck_events\n",
    "LIMIT\n",
    "    100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM truck_events_subset LIMIT 5;\n",
      "OK\n",
      "31\t18\t59:36.3\tNormal\t-94.58\t37.03\t31|18|9223370572464799462\t3.66E+18\tRommel Garcia\t1594289134\tMemphis to Little Rock Route 2\t2016-05-27-22\n",
      "18\t16\t59:36.3\tNormal\t-92.42\t39.76\t18|16|9223370572464799486\t3.66E+18\tGrant Liu\t1565885487\tSpringfield to KC Via Hanibal\t2016-05-27-22\n",
      "26\t57\t59:35.9\tNormal\t-92.74\t37.6\t26|57|9223370572464799895\t3.66E+18\tMichael Aube\t1325712174\tSaint Louis to Tulsa Route2\t2016-05-27-22\n",
      "14\t25\t59:35.8\tNormal\t-94.46\t37.16\t14|25|9223370572464800006\t3.66E+18\tAdis Cesir\t160405074\tJoplin to Kansas City Route 2\t2016-05-27-22\n",
      "27\t105\t59:35.6\tNormal\t-92.85\t38.93\t27|105|9223370572464800175\t3.66E+18\tMark Lochbihler\t1325562373\tSpringfield to KC Via Columbia Route 2\t2016-05-27-22\n",
      "Time taken: 0.1 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM truck_events_subset LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención de un subconjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código se obtienen algunas columnas de la tabla `truck_events_subset` para ser almacenadas en una tabla diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS specific_columns; \n",
      "OK\n",
      "Time taken: 0.008 seconds\n",
      "CREATE TABLE specific_columns \n",
      "AS\n",
      "    SELECT\n",
      "        driverId, \n",
      "        eventTime, \n",
      "        eventType\n",
      "    FROM\n",
      "        truck_events_subset;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191114234311_31e15baa-b4c5-43db-8e2e-9869a1e864a9\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1573774297554_0008, Tracking URL = http://0982451e3758:8088/proxy/application_1573774297554_0008/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1573774297554_0008\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2019-11-14 23:43:22,087 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-11-14 23:43:26,254 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.21 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 210 msec\n",
      "Ended Job = job_1573774297554_0008\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/.hive-staging_hive_2019-11-14_23-43-11_877_7266039308226205704-1/-ext-10002\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/specific_columns\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 1.21 sec   HDFS Read: 18146 HDFS Write: 1883 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 1 seconds 210 msec\n",
      "OK\n",
      "Time taken: 15.784 seconds\n",
      "SELECT * FROM specific_columns LIMIT 5;\n",
      "OK\n",
      "31\t59:36.3\tNormal\n",
      "18\t59:36.3\tNormal\n",
      "26\t59:35.9\tNormal\n",
      "14\t59:35.8\tNormal\n",
      "27\t59:35.6\tNormal\n",
      "Time taken: 0.086 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS specific_columns; \n",
    "\n",
    "CREATE TABLE specific_columns \n",
    "AS\n",
    "    SELECT\n",
    "        driverId, \n",
    "        eventTime, \n",
    "        eventType\n",
    "    FROM\n",
    "        truck_events_subset;\n",
    "\n",
    "SELECT * FROM specific_columns LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escritura de la tabla en el HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente, se procede a escribir el contenido de la tabla en el directorio `/tmp/drivers/specific-columns` del HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT OVERWRITE DIRECTORY '/tmp/drivers/specific-columns' \n",
      "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
      "SELECT \n",
      "    * \n",
      "FROM \n",
      "    specific_columns;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191114234328_b45a3c77-76b9-4ce6-975c-fd25941d9010\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1573774297554_0009, Tracking URL = http://0982451e3758:8088/proxy/application_1573774297554_0009/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1573774297554_0009\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2019-11-14 23:43:38,228 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-11-14 23:43:43,473 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.57 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 570 msec\n",
      "Ended Job = job_1573774297554_0009\n",
      "Stage-3 is selected by condition resolver.\n",
      "Stage-2 is filtered out by condition resolver.\n",
      "Stage-4 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/tmp/drivers/specific-columns/.hive-staging_hive_2019-11-14_23-43-28_153_2409794566538603448-1/-ext-10000\n",
      "Moving data to directory /tmp/drivers/specific-columns\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 1.57 sec   HDFS Read: 5509 HDFS Write: 1800 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 1 seconds 570 msec\n",
      "OK\n",
      "Time taken: 16.412 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "INSERT OVERWRITE DIRECTORY '/tmp/drivers/specific-columns' \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "SELECT \n",
    "    * \n",
    "FROM \n",
    "    specific_columns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxr-xr-x   1 root supergroup       1800 2019-11-14 23:43 /tmp/drivers/specific-columns/000000_0\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se visualiza el contenido del directorio\n",
    "##\n",
    "!hdfs dfs -ls /tmp/drivers/specific-columns/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",59:29.6,Normal\n",
      "13,59:29.5,Normal\n",
      "27,59:29.3,Normal\n",
      "17,59:29.2,Normal\n",
      "12,59:29.1,Normal\n",
      "15,59:28.8,Normal\n",
      "16,59:28.8,Normal\n",
      "13,59:28.5,Normal\n",
      "23,59:28.4,Normal\n",
      "11,59:28.3,Normal\n",
      "30,59:28.0,Normal\n",
      "24,59:27.9,Normal\n",
      "25,59:27.8,Normal\n",
      "28,59:27.7,Normal\n",
      "27,59:27.7,Normal\n",
      "13,59:27.6,Normal\n",
      "23,59:27.4,Normal\n",
      "25,59:27.0,Normal\n",
      "26,59:27.0,Normal\n",
      "28,59:26.9,Normal\n",
      "10,59:26.8,Normal\n",
      "22,59:26.6,Normal\n",
      "23,59:26.6,Normal\n",
      "25,59:26.2,Normal\n",
      "27,59:25.9,Normal\n",
      "19,59:25.9,Normal\n",
      "13,59:25.9,Normal\n",
      "21,59:25.7,Normal\n",
      "16,59:25.3,Normal\n",
      "26,59:25.2,Normal\n",
      "19,59:25.1,Normal\n",
      "18,59:25.0,Normal\n",
      "22,59:25.0,Normal\n",
      "29,59:24.7,Normal\n",
      "25,59:24.3,Normal\n",
      "24,59:24.3,Normal\n",
      "32,59:24.2,Normal\n",
      "22,59:24.2,Normal\n",
      "14,59:24.2,Normal\n",
      "25,59:23.5,Normal\n",
      "31,59:23.5,Normal\n",
      "16,59:23.4,Normal\n",
      "15,59:23.4,Normal\n",
      "28,59:23.3,Normal\n",
      "14,59:23.3,Normal\n",
      "17,59:23.2,Normal\n",
      "27,59:22.6,Normal\n",
      "32,59:22.5,Normal\n",
      "20,59:22.5,Normal\n",
      "11,59:22.5,Normal\n",
      "23,59:22.4,Normal\n",
      "22,59:22.3,Normal\n",
      "22,59:21.7,Normal\n",
      "11,59:21.7,Normal\n",
      "27,59:21.7,Normal\n",
      "18,59:21.7,Normal\n",
      "14,59:21.4,Normal\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se visualiza la parte final del archivo\n",
    "##\n",
    "!hdfs dfs -tail /tmp/drivers/specific-columns/000000_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE drivers;\n",
      "OK\n",
      "Time taken: 0.069 seconds\n",
      "DROP TABLE specific_columns;\n",
      "OK\n",
      "Time taken: 0.071 seconds\n",
      "DROP TABLE temp_drivers;\n",
      "OK\n",
      "Time taken: 0.011 seconds\n",
      "DROP TABLE temp_timesheet;\n",
      "OK\n",
      "Time taken: 0.011 seconds\n",
      "DROP TABLE timesheet;\n",
      "OK\n",
      "Time taken: 0.008 seconds\n",
      "DROP TABLE truck_events;\n",
      "OK\n",
      "Time taken: 0.057 seconds\n",
      "DROP TABLE truck_events_subset;\n",
      "OK\n",
      "Time taken: 0.059 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE drivers;\n",
    "DROP TABLE specific_columns;\n",
    "DROP TABLE temp_drivers;\n",
    "DROP TABLE temp_timesheet;\n",
    "DROP TABLE timesheet;\n",
    "DROP TABLE truck_events;\n",
    "DROP TABLE truck_events_subset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '*hql': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm *.csv *.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
