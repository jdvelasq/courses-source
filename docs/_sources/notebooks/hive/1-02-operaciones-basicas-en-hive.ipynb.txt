{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones básicas en Hive\n",
    "\n",
    "* *30 min* | Última modificación: Junio 22, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tutorial esta basado en https://es.hortonworks.com/tutorial/beginners-guide-to-apache-pig/\n",
    "\n",
    "En este tutorial se ejemplifica: \n",
    "\n",
    "* La carga de datos. \n",
    "\n",
    "* El uso básico de consultas.\n",
    "\n",
    "* La exportación de resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se usa el magic `bigdata` para usar interactivamente Hive desde un notebook de Jupyter. El parámetro `timeout` es el tiempo máximo de espera de procesamiento antes de que se reporte un error por procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bigdata extension is already loaded. To reload it, use:\n",
      "  %reload_ext bigdata\n"
     ]
    }
   ],
   "source": [
    "%load_ext bigdata\n",
    "%timeout 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos se encuentran almacenados en la carpeta `drivers` del directorio actual. A continución se procede a crear la carpeta `/tmp/drivers` en el sistema de archivos de Hadoop (HDFS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 vagrant supergroup       2043 2019-06-12 19:47 /tmp/drivers/drivers.csv\n",
      "-rw-r--r--   1 vagrant supergroup       4308 2019-06-12 19:47 /tmp/drivers/drivers.json\n",
      "-rw-r--r--   1 vagrant supergroup      26205 2019-06-12 19:47 /tmp/drivers/timesheet.csv\n",
      "-rw-r--r--   1 vagrant supergroup    2272077 2019-06-12 19:47 /tmp/drivers/truck_event_text_partition.csv\n"
     ]
    }
   ],
   "source": [
    "## Borra la carpeta si existe\n",
    "!hdfs dfs -rm -r -f /tmp/drivers\n",
    "\n",
    "##\n",
    "## Crea la carpeta drivers en el HDFS\n",
    "##\n",
    "!hdfs dfs -mkdir /tmp/drivers\n",
    "\n",
    "##\n",
    "## Copia los archivos al HDFS\n",
    "##\n",
    "!hdfs dfs -copyFromLocal drivers/*  /tmp/drivers/\n",
    "\n",
    "##\n",
    "## Lista los archivos al HDFS para verificar\n",
    "## que los archivos fueron copiados correctamente.\n",
    "##\n",
    "!hdfs dfs -ls /tmp/drivers/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los datos de los eventos de los conductores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código se crea crea la tabla de eventos de los conductores en el sistema; la primera instrucción borra la tabla si ya existe. Note que se debe especificar que los campos en las filas están delimitados por comas para que Hive los importe correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS truck_events;\n",
      "FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n",
      "CREATE TABLE truck_events (driverId       INT, \n",
      "                           truckId        INT,\n",
      "                           eventTime      STRING,\n",
      "                           eventType      STRING, \n",
      "                           longitude      DOUBLE, \n",
      "                           latitude       DOUBLE,\n",
      "                           eventKey       STRING, \n",
      "                           correlationId  STRING, \n",
      "                           driverName     STRING,\n",
      "                           routeId        BIGINT,\n",
      "                           routeName      STRING,\n",
      "                           eventDate      STRING)\n",
      "ROW FORMAT DELIMITED \n",
      "FIELDS TERMINATED BY ','\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS truck_events;\n",
    "\n",
    "CREATE TABLE truck_events (driverId       INT, \n",
    "                           truckId        INT,\n",
    "                           eventTime      STRING,\n",
    "                           eventType      STRING, \n",
    "                           longitude      DOUBLE, \n",
    "                           latitude       DOUBLE,\n",
    "                           eventKey       STRING, \n",
    "                           correlationId  STRING, \n",
    "                           driverName     STRING,\n",
    "                           routeId        BIGINT,\n",
    "                           routeName      STRING,\n",
    "                           eventDate      STRING)\n",
    "\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ','\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifican las tablas existentes en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW TABLES;\n",
      "OK\n",
      "docs\n",
      "drivers\n",
      "specific_columns\n",
      "temp_drivers\n",
      "temp_timesheet\n",
      "timesheet\n",
      "truck_events\n",
      "truck_events_subset\n",
      "word_counts\n",
      "Time taken: 0.112 seconds, Fetched: 9 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra la información detallada de creación de la tabla `truck_events`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW CREATE TABLE truck_events;\n",
      "OK\n",
      "CREATE TABLE `truck_events`(\n",
      "  `driverid` int, \n",
      "  `truckid` int, \n",
      "  `eventtime` string, \n",
      "  `eventtype` string, \n",
      "  `longitude` double, \n",
      "  `latitude` double, \n",
      "  `eventkey` string, \n",
      "  `correlationid` string, \n",
      "  `drivername` string, \n",
      "  `routeid` bigint, \n",
      "  `routename` string, \n",
      "  `eventdate` string)\n",
      "ROW FORMAT SERDE \n",
      "  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' \n",
      "WITH SERDEPROPERTIES ( \n",
      "  'field.delim'=',', \n",
      "  'serialization.format'=',') \n",
      "STORED AS INPUTFORMAT \n",
      "  'org.apache.hadoop.mapred.TextInputFormat' \n",
      "OUTPUTFORMAT \n",
      "  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION\n",
      "  'hdfs://0.0.0.0:9000/user/hive/warehouse/truck_events'\n",
      "TBLPROPERTIES (\n",
      "  'skip.header.line.count'='1', \n",
      "  'transient_lastDdlTime'='1559010444')\n",
      "Time taken: 0.074 seconds, Fetched: 27 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SHOW CREATE TABLE truck_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible visualizar los campos y su contenido con el comando `DESCRIBE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIBE truck_events;\n",
      "OK\n",
      "driverid            \tint                 \t                    \n",
      "truckid             \tint                 \t                    \n",
      "eventtime           \tstring              \t                    \n",
      "eventtype           \tstring              \t                    \n",
      "longitude           \tdouble              \t                    \n",
      "latitude            \tdouble              \t                    \n",
      "eventkey            \tstring              \t                    \n",
      "correlationid       \tstring              \t                    \n",
      "drivername          \tstring              \t                    \n",
      "routeid             \tbigint              \t                    \n",
      "routename           \tstring              \t                    \n",
      "eventdate           \tstring              \t                    \n",
      "Time taken: 0.033 seconds, Fetched: 12 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DESCRIBE truck_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La carga de datos se realiza con la siguiente consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD DATA INPATH '/tmp/drivers/truck_event_text_partition.csv' OVERWRITE \n",
      "INTO TABLE truck_events;\n",
      "Loading data to table default.truck_events\n",
      "OK\n",
      "Time taken: 0.872 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "LOAD DATA INPATH '/tmp/drivers/truck_event_text_partition.csv' OVERWRITE \n",
    "INTO TABLE truck_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifican las propieades de la tabla después de la carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW TBLPROPERTIES truck_events;\n",
      "OK\n",
      "numFiles\t1\n",
      "numRows\t0\n",
      "rawDataSize\t0\n",
      "skip.header.line.count\t1\n",
      "totalSize\t2272077\n",
      "transient_lastDdlTime\t1559010445\n",
      "Time taken: 0.028 seconds, Fetched: 6 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SHOW TBLPROPERTIES truck_events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La visualización se realiza mediante consultas con\n",
    "`SELECT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM truck_events LIMIT 10;\n",
      "OK\n",
      "14\t25\t59:21.4\tNormal\t-94.58\t37.03\t14|25|9223370572464814373\t3.66E+18\tAdis Cesir\t160405074\tJoplin to Kansas City Route 2\t2016-05-27-22\n",
      "18\t16\t59:21.7\tNormal\t-89.66\t39.78\t18|16|9223370572464814089\t3.66E+18\tGrant Liu\t1565885487\tSpringfield to KC Via Hanibal\t2016-05-27-22\n",
      "27\t105\t59:21.7\tNormal\t-90.21\t38.65\t27|105|9223370572464814070\t3.66E+18\tMark Lochbihler\t1325562373\tSpringfield to KC Via Columbia Route 2\t2016-05-27-22\n",
      "11\t74\t59:21.7\tNormal\t-90.2\t38.65\t11|74|9223370572464814123\t3.66E+18\tJamie Engesser\t1567254452\tSaint Louis to Memphis Route2\t2016-05-27-22\n",
      "22\t87\t59:21.7\tNormal\t-90.04\t35.19\t22|87|9223370572464814101\t3.66E+18\tNadeem Asghar\t1198242881\t Saint Louis to Chicago Route2\t2016-05-27-22\n",
      "22\t87\t59:22.3\tNormal\t-90.37\t35.21\t22|87|9223370572464813486\t3.66E+18\tNadeem Asghar\t1198242881\t Saint Louis to Chicago Route2\t2016-05-27-22\n",
      "23\t68\t59:22.4\tNormal\t-89.91\t40.86\t23|68|9223370572464813450\t3.66E+18\tAdam Diaz\t160405074\tJoplin to Kansas City Route 2\t2016-05-27-22\n",
      "11\t74\t59:22.5\tNormal\t-89.74\t39.1\t11|74|9223370572464813355\t3.66E+18\tJamie Engesser\t1567254452\tSaint Louis to Memphis Route2\t2016-05-27-22\n",
      "20\t41\t59:22.5\tNormal\t-93.36\t41.69\t20|41|9223370572464813344\t3.66E+18\tChris Harris\t160779139\tDes Moines to Chicago Route 2\t2016-05-27-22\n",
      "32\t42\t59:22.5\tNormal\t-90.37\t35.21\t32|42|9223370572464813296\t3.66E+18\tRyan Templeton\t1090292248\tPeoria to Ceder Rapids Route 2\t2016-05-27-22\n",
      "Time taken: 1.223 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM truck_events LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención de un subconjunto de datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En hive es posible un subconjunto de datos y almacenarlo en una nueva tabla a partir de una consulta que permita obtener los datos deseados. En el siguiente código, se crea la tabla `truck_events_subset` con los primeros 100 registros de la tabla `truck_events`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS truck_events_subset;\n",
      "OK\n",
      "Time taken: 0.106 seconds\n",
      "CREATE TABLE truck_events_subset \n",
      "AS\n",
      "    SELECT *\n",
      "    FROM truck_events\n",
      "    LIMIT 100;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = vagrant_20190528022727_4214daba-470c-425a-9e92-59712ae22f81\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1558824450495_0083, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558824450495_0083/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558824450495_0083\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-05-28 02:27:34,788 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-05-28 02:27:40,297 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.5 sec\n",
      "2019-05-28 02:27:45,690 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.79 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 790 msec\n",
      "Ended Job = job_1558824450495_0083\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/truck_events_subset\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.79 sec   HDFS Read: 28725 HDFS Write: 13676 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 790 msec\n",
      "OK\n",
      "Time taken: 19.35 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS truck_events_subset;\n",
    "\n",
    "CREATE TABLE truck_events_subset \n",
    "AS\n",
    "    SELECT *\n",
    "    FROM truck_events\n",
    "    LIMIT 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior es equivalente al siguiente, donde se usa `LIKE` en `CREATE TABLE` para indicar que la nueva tabla `truck_events_subset` tiene la misma estructura de la tabla existente `truck_events`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS truck_events_subset;\n",
      "OK\n",
      "Time taken: 0.058 seconds\n",
      "CREATE TABLE truck_events_subset LIKE truck_events;\n",
      "OK\n",
      "Time taken: 0.05 seconds\n",
      "INSERT OVERWRITE TABLE truck_events_subset\n",
      "SELECT\n",
      "    *\n",
      "FROM\n",
      "    truck_events\n",
      "LIMIT\n",
      "    100;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = vagrant_20190528022747_08f7c087-a6e6-46e9-bf6a-6b6f685a943c\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1558824450495_0084, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558824450495_0084/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558824450495_0084\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-05-28 02:27:57,147 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-05-28 02:28:01,475 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.27 sec\n",
      "2019-05-28 02:28:05,789 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.63 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 630 msec\n",
      "Ended Job = job_1558824450495_0084\n",
      "Loading data to table default.truck_events_subset\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.63 sec   HDFS Read: 29426 HDFS Write: 13676 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 630 msec\n",
      "OK\n",
      "Time taken: 19.428 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS truck_events_subset;\n",
    "\n",
    "CREATE TABLE truck_events_subset LIKE truck_events;\n",
    "\n",
    "INSERT OVERWRITE TABLE truck_events_subset\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    truck_events\n",
    "LIMIT\n",
    "    100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM truck_events_subset LIMIT 5;\n",
      "OK\n",
      "31\t18\t59:36.3\tNormal\t-94.58\t37.03\t31|18|9223370572464799462\t3.66E+18\tRommel Garcia\t1594289134\tMemphis to Little Rock Route 2\t2016-05-27-22\n",
      "18\t16\t59:36.3\tNormal\t-92.42\t39.76\t18|16|9223370572464799486\t3.66E+18\tGrant Liu\t1565885487\tSpringfield to KC Via Hanibal\t2016-05-27-22\n",
      "26\t57\t59:35.9\tNormal\t-92.74\t37.6\t26|57|9223370572464799895\t3.66E+18\tMichael Aube\t1325712174\tSaint Louis to Tulsa Route2\t2016-05-27-22\n",
      "14\t25\t59:35.8\tNormal\t-94.46\t37.16\t14|25|9223370572464800006\t3.66E+18\tAdis Cesir\t160405074\tJoplin to Kansas City Route 2\t2016-05-27-22\n",
      "27\t105\t59:35.6\tNormal\t-92.85\t38.93\t27|105|9223370572464800175\t3.66E+18\tMark Lochbihler\t1325562373\tSpringfield to KC Via Columbia Route 2\t2016-05-27-22\n",
      "Time taken: 0.105 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM truck_events_subset LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención de un subconjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código se obtienen algunas columnas de la tabla `truck_events_subset` para ser almacenadas en una tabla diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS specific_columns; \n",
      "OK\n",
      "Time taken: 0.052 seconds\n",
      "CREATE TABLE specific_columns \n",
      "AS\n",
      "    SELECT\n",
      "        driverId, \n",
      "        eventTime, \n",
      "        eventType\n",
      "    FROM\n",
      "        truck_events_subset;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = vagrant_20190528022808_dd79fb1f-73a1-4fb2-8fe8-ea19a6a258d3\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1558824450495_0085, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558824450495_0085/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558824450495_0085\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2019-05-28 02:28:16,936 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-05-28 02:28:21,236 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.22 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 220 msec\n",
      "Ended Job = job_1558824450495_0085\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/.hive-staging_hive_2019-05-28_02-28-08_013_7102130941379267562-1/-ext-10002\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/specific_columns\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 1.22 sec   HDFS Read: 18126 HDFS Write: 1883 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 1 seconds 220 msec\n",
      "OK\n",
      "Time taken: 14.766 seconds\n",
      "SELECT * FROM specific_columns LIMIT 5;\n",
      "OK\n",
      "31\t59:36.3\tNormal\n",
      "18\t59:36.3\tNormal\n",
      "26\t59:35.9\tNormal\n",
      "14\t59:35.8\tNormal\n",
      "27\t59:35.6\tNormal\n",
      "Time taken: 0.114 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS specific_columns; \n",
    "\n",
    "CREATE TABLE specific_columns \n",
    "AS\n",
    "    SELECT\n",
    "        driverId, \n",
    "        eventTime, \n",
    "        eventType\n",
    "    FROM\n",
    "        truck_events_subset;\n",
    "\n",
    "SELECT * FROM specific_columns LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escritura de la tabla en el HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente, se procede a escribir el contenido de la tabla en el directorio `/tmp/drivers/specific-columns` del HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT OVERWRITE DIRECTORY '/tmp/drivers/specific-columns' \n",
      "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
      "SELECT \n",
      "    * \n",
      "FROM \n",
      "    specific_columns;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = vagrant_20190528022823_a4bf0a87-a469-49ed-911b-4cac85617c15\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1558824450495_0086, Tracking URL = http://ubuntu-bionic:8088/proxy/application_1558824450495_0086/\n",
      "Kill Command = /usr/local/hadoop-2.8.5/bin/hadoop job  -kill job_1558824450495_0086\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2019-05-28 02:28:33,570 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-05-28 02:28:37,911 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.19 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 190 msec\n",
      "Ended Job = job_1558824450495_0086\n",
      "Stage-3 is selected by condition resolver.\n",
      "Stage-2 is filtered out by condition resolver.\n",
      "Stage-4 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/tmp/drivers/specific-columns/.hive-staging_hive_2019-05-28_02-28-23_366_3148359705070380007-1/-ext-10000\n",
      "Moving data to directory /tmp/drivers/specific-columns\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 1.19 sec   HDFS Read: 5509 HDFS Write: 1800 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 1 seconds 190 msec\n",
      "OK\n",
      "Time taken: 15.672 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "INSERT OVERWRITE DIRECTORY '/tmp/drivers/specific-columns' \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "SELECT \n",
    "    * \n",
    "FROM \n",
    "    specific_columns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxr-xr-x   1 vagrant supergroup       1800 2019-05-28 02:28 /tmp/drivers/specific-columns/000000_0\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se visualiza el contenido del directorio\n",
    "##\n",
    "!hdfs dfs -ls /tmp/drivers/specific-columns/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",59:29.6,Normal\n",
      "13,59:29.5,Normal\n",
      "27,59:29.3,Normal\n",
      "17,59:29.2,Normal\n",
      "12,59:29.1,Normal\n",
      "15,59:28.8,Normal\n",
      "16,59:28.8,Normal\n",
      "13,59:28.5,Normal\n",
      "23,59:28.4,Normal\n",
      "11,59:28.3,Normal\n",
      "30,59:28.0,Normal\n",
      "24,59:27.9,Normal\n",
      "25,59:27.8,Normal\n",
      "28,59:27.7,Normal\n",
      "27,59:27.7,Normal\n",
      "13,59:27.6,Normal\n",
      "23,59:27.4,Normal\n",
      "25,59:27.0,Normal\n",
      "26,59:27.0,Normal\n",
      "28,59:26.9,Normal\n",
      "10,59:26.8,Normal\n",
      "22,59:26.6,Normal\n",
      "23,59:26.6,Normal\n",
      "25,59:26.2,Normal\n",
      "27,59:25.9,Normal\n",
      "19,59:25.9,Normal\n",
      "13,59:25.9,Normal\n",
      "21,59:25.7,Normal\n",
      "16,59:25.3,Normal\n",
      "26,59:25.2,Normal\n",
      "19,59:25.1,Normal\n",
      "18,59:25.0,Normal\n",
      "22,59:25.0,Normal\n",
      "29,59:24.7,Normal\n",
      "25,59:24.3,Normal\n",
      "24,59:24.3,Normal\n",
      "32,59:24.2,Normal\n",
      "22,59:24.2,Normal\n",
      "14,59:24.2,Normal\n",
      "25,59:23.5,Normal\n",
      "31,59:23.5,Normal\n",
      "16,59:23.4,Normal\n",
      "15,59:23.4,Normal\n",
      "28,59:23.3,Normal\n",
      "14,59:23.3,Normal\n",
      "17,59:23.2,Normal\n",
      "27,59:22.6,Normal\n",
      "32,59:22.5,Normal\n",
      "20,59:22.5,Normal\n",
      "11,59:22.5,Normal\n",
      "23,59:22.4,Normal\n",
      "22,59:22.3,Normal\n",
      "22,59:21.7,Normal\n",
      "11,59:21.7,Normal\n",
      "27,59:21.7,Normal\n",
      "18,59:21.7,Normal\n",
      "14,59:21.4,Normal\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se visualiza la parte final del archivo\n",
    "##\n",
    "!hdfs dfs -tail /tmp/drivers/specific-columns/000000_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE drivers;\n",
      "OK\n",
      "Time taken: 0.154 seconds\n",
      "DROP TABLE specific_columns;\n",
      "OK\n",
      "Time taken: 0.045 seconds\n",
      "DROP TABLE temp_drivers;\n",
      "OK\n",
      "Time taken: 0.049 seconds\n",
      "DROP TABLE temp_timesheet;\n",
      "OK\n",
      "Time taken: 0.057 seconds\n",
      "DROP TABLE timesheet;\n",
      "OK\n",
      "Time taken: 0.056 seconds\n",
      "DROP TABLE truck_events;\n",
      "OK\n",
      "Time taken: 0.048 seconds\n",
      "DROP TABLE truck_events_subset;\n",
      "OK\n",
      "Time taken: 0.046 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE drivers;\n",
    "DROP TABLE specific_columns;\n",
    "DROP TABLE temp_drivers;\n",
    "DROP TABLE temp_timesheet;\n",
    "DROP TABLE timesheet;\n",
    "DROP TABLE truck_events;\n",
    "DROP TABLE truck_events_subset;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
