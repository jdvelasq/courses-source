{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conteo de palabras en Apache Hive en modo standalone\n",
    "===\n",
    "\n",
    "* *30 min* | Última modificación: Noviembre 14, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial se aborda el proceso de construcción de aplicaciones en Apache Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de Hive en un contenedor de Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usando el directorio de trabajo de la máquina local:\n",
    "\n",
    "```\n",
    "docker run --rm -it -v \"$PWD\":/datalake  --name hive -p 8888:8888 jdvelasq/hive:2.3.6-standalone\n",
    "```\n",
    "\n",
    "* Usando un volumen de Docker (llamado `datalake`):\n",
    "\n",
    "```\n",
    "docker run --rm -it -v datalake:/datalake --name hive -p 8888:8888 jdvelasq/hive:2.3.6-standalone\n",
    "```\n",
    "\n",
    "\n",
    "* Consola conectada a un contendor que ya está corriendo:\n",
    "\n",
    "```\n",
    "docker exec -it hive bash\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de la librería de Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta librería permite enviar comandos a Hive de forma interactiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%hive_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificación y pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta primera parte se aborda el proceso de desarrollo y depuración de una aplicación de Hive en la máquina local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de datos de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos que usa la aplicación se encuentran localizados como una carpeta en el directorio actual de trabajo. Estos datos serán consumidos por Hive directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "## Se crea el directorio wordcount en la carpeta actual de trabajo\n",
    "## y se escriben tres archivos en ella.\n",
    "##\n",
    "!mkdir -p wordcount/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generan tres archivos de prueba que se almacenan en la carpeta `wordcount/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de tablas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta aplicación se usarán dos tablas:\n",
    "\n",
    "* `docs`: para cargar el contenido de los archivos de texto, donde cada línea equivale a un registro.\n",
    "\n",
    "* `word_counts`: En donde aparece cada palabra y su respectivo conteo.\n",
    "\n",
    "A continuación se elimnan dichas tablas si existen en el sistema, y luego se crea la tabla `docs` con un solo campo del tipo `STRING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS docs;\n",
      "OK\n",
      "Time taken: 6.618 seconds\n",
      "DROP TABLE IF EXISTS word_counts;\n",
      "OK\n",
      "Time taken: 0.008 seconds\n",
      "CREATE TABLE docs (line STRING);\n",
      "OK\n",
      "Time taken: 0.96 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS docs;\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "CREATE TABLE docs (line STRING);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código, se hace la carga directa de todos los archivos que se encuentran en el directorio `wordcount` en la tabla `docs`. Luego, se imprimen los primeros cinco registros de la tabla para verificar que la lectura fue correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD DATA LOCAL INPATH \"wordcount/\" OVERWRITE INTO TABLE docs;\n",
      "Loading data to table default.docs\n",
      "OK\n",
      "Time taken: 1.173 seconds\n",
      "SELECT * FROM docs LIMIT 5;\n",
      "OK\n",
      "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
      "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
      "on the simultaneous application of statistics, computer programming and operations research \n",
      "to quantify performance.\n",
      "\n",
      "Time taken: 2.242 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "LOAD DATA LOCAL INPATH \"wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "SELECT * FROM docs LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargados los archivos, se procede a partir las líneas por palabras, usando la función `split(line, '\\\\s')`;  la expresión `\\\\s` indica que se realice la partición por los espacios en blanco; de esta forma, `split()` genera una lista de palabras. La función `explode(.)` de Hive en conjunto con `SELECT`, genera un nuevo registro por cada palabra en `line`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT explode(split(line, '\\\\s')) AS word FROM docs LIMIT 5;\n",
      "OK\n",
      "Analytics\n",
      "is\n",
      "the\n",
      "discovery,\n",
      "interpretation,\n",
      "Time taken: 0.688 seconds, Fetched: 5 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT explode(split(line, '\\\\s')) AS word FROM docs LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conteo de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar el conteo, la expresión `SELECT word, count(1) AS count ... GROUP BY word` genera una tabla con dos columnas, donde la primera columna (`word`) correspodne a cada palabra en el texto, y la segunda columna representa la cantidad de veces que aparece en los registros generados por la expresión `SELECT explode(split(line, '\\\\s')) AS word FROM docs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE word_counts \n",
      "AS\n",
      "    SELECT word, count(1) AS count \n",
      "    FROM\n",
      "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
      "GROUP BY \n",
      "    word\n",
      "ORDER BY \n",
      "    word;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191115154030_a472520b-a145-439e-902b-a7adae054ec1\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2019-11-15 15:40:32,861 Stage-1 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local2011660276_0001\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Job running in-process (local Hadoop)\n",
      "2019-11-15 15:40:34,157 Stage-2 map = 100%,  reduce = 100%\n",
      "Ended Job = job_local597312914_0002\n",
      "Moving data to directory file:/user/hive/warehouse/word_counts\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Stage-Stage-2:  HDFS Read: 0 HDFS Write: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 0 msec\n",
      "OK\n",
      "Time taken: 4.126 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "CREATE TABLE word_counts \n",
    "AS\n",
    "    SELECT word, count(1) AS count \n",
    "    FROM\n",
    "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
    "GROUP BY \n",
    "    word\n",
    "ORDER BY \n",
    "    word;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar los resultados obtenidos, se realiza un `SELECT` sobre la tabla `word_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM word_counts LIMIT 10;\n",
      "OK\n",
      "\t20\n",
      "(DA)\t1\n",
      "(see\t1\n",
      "Analytics\t2\n",
      "Analytics,\t1\n",
      "Big\t1\n",
      "Data\t3\n",
      "Especially\t1\n",
      "Organizations\t1\n",
      "Since\t1\n",
      "Time taken: 0.137 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM word_counts LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cierre de Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, y una vez se ha terminado de depurar el código, se cierra el interprete de Hive que se abrió en el background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%hive_quit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versión en productivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la segunda parte, se procede a llevar el aplicativo a productivo con los siguientes cambios:\n",
    "\n",
    "* Los datos son leidos del sistema HDFS de Hadoop.\n",
    "\n",
    "* Los resultdos son guardados en una carpeta del sistema Hadoop.\n",
    "\n",
    "* El script se almacena en un archivo en el disco duro, para su uso posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copia de los datos al sistema HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "drwxr-xr-x   - root root       4096 2019-11-15 15:40 /tmp/hadoop-root\n",
      "drwxrwxrwx   - root root       4096 2019-11-15 15:40 /tmp/hive\n",
      "drwxr-xr-x   - root root       4096 2019-11-15 15:40 /tmp/hsperfdata_root\n",
      "drwxr-xr-x   - root root       4096 2019-11-15 15:40 /tmp/root\n",
      "-rw-r--r--   1 root root          0 2019-11-15 15:35 /tmp/stderr\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se usan un directorio temporal en el HDFS. La siguiente\n",
    "## instrucción muestra el contenido del dicho directorio\n",
    "##\n",
    "!hdfs dfs -ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Crea la carpeta wordcount en el hdfs\n",
    "##\n",
    "!hdfs dfs -mkdir /tmp/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "drwxr-xr-x   - root root       4096 2019-11-15 15:40 /tmp/hadoop-root\n",
      "drwxrwxrwx   - root root       4096 2019-11-15 15:40 /tmp/hive\n",
      "drwxr-xr-x   - root root       4096 2019-11-15 15:40 /tmp/hsperfdata_root\n",
      "drwxr-xr-x   - root root       4096 2019-11-15 15:40 /tmp/root\n",
      "-rw-r--r--   1 root root          0 2019-11-15 15:35 /tmp/stderr\n",
      "drwxr-xr-x   - root root       4096 2019-11-15 15:40 /tmp/wordcount\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Verifica la creación de la carpeta\n",
    "##\n",
    "!hdfs dfs -ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Copia los archvios del directorio local wordcount/\n",
    "## al directorio /tmp/wordcount/ en el hdfs\n",
    "##\n",
    "!hdfs dfs -copyFromLocal wordcount/*  /tmp/wordcount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 root root       1093 2019-11-15 15:40 /tmp/wordcount/text0.txt\n",
      "-rw-r--r--   1 root root        352 2019-11-15 15:40 /tmp/wordcount/text1.txt\n",
      "-rw-r--r--   1 root root        440 2019-11-15 15:40 /tmp/wordcount/text2.txt\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Verifica que los archivos esten copiados\n",
    "## en el hdfs\n",
    "##\n",
    "!hdfs dfs -ls /tmp/wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación del script y ajuste del código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizan dos cambios. En primer lugar, se sustituye la línea \n",
    "\n",
    "    LOAD DATA LOCAL INPATH \"wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "    \n",
    "por:\n",
    "\n",
    "    LOAD DATA INPATH \"/tmp/wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "\n",
    "para que Hive lea los datos del directorio `/tmp/wordcount/` en el HDFS. En segundo lugar, se agrega\n",
    "\n",
    "    INSERT OVERWRITE DIRECTORY '/tmp/output' \n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "    SELECT * FROM word_counts;\n",
    "    \n",
    "para que los resultados sean almacenados en la carpeta `/tmp/output` como un archivo en formato CSV. El programa es guadado como `wordcount.hql` en el computador local. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcount.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.hql\n",
    "\n",
    "DROP TABLE IF EXISTS docs;\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "\n",
    "CREATE TABLE docs (line STRING);\n",
    "\n",
    "LOAD DATA INPATH \"/tmp/wordcount/\" OVERWRITE INTO TABLE docs;\n",
    "\n",
    "CREATE TABLE word_counts \n",
    "AS\n",
    "    SELECT word, count(1) AS count \n",
    "    FROM\n",
    "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
    "GROUP BY \n",
    "    word\n",
    "ORDER BY \n",
    "    word;\n",
    "    \n",
    "INSERT OVERWRITE DIRECTORY '/tmp/output' \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "SELECT * FROM word_counts;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se ha almacenado el programa, se puede ejecutar con:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    }
   ],
   "source": [
    "!hive -S -e 'source wordcount.hql'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde `-S` indica que Hive se ejecute en modo silencioso; `-e` que se ejecute la expresión `source wordcount.hql`. Lo anterior es equivalente a abrir Hive y luego ejecutar:\n",
    "\n",
    "     hive> source 'wordcount.hql'\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados quedan almacenados en la carpeta `/tmp/output` del sistema HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxrwxrwt   1 root root       1653 2019-11-15 15:41 /tmp/output/000000_0\n"
     ]
    }
   ],
   "source": [
    "## Se lista el contenido del archivo.\n",
    "!hdfs dfs -ls /tmp/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",20\n",
      "(DA),1\n",
      "(see,1\n",
      "Analytics,2\n",
      "Analytics,,1\n",
      "Big,1\n",
      "Data,3\n",
      "Especially,1\n",
      "Organizations,1\n",
      "Since,1\n"
     ]
    }
   ],
   "source": [
    "## se visualiza la cabecera del archivo.\n",
    "!hdfs dfs -cat /tmp/output/000000_0 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copia de los resultados a la máquina local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/000000_0\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal /tmp/output output\n",
    "!ls output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",20\n",
      "(DA),1\n",
      "(see,1\n",
      "Analytics,2\n",
      "Analytics,,1\n",
      "Big,1\n",
      "Data,3\n",
      "Especially,1\n",
      "Organizations,1\n",
      "Since,1\n",
      "Specifically,,1\n",
      "The,2\n",
      "a,1\n",
      "about,1\n",
      "aid,1\n",
      "algorithms,1\n",
      "analysis,,1\n",
      "analysis.,1\n",
      "analytics,8\n",
      "analytics,,8\n",
      "analytics.,1\n",
      "analyze,1\n",
      "and,15\n",
      "application,1\n",
      "apply,1\n",
      "are,1\n",
      "areas,2\n",
      "assortment,1\n",
      "be,1\n",
      "big,1\n",
      "business,4\n",
      "by,2\n",
      "call,1\n",
      "can,2\n",
      "certain,1\n",
      "changes.,1\n",
      "cognitive,1\n",
      "commercial,1\n",
      "communication,1\n",
      "computation,1\n",
      "computer,2\n",
      "conclusions,1\n",
      "contain,,1\n",
      "credit,1\n",
      "current,1\n",
      "data,4\n",
      "data),,1\n",
      "data.,1\n",
      "decision,1\n",
      "decisions,2\n",
      "describe,,1\n",
      "descriptive,1\n",
      "discovery,,1\n",
      "disprove,1\n",
      "draw,1\n",
      "effects,1\n",
      "enable,1\n",
      "enterprise,1\n",
      "evaluate,1\n",
      "events,,1\n",
      "examining,1\n",
      "extensive,1\n",
      "field,1\n",
      "for,1\n",
      "force,1\n",
      "fraud,1\n",
      "gaining,1\n",
      "given,1\n",
      "goal,1\n",
      "harness,1\n",
      "historical,1\n",
      "hypotheses.,1\n",
      "improve,2\n",
      "improvements,1\n",
      "in,5\n",
      "include,1\n",
      "increasingly,1\n",
      "industries,1\n",
      "information,1\n",
      "information,,1\n",
      "interpretation,,1\n",
      "involves,1\n",
      "is,3\n",
      "knowledge,1\n",
      "make,2\n",
      "management,,1\n",
      "marketing,2\n",
      "mathematics.,1\n",
      "may,1\n",
      "meaningful,1\n",
      "methods,1\n",
      "mix,1\n",
      "modeling,,2\n",
      "models,,1\n",
      "more-informed,1\n",
      "most,1\n",
      "of,8\n",
      "often,1\n",
      "on,1\n",
      "operations,1\n",
      "optimization,1\n",
      "optimization,,2\n",
      "or,5\n",
      "order,1\n",
      "organizations,1\n",
      "past,1\n",
      "patterns,1\n",
      "performance,1\n",
      "performance.,2\n",
      "potential,1\n",
      "predict,,1\n",
      "predictive,2\n",
      "prescriptive,1\n",
      "price,1\n",
      "process,1\n",
      "programming,1\n",
      "promotion,1\n",
      "quantify,1\n",
      "recorded,1\n",
      "relies,1\n",
      "require,1\n",
      "research,2\n",
      "researchers,1\n",
      "retail,1\n",
      "rich,1\n",
      "risk,1\n",
      "sales,1\n",
      "scenario.,1\n",
      "science,,2\n",
      "scientific,1\n",
      "scientists,1\n",
      "sets,1\n",
      "simultaneous,1\n",
      "sizing,1\n",
      "software,1\n",
      "software.,1\n",
      "specialized,1\n",
      "speech,1\n",
      "statistics,,2\n",
      "stock-keeping,1\n",
      "store,1\n",
      "studying,1\n",
      "systems,1\n",
      "techniques,1\n",
      "technologies,1\n",
      "the,10\n",
      "theories,1\n",
      "they,1\n",
      "to,12\n",
      "tool,1\n",
      "trends,,1\n",
      "unit,1\n",
      "used,3\n",
      "valuable,1\n",
      "verify,1\n",
      "web,1\n",
      "which,1\n",
      "widely,1\n",
      "with,2\n",
      "within,1\n"
     ]
    }
   ],
   "source": [
    "!cat output/000000_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción para extraer los resultados es usar\n",
    "\n",
    "      $ hive -S -e 'SELECT * FROM word_counts;' > result.csv\n",
    "     \n",
    "     \n",
    "en donde el archivo `result.txt` se almacena localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf output wordcount *.hql *.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
